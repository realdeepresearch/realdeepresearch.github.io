{
  "cluster_id": 12,
  "papers": [
    {
      "id": "cvpr21_941",
      "paper_id": "",
      "title": "2D or not 2D? Adaptive 3D Convolution Selection for Efficient Video Recognition",
      "authors": "Hengduo Li, Zuxuan Wu, Abhinav Shrivastava, Larry S. Davis",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_2D_or_not_2D_Adaptive_3D_Convolution_Selection_for_Efficient_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2036",
      "paper_id": "",
      "title": "360+x: A Panoptic Multi-modal Scene Understanding Dataset",
      "authors": "Hao Chen, Yuqi Hou, Chenyuan Qu, Irene Testini, Xiaohan Hong, Jianbo Jiao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_360x_A_Panoptic_Multi-modal_Scene_Understanding_Dataset_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1440",
      "paper_id": "",
      "title": "3D CNNs With Adaptive Temporal Feature Resolutions",
      "authors": "Mohsen Fayyaz, Emad Bahrami, Ali Diba, Mehdi Noroozi, Ehsan Adeli, Luc Van Gool, Jurgen Gall",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Fayyaz_3D_CNNs_With_Adaptive_Temporal_Feature_Resolutions_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_1425",
      "paper_id": "",
      "title": "3MASSIV: Multilingual, Multimodal and Multi-Aspect Dataset of Social Media Short Videos",
      "authors": "Vikram Gupta, Trisha Mittal, Puneet Mathur, Vaibhav Mishra, Mayank Maheshwari, Aniket Bera, Debdoot Mukherjee, Dinesh Manocha",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_3MASSIV_Multilingual_Multimodal_and_Multi-Aspect_Dataset_of_Social_Media_Short_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1537",
      "paper_id": "",
      "title": "A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives",
      "authors": "Simone Alberto Peirone, Francesca Pistilli, Antonio Alliegro, Giuseppe Averta",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Peirone_A_Backpack_Full_of_Skills_Egocentric_Video_Understanding_with_Diverse_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1302",
      "paper_id": "",
      "title": "A Deeper Dive Into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information",
      "authors": "Matthew Kowal, Mennatullah Siam, Md Amirul Islam, Neil D. B. Bruce, Richard P. Wildes, Konstantinos G. Derpanis",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kowal_A_Deeper_Dive_Into_What_Deep_Spatiotemporal_Networks_Encode_Quantifying_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_912",
      "paper_id": "",
      "title": "A Large-Scale Comprehensive Dataset and Copy-Overlap Aware Evaluation Protocol for Segment-Level Video Copy Detection",
      "authors": "Sifeng He, Xudong Yang, Chen Jiang, Gang Liang, Wei Zhang, Tan Pan, Qing Wang, Furong Xu, Chunguang Li, JinXiong Liu, Hui Xu, Kaiming Huang, Yuan Cheng, Feng Qian, Xiaobo Zhang, Lei Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/He_A_Large-Scale_Comprehensive_Dataset_and_Copy-Overlap_Aware_Evaluation_Protocol_for_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1887",
      "paper_id": "",
      "title": "A Large-Scale Robustness Analysis of Video Action Recognition Models",
      "authors": "Madeline Chantry Schiappa, Naman Biyani, Prudvi Kamtam, Shruti Vyas, Hamid Palangi, Vibhav Vineet, Yogesh S. Rawat",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Schiappa_A_Large-Scale_Robustness_Analysis_of_Video_Action_Recognition_Models_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_1173",
      "paper_id": "",
      "title": "A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning",
      "authors": "Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, Kaiming He",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Feichtenhofer_A_Large-Scale_Study_on_Unsupervised_Spatiotemporal_Representation_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_560",
      "paper_id": "",
      "title": "A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames",
      "authors": "Pinelopi Papalampidi, Skanda Koppula, Shreya Pathak, Justin Chiu, Joe Heyward, Viorica Patraucean, Jiajun Shen, Antoine Miech, Andrew Zisserman, Aida Nematzdeh",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Papalampidi_A_Simple_Recipe_for_Contrastively_Pre-training_Video-First_Encoders_Beyond_16_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_604",
      "paper_id": "",
      "title": "A-Cap: Anticipation Captioning With Commonsense Knowledge",
      "authors": "Duc Minh Vo, Quoc-An Luong, Akihiro Sugimoto, Hideki Nakayama",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Vo_A-Cap_Anticipation_Captioning_With_Commonsense_Knowledge_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2329",
      "paper_id": "",
      "title": "Abductive Ego-View Accident Video Understanding for Safe Driving Perception",
      "authors": "Jianwu Fang, Lei-lei Li, Junfei Zhou, Junbin Xiao, Hongkai Yu, Chen Lv, Jianru Xue, Tat-Seng Chua",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Fang_Abductive_Ego-View_Accident_Video_Understanding_for_Safe_Driving_Perception_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1037",
      "paper_id": "",
      "title": "Action Detail Matters: Refining Video Recognition with Local Action Queries",
      "authors": "Mengmeng Wang, Zeyi Huang, Xiangjie Kong, Guojiang Shen, Guang Dai, Jingdong Wang, Yong Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Action_Detail_Matters_Refining_Video_Recognition_with_Local_Action_Queries_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_12",
      "paper_id": "",
      "title": "Action Detection via an Image Diffusion Process",
      "authors": "Lin Geng Foo, Tianjiao Li, Hossein Rahmani, Jun Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Foo_Action_Detection_via_an_Image_Diffusion_Process_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1561",
      "paper_id": "",
      "title": "Action Scene Graphs for Long-Form Understanding of Egocentric Videos",
      "authors": "Ivan Rodin, Antonino Furnari, Kyle Min, Subarna Tripathi, Giovanni Maria Farinella",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Rodin_Action_Scene_Graphs_for_Long-Form_Understanding_of_Egocentric_Videos_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_788",
      "paper_id": "",
      "title": "Action Shuffle Alternating Learning for Unsupervised Action Segmentation",
      "authors": "Jun Li, Sinisa Todorovic",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Action_Shuffle_Alternating_Learning_for_Unsupervised_Action_Segmentation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1578",
      "paper_id": "",
      "title": "Action Unit Memory Network for Weakly Supervised Temporal Action Localization",
      "authors": "Wang Luo, Tianzhu Zhang, Wenfei Yang, Jingen Liu, Tao Mei, Feng Wu, Yongdong Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Action_Unit_Memory_Network_for_Weakly_Supervised_Temporal_Action_Localization_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_979",
      "paper_id": "",
      "title": "ACTION-Net: Multipath Excitation for Action Recognition",
      "authors": "Zhengwei Wang, Qi She, Aljosa Smolic",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_ACTION-Net_Multipath_Excitation_for_Action_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2317",
      "paper_id": "",
      "title": "Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes",
      "authors": "Chi-Hsi Kung, Shu-Wei Lu, Yi-Hsuan Tsai, Yi-Ting Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kung_Action-slot_Visual_Action-centric_Representations_for_Multi-label_Atomic_Activity_Recognition_in_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_904",
      "paper_id": "",
      "title": "Active Exploration of Multimodal Complementarity for Few-Shot Action Recognition",
      "authors": "Yuyang Wanyan, Xiaoshan Yang, Chaofan Chen, Changsheng Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wanyan_Active_Exploration_of_Multimodal_Complementarity_for_Few-Shot_Action_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_894",
      "paper_id": "",
      "title": "Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization",
      "authors": "Junting Pan, Siyu Chen, Mike Zheng Shou, Yu Liu, Jing Shao, Hongsheng Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_Actor-Context-Actor_Relation_Network_for_Spatio-Temporal_Action_Localization_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_748",
      "paper_id": "",
      "title": "AdaCM^2: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction",
      "authors": "Yuanbin Man, Ying Huang, Chengming Zhang, Bingzhe Li, Wei Niu, Miao Yin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1845",
      "paper_id": "",
      "title": "AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition",
      "authors": "Yulin Wang, Yang Yue, Yuanze Lin, Haojun Jiang, Zihang Lai, Victor Kulikov, Nikita Orlov, Humphrey Shi, Gao Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_AdaFocus_V2_End-to-End_Training_of_Spatial_Dynamic_Networks_for_Video_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1979",
      "paper_id": "",
      "title": "AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning With Masked Autoencoders",
      "authors": "Wele Gedara Chaminda Bandara, Naman Patel, Ali Gholami, Mehdi Nikkhah, Motilal Agrawal, Vishal M. Patel",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bandara_AdaMAE_Adaptive_Masking_for_Efficient_Spatiotemporal_Learning_With_Masked_Autoencoders_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1474",
      "paper_id": "",
      "title": "AdamsFormer for Spatial Action Localization in the Future",
      "authors": "Hyung-gun Chi, Kwonjoon Lee, Nakul Agarwal, Yi Xu, Karthik Ramani, Chiho Choi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chi_AdamsFormer_for_Spatial_Action_Localization_in_the_Future_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2414",
      "paper_id": "",
      "title": "Adapting Short-Term Transformers for Action Detection in Untrimmed Videos",
      "authors": "Min Yang, Huan Gao, Ping Guo, Limin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Adapting_Short-Term_Transformers_for_Action_Detection_in_Untrimmed_Videos_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_945",
      "paper_id": "",
      "title": "Adaptive Keyframe Sampling for Long Video Understanding",
      "authors": "Xi Tang, Jihao Qiu, Lingxi Xie, Yunjie Tian, Jianbin Jiao, Qixiang Ye",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_Adaptive_Keyframe_Sampling_for_Long_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_714",
      "paper_id": "",
      "title": "Advancing High-Resolution Video-Language Representation With Large-Scale Video Transcriptions",
      "authors": "Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, Baining Guo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xue_Advancing_High-Resolution_Video-Language_Representation_With_Large-Scale_Video_Transcriptions_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_1527",
      "paper_id": "",
      "title": "AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning",
      "authors": "Madeleine Grunde-McLaughlin, Ranjay Krishna, Maneesh Agrawala",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Grunde-McLaughlin_AGQA_A_Benchmark_for_Compositional_Spatio-Temporal_Reasoning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2310",
      "paper_id": "",
      "title": "Align and Aggregate: Compositional Reasoning with Video Alignment and Answer Aggregation for Video Question-Answering",
      "authors": "Zhaohe Liao, Jiangtong Li, Li Niu, Liqing Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liao_Align_and_Aggregate_Compositional_Reasoning_with_Video_Alignment_and_Answer_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1279",
      "paper_id": "",
      "title": "Align and Attend: Multimodal Summarization With Dual Contrastive Losses",
      "authors": "Bo He, Jun Wang, Jielin Qiu, Trung Bui, Abhinav Shrivastava, Zhaowen Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/He_Align_and_Attend_Multimodal_Summarization_With_Dual_Contrastive_Losses_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_398",
      "paper_id": "",
      "title": "Align and Prompt: Video-and-Language Pre-Training With Entity Prompts",
      "authors": "Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, Steven C.H. Hoi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Align_and_Prompt_Video-and-Language_Pre-Training_With_Entity_Prompts_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1411",
      "paper_id": "",
      "title": "Align Before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition",
      "authors": "Yifei Chen, Dapeng Chen, Ruijin Liu, Sai Zhou, Wenyuan Xue, Wei Peng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Align_Before_Adapt_Leveraging_Entity-to-Region_Alignments_for_Generalizable_Video_Action_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_438",
      "paper_id": "",
      "title": "Aligning Step-by-Step Instructional Diagrams to Video Demonstrations",
      "authors": "Jiahao Zhang, Anoop Cherian, Yanbin Liu, Yizhak Ben-Shabat, Cristian Rodriguez, Stephen Gould",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Aligning_Step-by-Step_Instructional_Diagrams_to_Video_Demonstrations_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1260",
      "paper_id": "",
      "title": "Alignment-Uniformity Aware Representation Learning for Zero-Shot Video Classification",
      "authors": "Shi Pu, Kaili Zhao, Mao Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Pu_Alignment-Uniformity_Aware_Representation_Learning_for_Zero-Shot_Video_Classification_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1006",
      "paper_id": "",
      "title": "All in One: Exploring Unified Video-Language Pre-Training",
      "authors": "Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Kevin Qinghong Lin, Satoshi Tsutsui, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, Mike Zheng Shou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_241",
      "paper_id": "",
      "title": "An Actor-Centric Causality Graph for Asynchronous Temporal Inference in Group Activity",
      "authors": "Zhao Xie, Tian Gao, Kewei Wu, Jiao Chang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_An_Actor-Centric_Causality_Graph_for_Asynchronous_Temporal_Inference_in_Group_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_725",
      "paper_id": "",
      "title": "An Empirical Study of End-to-End Temporal Action Detection",
      "authors": "Xiaolong Liu, Song Bai, Xiang Bai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_An_Empirical_Study_of_End-to-End_Temporal_Action_Detection_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_411",
      "paper_id": "",
      "title": "An Empirical Study of End-to-End Video-Language Transformers With Masked Visual Modeling",
      "authors": "Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, Zicheng Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_An_Empirical_Study_of_End-to-End_Video-Language_Transformers_With_Masked_Visual_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1497",
      "paper_id": "",
      "title": "Anchor-Aware Similarity Cohesion in Target Frames Enables Predicting Temporal Moment Boundaries in 2D",
      "authors": "Jiawei Tan, Hongxing Wang, Junwu Weng, Jiaxin Li, Zhilong Ou, Kang Dang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tan_Anchor-Aware_Similarity_Cohesion_in_Target_Frames_Enables_Predicting_Temporal_Moment_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_846",
      "paper_id": "",
      "title": "Anchor-Constrained Viterbi for Set-Supervised Action Segmentation",
      "authors": "Jun Li, Sinisa Todorovic",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Anchor-Constrained_Viterbi_for_Set-Supervised_Action_Segmentation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1065",
      "paper_id": "",
      "title": "ANetQA: A Large-Scale Benchmark for Fine-Grained Compositional Reasoning Over Untrimmed Videos",
      "authors": "Zhou Yu, Lixiang Zheng, Zhou Zhao, Fei Wu, Jianping Fan, Kui Ren, Jun Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_ANetQA_A_Large-Scale_Benchmark_for_Fine-Grained_Compositional_Reasoning_Over_Untrimmed_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1952",
      "paper_id": "",
      "title": "ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction",
      "authors": "Yuejiao Su, Yi Wang, Qiongyang Hu, Chuang Yang, Lap-Pui Chau",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Su_ANNEXE_Unified_Analyzing_Answering_and_Pixel_Grounding_for_Egocentric_Interaction_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_381",
      "paper_id": "",
      "title": "Anticipating Human Actions by Correlating Past With the Future With Jaccard Similarity Measures",
      "authors": "Basura Fernando, Samitha Herath",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Fernando_Anticipating_Human_Actions_by_Correlating_Past_With_the_Future_With_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1472",
      "paper_id": "",
      "title": "Apollo:  An Exploration of Video Understanding in Large Multimodal Models",
      "authors": "Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, Serena Yeung-Levy, Xide Xia",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zohar_Apollo__An_Exploration_of_Video_Understanding_in_Large_Multimodal_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_686",
      "paper_id": "",
      "title": "Are Binary Annotations Sufficient? Video Moment Retrieval via Hierarchical Uncertainty-Based Active Learning",
      "authors": "Wei Ji, Renjie Liang, Zhedong Zheng, Wenqiao Zhang, Shengyu Zhang, Juncheng Li, Mengze Li, Tat-seng Chua",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Are_Binary_Annotations_Sufficient_Video_Moment_Retrieval_via_Hierarchical_Uncertainty-Based_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_409",
      "paper_id": "",
      "title": "ASM-Loc: Action-Aware Segment Modeling for Weakly-Supervised Temporal Action Localization",
      "authors": "Bo He, Xitong Yang, Le Kang, Zhiyu Cheng, Xin Zhou, Abhinav Shrivastava",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/He_ASM-Loc_Action-Aware_Segment_Modeling_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_2051",
      "paper_id": "",
      "title": "ASPnet: Action Segmentation With Shared-Private Representation of Multiple Data Sources",
      "authors": "Beatrice van Amsterdam, Abdolrahim Kadkhodamohammadi, Imanol Luengo, Danail Stoyanov",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/van_Amsterdam_ASPnet_Action_Segmentation_With_Shared-Private_Representation_of_Multiple_Data_Sources_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_608",
      "paper_id": "",
      "title": "Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities",
      "authors": "Fadime Sener, Dibyadip Chatterjee, Daniel Shelepov, Kun He, Dipika Singhania, Robert Wang, Angela Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Sener_Assembly101_A_Large-Scale_Multi-View_Video_Dataset_for_Understanding_Procedural_Activities_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1105",
      "paper_id": "",
      "title": "Audio-Adaptive Activity Recognition Across Video Domains",
      "authors": "Yunhua Zhang, Hazel Doughty, Ling Shao, Cees G. M. Snoek",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Audio-Adaptive_Activity_Recognition_Across_Video_Domains_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1134",
      "paper_id": "",
      "title": "Audio-Visual Generalised Zero-Shot Learning With Cross-Modal Attention and Language",
      "authors": "Otniel-Bogdan Mercea, Lukas Riesch, A. Sophia Koepke, Zeynep Akata",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Mercea_Audio-Visual_Generalised_Zero-Shot_Learning_With_Cross-Modal_Attention_and_Language_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2081",
      "paper_id": "",
      "title": "AutoAD III: The Prequel - Back to the Pixels",
      "authors": "Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Han_AutoAD_III_The_Prequel_-_Back_to_the_Pixels_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2021",
      "paper_id": "",
      "title": "AutoAD: Movie Description in Context",
      "authors": "Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Han_AutoAD_Movie_Description_in_Context_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1387",
      "paper_id": "",
      "title": "AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval",
      "authors": "Riku Togashi, Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkilä, Tetsuya Sakai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Togashi_AxIoU_An_Axiomatically_Justified_Measure_for_Video_Moment_Retrieval_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1795",
      "paper_id": "",
      "title": "BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation",
      "authors": "Yulu Pan, Ce Zhang, Gedas Bertasius",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Pan_BASKET_A_Large-Scale_Video_Dataset_for_Fine-Grained_Skill_Estimation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_565",
      "paper_id": "",
      "title": "Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions",
      "authors": "Runhao Zeng, Xiaoyong Chen, Jiaming Liang, Huisi Wu, Guangzhong Cao, Yong Guo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_Benchmarking_the_Robustness_of_Temporal_Action_Detection_Models_Against_Temporal_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1141",
      "paper_id": "",
      "title": "BEVT: BERT Pretraining of Video Transformers",
      "authors": "Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou, Lu Yuan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_BEVT_BERT_Pretraining_of_Video_Transformers_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_139",
      "paper_id": "",
      "title": "Beyond Short Clips: End-to-End Video-Level Learning With Collaborative Memories",
      "authors": "Xitong Yang, Haoqi Fan, Lorenzo Torresani, Larry S. Davis, Heng Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Beyond_Short_Clips_End-to-End_Video-Level_Learning_With_Collaborative_Memories_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2258",
      "paper_id": "",
      "title": "Bi-Causal: Group Activity Recognition via Bidirectional Causality",
      "authors": "Youliang Zhang, Wenxuan Liu, Danni Xu, Zhuo Zhou, Zheng Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Bi-Causal_Group_Activity_Recognition_via_Bidirectional_Causality_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_349",
      "paper_id": "",
      "title": "Bidirectional Cross-Modal Knowledge Exploration for Video Recognition With Pre-Trained Vision-Language Models",
      "authors": "Wenhao Wu, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, Wanli Ouyang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Bidirectional_Cross-Modal_Knowledge_Exploration_for_Video_Recognition_With_Pre-Trained_Vision-Language_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1646",
      "paper_id": "",
      "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question Answering",
      "authors": "Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Gedas Bertasius, Lorenzo Torresani",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Islam_BIMBA_Selective-Scan_Compression_for_Long-Range_Video_Question_Answering_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_842",
      "paper_id": "",
      "title": "BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding",
      "authors": "Shuming Liu, Chen Zhao, Tianqi Xu, Bernard Ghanem",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_BOLT_Boost_Large_Vision-Language_Model_Without_Training_for_Long-form_Video_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_298",
      "paper_id": "",
      "title": "Boosting Point-Supervised Temporal Action Localization through Integrating Query Reformation and Optimal Transport",
      "authors": "Mengnan Liu, Le Wang, Sanping Zhou, Kun Xia, Xiaolong Sun, Gang Hua",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Boosting_Point-Supervised_Temporal_Action_Localization_through_Integrating_Query_Reformation_and_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_523",
      "paper_id": "",
      "title": "Boosting Video Representation Learning With Multi-Faceted Integration",
      "authors": "Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xiao-Ping Zhang, Dong Wu, Tao Mei",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Qiu_Boosting_Video_Representation_Learning_With_Multi-Faceted_Integration_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1244",
      "paper_id": "",
      "title": "Boosting Weakly-Supervised Temporal Action Localization With Text Information",
      "authors": "Guozhang Li, De Cheng, Xinpeng Ding, Nannan Wang, Xiaoyu Wang, Xinbo Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Boosting_Weakly-Supervised_Temporal_Action_Localization_With_Text_Information_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1560",
      "paper_id": "",
      "title": "Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations",
      "authors": "Jungin Park, Jiyoung Lee, Kwanghoon Sohn",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Park_Bootstrap_Your_Own_Views_Masked_Ego-Exo_Modeling_for_Fine-grained_View-invariant_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2866",
      "paper_id": "",
      "title": "Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer",
      "authors": "Ziyi Liu, Yangcen Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Bridge_the_Gap_From_Weak_to_Full_Supervision_for_Temporal_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_628",
      "paper_id": "",
      "title": "Bridge To Answer: Structure-Aware Graph Interaction Network for Video Question Answering",
      "authors": "Jungin Park, Jiyoung Lee, Kwanghoon Sohn",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Park_Bridge_To_Answer_Structure-Aware_Graph_Interaction_Network_for_Video_Question_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_83",
      "paper_id": "",
      "title": "Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos",
      "authors": "Muheng Li, Lei Chen, Yueqi Duan, Zhilan Hu, Jianjiang Feng, Jie Zhou, Jiwen Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Bridge-Prompt_Towards_Ordinal_Action_Understanding_in_Instructional_Videos_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_834",
      "paper_id": "",
      "title": "Bridging the Gap: A Unified Video Comprehension Framework for Moment Retrieval and Highlight Detection",
      "authors": "Yicheng Xiao, Zhuoyan Luo, Yong Liu, Yue Ma, Hengwei Bian, Yatai Ji, Yujiu Yang, Xiu Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Bridging_the_Gap_A_Unified_Video_Comprehension_Framework_for_Moment_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_208",
      "paper_id": "",
      "title": "Bridging Video-Text Retrieval With Multiple Choice Questions",
      "authors": "Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, Ping Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ge_Bridging_Video-Text_Retrieval_With_Multiple_Choice_Questions_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1589",
      "paper_id": "",
      "title": "BT-Adapter: Video Conversation is Feasible Without Video Instruction Tuning",
      "authors": "Ruyang Liu, Chen Li, Yixiao Ge, Thomas H. Li, Ying Shan, Ge Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_BT-Adapter_Video_Conversation_is_Feasible_Without_Video_Instruction_Tuning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2871",
      "paper_id": "",
      "title": "Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs",
      "authors": "Zeyi Huang, Yuyang Ji, Xiaofang Wang, Nikhil Mehta, Tong Xiao, Donghyun Lee, Sigmund Vanvalkenburgh, Shengxin Zha, Bolin Lai, Licheng Yu, Ning Zhang, Yong Jae Lee, Miao Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Building_a_Mind_Palace_Structuring_Environment-Grounded_Semantic_Graphs_for_Effective_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1421",
      "paper_id": "",
      "title": "Can I Trust Your Answer? Visually Grounded Video Question Answering",
      "authors": "Junbin Xiao, Angela Yao, Yicong Li, Tat-Seng Chua",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Can_I_Trust_Your_Answer_Visually_Grounded_Video_Question_Answering_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_398",
      "paper_id": "",
      "title": "Can Text-to-Video Generation help Video-Language Alignment?",
      "authors": "Luca Zanella, Massimiliano Mancini, Willi Menapace, Sergey Tulyakov, Yiming Wang, Elisa Ricci",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zanella_Can_Text-to-Video_Generation_help_Video-Language_Alignment_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1511",
      "paper_id": "",
      "title": "Can't Make an Omelette Without Breaking Some Eggs: Plausible Action Anticipation Using Large Video-Language Models",
      "authors": "Himangi Mittal, Nakul Agarwal, Shao-Yuan Lo, Kwonjoon Lee",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Mittal_Cant_Make_an_Omelette_Without_Breaking_Some_Eggs_Plausible_Action_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2134",
      "paper_id": "",
      "title": "Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?",
      "authors": "Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, Wanli Ouyang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Cap4Video_What_Can_Auxiliary_Captions_Do_for_Text-Video_Retrieval_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1805",
      "paper_id": "",
      "title": "Cascade Evidential Learning for Open-World Weakly-Supervised Temporal Action Localization",
      "authors": "Mengyuan Chen, Junyu Gao, Changsheng Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Cascade_Evidential_Learning_for_Open-World_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_224",
      "paper_id": "",
      "title": "Cascaded Prediction Network via Segment Tree for Temporal Video Grounding",
      "authors": "Yang Zhao, Zhou Zhao, Zhu Zhang, Zhijie Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Cascaded_Prediction_Network_via_Segment_Tree_for_Temporal_Video_Grounding_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_621",
      "paper_id": "",
      "title": "Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs",
      "authors": "Lucas Ventura, Antoine Yang, Cordelia Schmid, Gül Varol",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ventura_Chapter-Llama_Efficient_Chaptering_in_Hour-Long_Videos_with_LLMs_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1249",
      "paper_id": "",
      "title": "Class Prototypes Based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos",
      "authors": "Rohit Gupta, Anirban Roy, Claire Christensen, Sujeong Kim, Sarah Gerard, Madeline Cincebeaux, Ajay Divakaran, Todd Grindal, Mubarak Shah",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Class_Prototypes_Based_Contrastive_Learning_for_Classifying_Multi-Label_and_Fine-Grained_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_2097",
      "paper_id": "",
      "title": "Clover: Towards a Unified Video-Language Alignment and Fusion Model",
      "authors": "Jingjia Huang, Yinan Li, Jiashi Feng, Xinglong Wu, Xiaoshuai Sun, Rongrong Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Clover_Towards_a_Unified_Video-Language_Alignment_and_Fusion_Model_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_773",
      "paper_id": "",
      "title": "CNVid-3.5M: Build, Filter, and Pre-Train the Large-Scale Public Chinese Video-Text Dataset",
      "authors": "Tian Gan, Qing Wang, Xingning Dong, Xiangyuan Ren, Liqiang Nie, Qingpei Guo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Gan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_1438",
      "paper_id": "",
      "title": "Co-Grounding Networks With Semantic Attention for Referring Expression Comprehension in Videos",
      "authors": "Sijie Song, Xudong Lin, Jiaying Liu, Zongming Guo, Shih-Fu Chang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Co-Grounding_Networks_With_Semantic_Attention_for_Referring_Expression_Comprehension_in_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_114",
      "paper_id": "",
      "title": "Coarse-Fine Networks for Temporal Activity Detection in Videos",
      "authors": "Kumara Kahatapitiya, Michael S. Ryoo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Kahatapitiya_Coarse-Fine_Networks_for_Temporal_Activity_Detection_in_Videos_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_573",
      "paper_id": "",
      "title": "CoLA: Weakly-Supervised Temporal Action Localization With Snippet Contrastive Learning",
      "authors": "Can Zhang, Meng Cao, Dongming Yang, Jie Chen, Yuexian Zou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_CoLA_Weakly-Supervised_Temporal_Action_Localization_With_Snippet_Contrastive_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_2019",
      "paper_id": "",
      "title": "Colar: Effective and Efficient Online Action Detection by Consulting Exemplars",
      "authors": "Le Yang, Junwei Han, Dingwen Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Colar_Effective_and_Efficient_Online_Action_Detection_by_Consulting_Exemplars_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1361",
      "paper_id": "",
      "title": "Collaborative Noisy Label Cleaner: Learning Scene-Aware Trailers for Multi-Modal Highlight Detection in Movies",
      "authors": "Bei Gan, Xiujun Shu, Ruizhi Qiao, Haoqian Wu, Keyu Chen, Hanjun Li, Bo Ren",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Gan_Collaborative_Noisy_Label_Cleaner_Learning_Scene-Aware_Trailers_for_Multi-Modal_Highlight_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_916",
      "paper_id": "",
      "title": "Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor Segmentation",
      "authors": "Tianrui Hui, Shaofei Huang, Si Liu, Zihan Ding, Guanbin Li, Wenguan Wang, Jizhong Han, Fei Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Hui_Collaborative_Spatial-Temporal_Modeling_for_Language-Queried_Video_Actor_Segmentation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_726",
      "paper_id": "",
      "title": "Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding",
      "authors": "Zihang Lin, Chaolei Tan, Jian-Fang Hu, Zhi Jin, Tiancai Ye, Wei-Shi Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_Grounding_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_343",
      "paper_id": "",
      "title": "Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning",
      "authors": "Huabin Liu, Filip Ilievski, Cees G. M. Snoek",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Commonsense_Video_Question_Answering_through_Video-Grounded_Entailment_Tree_Reasoning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1932",
      "paper_id": "",
      "title": "Complex Video Action Reasoning via Learnable Markov Logic Network",
      "authors": "Yang Jin, Linchao Zhu, Yadong Mu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Jin_Complex_Video_Action_Reasoning_via_Learnable_Markov_Logic_Network_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1888",
      "paper_id": "",
      "title": "Compositional Temporal Grounding With Structured Variational Cross-Graph Correspondence Learning",
      "authors": "Juncheng Li, Junlin Xie, Long Qian, Linchao Zhu, Siliang Tang, Fei Wu, Yi Yang, Yueting Zhuang, Xin Eric Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Compositional_Temporal_Grounding_With_Structured_Variational_Cross-Graph_Correspondence_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_232",
      "paper_id": "",
      "title": "Compositional Video Understanding with Spatiotemporal Structure-based Transformers",
      "authors": "Hoyeoung Yun, Jinwoo Ahn, Minseo Kim, Eun-Sol Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yun_Compositional_Video_Understanding_with_Spatiotemporal_Structure-based_Transformers_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_248",
      "paper_id": "",
      "title": "Connecting Vision and Language With Video Localized Narratives",
      "authors": "Paul Voigtlaender, Soravit Changpinyo, Jordi Pont-Tuset, Radu Soricut, Vittorio Ferrari",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Voigtlaender_Connecting_Vision_and_Language_With_Video_Localized_Narratives_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_378",
      "paper_id": "",
      "title": "Context-Aware Biaffine Localizing Network for Temporal Sentence Grounding",
      "authors": "Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou, Yu Cheng, Wei Wei, Zichuan Xu, Yulai Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Context-Aware_Biaffine_Localizing_Network_for_Temporal_Sentence_Grounding_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_1067",
      "paper_id": "",
      "title": "Context-Aware Sequence Alignment Using 4D Skeletal Augmentation",
      "authors": "Taein Kwon, Bugra Tekin, Siyu Tang, Marc Pollefeys",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kwon_Context-Aware_Sequence_Alignment_Using_4D_Skeletal_Augmentation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2119",
      "paper_id": "",
      "title": "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
      "authors": "Zhanzhong Pang, Fadime Sener, Angela Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Pang_Context-Enhanced_Memory-Refined_Transformer_for_Online_Action_Detection_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_606",
      "paper_id": "",
      "title": "Context-Guided Spatio-Temporal Video Grounding",
      "authors": "Xin Gu, Heng Fan, Yan Huang, Tiejian Luo, Libo Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Gu_Context-Guided_Spatio-Temporal_Video_Grounding_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1916",
      "paper_id": "",
      "title": "Contextual AD Narration with Interleaved Multimodal Sequence",
      "authors": "Hanlin Wang, Zhan Tong, Kecheng Zheng, Yujun Shen, Limin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Contextual_AD_Narration_with_Interleaved_Multimodal_Sequence_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_240",
      "paper_id": "",
      "title": "Contextualized Spatio-Temporal Contrastive Learning With Self-Supervision",
      "authors": "Liangzhe Yuan, Rui Qian, Yin Cui, Boqing Gong, Florian Schroff, Ming-Hsuan Yang, Hartwig Adam, Ting Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yuan_Contextualized_Spatio-Temporal_Contrastive_Learning_With_Self-Supervision_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1653",
      "paper_id": "",
      "title": "Contrastive Conditional Neural Processes",
      "authors": "Zesheng Ye, Lina Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ye_Contrastive_Conditional_Neural_Processes_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1611",
      "paper_id": "",
      "title": "Contrastive Learning for Space-Time Correspondence via Self-Cycle Consistency",
      "authors": "Jeany Son",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Son_Contrastive_Learning_for_Space-Time_Correspondence_via_Self-Cycle_Consistency_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1181",
      "paper_id": "",
      "title": "Contrastive Learning for Unsupervised Video Highlight Detection",
      "authors": "Taivanbat Badamdorj, Mrigank Rochan, Yang Wang, Li Cheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Badamdorj_Contrastive_Learning_for_Unsupervised_Video_Highlight_Detection_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1576",
      "paper_id": "",
      "title": "COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval",
      "authors": "Haoyu Lu, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu, Ji-Rong Wen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_COTS_Collaborative_Two-Stream_Vision-Language_Pre-Training_Model_for_Cross-Modal_Retrieval_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1888",
      "paper_id": "",
      "title": "CPR-Coach: Recognizing Composite Error Actions based on Single-class Training",
      "authors": "Shunli Wang, Shuaibing Wang, Dingkang Yang, Mingcheng Li, Haopeng Kuang, Xiao Zhao, Liuzhen Su, Peng Zhai, Lihua Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_CPR-Coach_Recognizing_Composite_Error_Actions_based_on_Single-class_Training_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_794",
      "paper_id": "",
      "title": "Cross-Architecture Self-Supervised Video Representation Learning",
      "authors": "Sheng Guo, Zihua Xiong, Yujie Zhong, Limin Wang, Xiaobo Guo, Bing Han, Weilin Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Cross-Architecture_Self-Supervised_Video_Representation_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_10",
      "paper_id": "",
      "title": "Cross-modal Causal Relation Alignment for Video Question Grounding",
      "authors": "Weixing Chen, Yang Liu, Binglin Chen, Jiandong Su, Yongsen Zheng, Liang Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1041",
      "paper_id": "",
      "title": "Cross-Modal Representation Learning for Zero-Shot Action Recognition",
      "authors": "Chung-Ching Lin, Kevin Lin, Lijuan Wang, Zicheng Liu, Linjie Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_Cross-Modal_Representation_Learning_for_Zero-Shot_Action_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1759",
      "paper_id": "",
      "title": "Cross-Model Pseudo-Labeling for Semi-Supervised Action Recognition",
      "authors": "Yinghao Xu, Fangyun Wei, Xiao Sun, Ceyuan Yang, Yujun Shen, Bo Dai, Bolei Zhou, Stephen Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Cross-Model_Pseudo-Labeling_for_Semi-Supervised_Action_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2455",
      "paper_id": "",
      "title": "CSTA: CNN-based Spatiotemporal Attention for Video Summarization",
      "authors": "Jaewon Son, Jaehun Park, Kwangsu Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Son_CSTA_CNN-based_Spatiotemporal_Attention_for_Video_Summarization_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2019",
      "paper_id": "",
      "title": "DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos",
      "authors": "Zijia Lu, A S M Iftekhar, Gaurav Mittal, Tianjian Meng, Xiawei Wang, Cheng Zhao, Rohith Kukkala, Ehsan Elhamifar, Mei Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lu_DeCafNet_Delegate_and_Conquer_for_Efficient_Temporal_Grounding_in_Long_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_184",
      "paper_id": "",
      "title": "DeCo: Decomposition and Reconstruction for Compositional Temporal Grounding via Coarse-To-Fine Contrastive Ranking",
      "authors": "Lijin Yang, Quan Kong, Hsuan-Kung Yang, Wadim Kehl, Yoichi Sato, Norimasa Kobori",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_DeCo_Decomposition_and_Reconstruction_for_Compositional_Temporal_Grounding_via_Coarse-To-Fine_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1415",
      "paper_id": "",
      "title": "Decomposed Cross-Modal Distillation for RGB-Based Temporal Action Detection",
      "authors": "Pilhyeon Lee, Taeoh Kim, Minho Shim, Dongyoon Wee, Hyeran Byun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Decomposed_Cross-Modal_Distillation_for_RGB-Based_Temporal_Action_Detection_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_831",
      "paper_id": "",
      "title": "Decoupled Motion Expression Video Segmentation",
      "authors": "Hao Fang, Runmin Cong, Xiankai Lu, Xiaofei Zhou, Sam Kwong, Wei Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Fang_Decoupled_Motion_Expression_Video_Segmentation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1589",
      "paper_id": "",
      "title": "Decoupling and Recoupling Spatiotemporal Representation for RGB-D-Based Motion Recognition",
      "authors": "Benjia Zhou, Pichao Wang, Jun Wan, Yanyan Liang, Fan Wang, Du Zhang, Zhen Lei, Hao Li, Rong Jin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Decoupling_and_Recoupling_Spatiotemporal_Representation_for_RGB-D-Based_Motion_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1959",
      "paper_id": "",
      "title": "Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation",
      "authors": "Shuting He, Henghui Ding",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/He_Decoupling_Static_and_Hierarchical_Motion_Perception_for_Referring_Video_Segmentation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_877",
      "paper_id": "",
      "title": "Deep Analysis of CNN-Based Spatio-Temporal Representations for Action Recognition",
      "authors": "Chun-Fu Richard Chen, Rameswar Panda, Kandan Ramakrishnan, Rogerio Feris, John Cohn, Aude Oliva, Quanfu Fan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Deep_Analysis_of_CNN-Based_Spatio-Temporal_Representations_for_Action_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_458",
      "paper_id": "",
      "title": "Deformable Video Transformer",
      "authors": "Jue Wang, Lorenzo Torresani",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Deformable_Video_Transformer_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1591",
      "paper_id": "",
      "title": "DejaVid: Encoder-Agnostic Learned Temporal Matching for Video Classification",
      "authors": "Darryl Ho, Samuel Madden",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ho_DejaVid_Encoder-Agnostic_Learned_Temporal_Matching_for_Video_Classification_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1303",
      "paper_id": "",
      "title": "Detector-Free Weakly Supervised Group Activity Recognition",
      "authors": "Dongkeun Kim, Jinsung Lee, Minsu Cho, Suha Kwak",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_Detector-Free_Weakly_Supervised_Group_Activity_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1427",
      "paper_id": "",
      "title": "Detours for Navigating Instructional Videos",
      "authors": "Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, Kristen Grauman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ashutosh_Detours_for_Navigating_Instructional_Videos_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_388",
      "paper_id": "",
      "title": "DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement",
      "authors": "Hao Wu, Huabin Liu, Yu Qiao, Xiao Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_DIBS_Enhancing_Dense_Video_Captioning_with_Unlabeled_Videos_via_Pseudo_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_951",
      "paper_id": "",
      "title": "DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer",
      "authors": "Ho-Joong Kim, Yearang Lee, Jung-Ho Hong, Seong-Whan Lee",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_DiGIT_Multi-Dilated_Gated_Encoder_and_Central-Adjacent_Region_Integrated_Decoder_for_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_850",
      "paper_id": "",
      "title": "DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition",
      "authors": "Thanh-Dat Truong, Quoc-Huy Bui, Chi Nhan Duong, Han-Seok Seo, Son Lam Phung, Xin Li, Khoa Luu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Truong_DirecFormer_A_Directed_Attention_in_Transformer_Approach_to_Robust_Action_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_219",
      "paper_id": "",
      "title": "Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering",
      "authors": "Chuanqi Zang, Hanqing Wang, Mingtao Pei, Wei Liang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zang_Discovering_the_Real_Association_Multimodal_Causal_Reasoning_in_Video_Question_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2770",
      "paper_id": "",
      "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval",
      "authors": "Leqi Shen, Guoqiang Gong, Tianxiang Hao, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, Jungong Han, Guiguang Ding",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Shen_DiscoVLA_Discrepancy_Reduction_in_Vision_Language_and_Alignment_for_Parameter-Efficient_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_863",
      "paper_id": "",
      "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
      "authors": "Rui Qian, Shuangrui Ding, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Qian_Dispider_Enabling_Video_LLMs_with_Active_Real-Time_Interaction_via_Disentangled_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_920",
      "paper_id": "",
      "title": "Distilling Vision-Language Models on Millions of Videos",
      "authors": "Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu, Hui Miao, Florian Schroff, Hartwig Adam, Ting Liu, Boqing Gong, Philipp Krahenbuhl, Liangzhe Yuan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Distilling_Vision-Language_Models_on_Millions_of_Videos_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_38",
      "paper_id": "",
      "title": "Distilling Vision-Language Pre-Training To Collaborate With Weakly-Supervised Temporal Action Localization",
      "authors": "Chen Ju, Kunhao Zheng, Jinxiang Liu, Peisen Zhao, Ya Zhang, Jianlong Chang, Qi Tian, Yanfeng Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ju_Distilling_Vision-Language_Pre-Training_To_Collaborate_With_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2238",
      "paper_id": "",
      "title": "DistinctAD: Distinctive Audio Description Generation in Contexts",
      "authors": "Bo Fang, Wenhao Wu, Qiangqiang Wu, Yuxin Song, Antoni B. Chan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2562",
      "paper_id": "",
      "title": "DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos",
      "authors": "Lorenzo Mur-Labadia, Josechu Guerrero, Ruben Martinez-Cantin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_549",
      "paper_id": "",
      "title": "Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval",
      "authors": "Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, Seong Tae Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Do_You_Remember_Dense_Video_Captioning_with_Cross-Modal_Memory_Retrieval_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_685",
      "paper_id": "",
      "title": "DropMAE: Masked Autoencoders With Spatial-Attention Dropout for Tracking Tasks",
      "authors": "Qiangqiang Wu, Tianyu Yang, Ziquan Liu, Baoyuan Wu, Ying Shan, Antoni B. Chan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_DropMAE_Masked_Autoencoders_With_Spatial-Attention_Dropout_for_Tracking_Tasks_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_118",
      "paper_id": "",
      "title": "DrVideo: Document Retrieval Based Long Video Understanding",
      "authors": "Ziyu Ma, Chenhui Gou, Hengcan Shi, Bin Sun, Shutao Li, Hamid Rezatofighi, Jianfei Cai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ma_DrVideo_Document_Retrieval_Based_Long_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2657",
      "paper_id": "",
      "title": "DTOS: Dynamic Time Object Sensing with Large Multimodal Model",
      "authors": "Jirui Tian, Jinrong Zhang, Shenglan Liu, Luhao Xu, Zhixiong Huang, Gao Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_DTOS_Dynamic_Time_Object_Sensing_with_Large_Multimodal_Model_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_531",
      "paper_id": "",
      "title": "Dual DETRs for Multi-Label Temporal Action Detection",
      "authors": "Yuhan Zhu, Guozhen Zhang, Jing Tan, Gangshan Wu, Limin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Dual_DETRs_for_Multi-Label_Temporal_Action_Detection_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1008",
      "paper_id": "",
      "title": "Dual-AI: Dual-Path Actor Interaction Learning for Group Activity Recognition",
      "authors": "Mingfei Han, David Junhao Zhang, Yali Wang, Rui Yan, Lina Yao, Xiaojun Chang, Yu Qiao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Dual-AI_Dual-Path_Actor_Interaction_Learning_for_Group_Activity_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_363",
      "paper_id": "",
      "title": "Dual-Path Adaptation From Image to Video Transformers",
      "authors": "Jungin Park, Jiyoung Lee, Kwanghoon Sohn",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Dual-Path_Adaptation_From_Image_to_Video_Transformers_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2704",
      "paper_id": "",
      "title": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video Understanding",
      "authors": "Yudong Han, Qingpei Guo, Liyuan Pan, Liu Liu, Yu Guan, Ming Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Han_DynFocus_Dynamic_Cooperative_Network_Empowers_LLMs_with_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_683",
      "paper_id": "",
      "title": "ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark",
      "authors": "Ronghao Dang, Yuqian Yuan, Wenqi Zhang, Yifei Xin, Boqiang Zhang, Long Li, Liuyi Wang, Qinyang Zeng, Xin Li, Lidong Bing",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Dang_ECBench_Can_Multi-modal_Foundation_Models_Understand_the_Egocentric_World_A_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1016",
      "paper_id": "",
      "title": "EchoTraffic: Enhancing Traffic Anomaly Understanding with Audio-Visual Insights",
      "authors": "Zhenghao Xing, Hao Chen, Binzhu Xie, Jiaqi Xu, Ziyu Guo, Xuemiao Xu, Jianye Hao, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xing_EchoTraffic_Enhancing_Traffic_Anomaly_Understanding_with_Audio-Visual_Insights_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1476",
      "paper_id": "",
      "title": "Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment",
      "authors": "Angchi Xu, Wei-Shi Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Efficient_and_Effective_Weakly-Supervised_Action_Segmentation_via_Action-Transition-Aware_Boundary_Alignment_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2648",
      "paper_id": "",
      "title": "Efficient Motion-Aware Video MLLM",
      "authors": "Zijia Zhao, Yuqi Huo, Tongtian Yue, Longteng Guo, Haoyu Lu, Bingning Wang, Weipeng Chen, Jing Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_Efficient_Motion-Aware_Video_MLLM_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1806",
      "paper_id": "",
      "title": "Efficient Movie Scene Detection Using State-Space Transformers",
      "authors": "Md Mohaiminul Islam, Mahmudul Hasan, Kishan Shamsundar Athrey, Tony Braskich, Gedas Bertasius",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Islam_Efficient_Movie_Scene_Detection_Using_State-Space_Transformers_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1950",
      "paper_id": "",
      "title": "Efficient Transfer Learning for Video-language Foundation Models",
      "authors": "Haoxing Chen, Zizheng Huang, Yan Hong, Yanshuo Wang, Zhongcai Lyu, Zhuoer Xu, Jun Lan, Zhangxuan Gu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Efficient_Transfer_Learning_for_Video-language_Foundation_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1775",
      "paper_id": "",
      "title": "Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives",
      "authors": "Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin J Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas Bertasius, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C.V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, Michael Wray",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Grauman_Ego-Exo4D_Understanding_Skilled_Human_Activity_from_First-_and_Third-Person_Perspectives_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_458",
      "paper_id": "",
      "title": "Ego-Exo: Transferring Visual Representations From Third-Person to First-Person Videos",
      "authors": "Yanghao Li, Tushar Nagarajan, Bo Xiong, Kristen Grauman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Ego-Exo_Transferring_Visual_Representations_From_Third-Person_to_First-Person_Videos_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_1637",
      "paper_id": "",
      "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
      "authors": "Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina González, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jáchym Kolář, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbeláez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, Jitendra Malik",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1470",
      "paper_id": "",
      "title": "Egocentric Video Task Translation",
      "authors": "Zihui Xue, Yale Song, Kristen Grauman, Lorenzo Torresani",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_Egocentric_Video_Task_Translation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2373",
      "paper_id": "",
      "title": "EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World",
      "authors": "Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, Baoqi Pei, Hongjie Zhang, Lu Dong, Yali Wang, Limin Wang, Yu Qiao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_EgoExoLearn_A_Dataset_for_Bridging_Asynchronous_Ego-_and_Exo-centric_View_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_613",
      "paper_id": "",
      "title": "EgoLife: Towards Egocentric Life Assistant",
      "authors": "Jingkang Yang, Shuai Liu, Hongming Guo, Yuhao Dong, Xiamengwei Zhang, Sicheng Zhang, Pengyun Wang, Zitang Zhou, Binzhu Xie, Ziyue Wang, Bei Ouyang, Zhengyu Lin, Marco Cominelli, Zhongang Cai, Bo Li, Yuanhan Zhang, Peiyuan Zhang, Fangzhou Hong, Joerg Widmer, Francesco Gringoli, Lei Yang, Ziwei Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_EgoLife_Towards_Egocentric_Life_Assistant_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1669",
      "paper_id": "",
      "title": "EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering",
      "authors": "Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, Angela Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_EgoTextVQA_Towards_Egocentric_Scene-Text_Aware_Video_Question_Answering_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_983",
      "paper_id": "",
      "title": "EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models",
      "authors": "Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, Yang Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_EgoThink_Evaluating_First-Person_Perspective_Thinking_Capability_of_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_778",
      "paper_id": "",
      "title": "Embracing Uncertainty: Decoupling and De-Bias for Robust Temporal Grounding",
      "authors": "Hao Zhou, Chongyang Zhang, Yan Luo, Yanjun Chen, Chuanping Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Embracing_Uncertainty_Decoupling_and_De-Bias_for_Robust_Temporal_Grounding_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_1469",
      "paper_id": "",
      "title": "End-to-End Compressed Video Representation Learning for Generic Event Boundary Detection",
      "authors": "Congcong Li, Xinyao Wang, Longyin Wen, Dexiang Hong, Tiejian Luo, Libo Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_End-to-End_Compressed_Video_Representation_Learning_for_Generic_Event_Boundary_Detection_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_34",
      "paper_id": "",
      "title": "End-to-End Generative Pretraining for Multimodal Video Captioning",
      "authors": "Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Seo_End-to-End_Generative_Pretraining_for_Multimodal_Video_Captioning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_347",
      "paper_id": "",
      "title": "End-to-End Referring Video Object Segmentation With Multimodal Transformers",
      "authors": "Adam Botach, Evgenii Zheltonozhskii, Chaim Baskin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Botach_End-to-End_Referring_Video_Object_Segmentation_With_Multimodal_Transformers_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1729",
      "paper_id": "",
      "title": "End-to-End Semi-Supervised Learning for Video Action Detection",
      "authors": "Akash Kumar, Yogesh Singh Rawat",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kumar_End-to-End_Semi-Supervised_Learning_for_Video_Action_Detection_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2614",
      "paper_id": "",
      "title": "End-to-End Spatio-Temporal Action Localisation with Video Transformers",
      "authors": "Alexey A. Gritsenko, Xuehan Xiong, Josip Djolonga, Mostafa Dehghani, Chen Sun, Mario Lucic, Cordelia Schmid, Anurag Arnab",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Gritsenko_End-to-End_Spatio-Temporal_Action_Localisation_with_Video_Transformers_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2617",
      "paper_id": "",
      "title": "End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames",
      "authors": "Shuming Liu, Chen-Lin Zhang, Chen Zhao, Bernard Ghanem",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_End-to-End_Temporal_Action_Detection_with_1B_Parameters_Across_1000_Frames_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_680",
      "paper_id": "",
      "title": "Enhanced Motion-Text Alignment for Image-to-Video Transfer Learning",
      "authors": "Wei Zhang, Chaoqun Wan, Tongliang Liu, Xinmei Tian, Xu Shen, Jieping Ye",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Enhanced_Motion-Text_Alignment_for_Image-to-Video_Transfer_Learning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_59",
      "paper_id": "",
      "title": "Enhancing Video-LLM Reasoning via Agent-of-Thoughts Distillation",
      "authors": "Yudi Shi, Shangzhe Di, Qirui Chen, Weidi Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Shi_Enhancing_Video-LLM_Reasoning_via_Agent-of-Thoughts_Distillation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_986",
      "paper_id": "",
      "title": "Enlarging Instance-Specific and Class-Specific Information for Open-Set Action Recognition",
      "authors": "Jun Cen, Shiwei Zhang, Xiang Wang, Yixuan Pei, Zhiwu Qing, Yingya Zhang, Qifeng Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cen_Enlarging_Instance-Specific_and_Class-Specific_Information_for_Open-Set_Action_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1555",
      "paper_id": "",
      "title": "Episodic Memory Question Answering",
      "authors": "Samyak Datta, Sameer Dharur, Vincent Cartillier, Ruta Desai, Mukul Khanna, Dhruv Batra, Devi Parikh",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Datta_Episodic_Memory_Question_Answering_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_959",
      "paper_id": "",
      "title": "Error Detection in Egocentric Procedural Task Videos",
      "authors": "Shih-Po Lee, Zijia Lu, Zekun Zhang, Minh Hoai, Ehsan Elhamifar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Error_Detection_in_Egocentric_Procedural_Task_Videos_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_468",
      "paper_id": "",
      "title": "Event-Equalized Dense Video Captioning",
      "authors": "Kangyi Wu, Pengna Li, Jingwen Fu, Yizhe Li, Yang Wu, Yuhan Liu, Jinjun Wang, Sanping Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Event-Equalized_Dense_Video_Captioning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_640",
      "paper_id": "",
      "title": "EventGPT: Event Stream Understanding with Multimodal Large Language Models",
      "authors": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Xin Meng, Fei Richard Yu, Xiangyang Ji, Ming Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_EventGPT_Event_Stream_Understanding_with_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_118",
      "paper_id": "",
      "title": "Everything at Once - Multi-Modal Fusion Transformer for Video Retrieval",
      "authors": "Nina Shvetsova, Brian Chen, Andrew Rouditchenko, Samuel Thomas, Brian Kingsbury, Rogerio S. Feris, David Harwath, James Glass, Hilde Kuehne",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Shvetsova_Everything_at_Once_-_Multi-Modal_Fusion_Transformer_for_Video_Retrieval_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2572",
      "paper_id": "",
      "title": "ExpertAF: Expert Actionable Feedback from Video",
      "authors": "Kumar Ashutosh, Tushar Nagarajan, Georgios Pavlakos, Kris Kitani, Kristen Grauman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ashutosh_ExpertAF_Expert_Actionable_Feedback_from_Video_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1478",
      "paper_id": "",
      "title": "Exploring Denoised Cross-Video Contrast for Weakly-Supervised Temporal Action Localization",
      "authors": "Jingjing Li, Tianyu Yang, Wei Ji, Jue Wang, Li Cheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Exploring_Denoised_Cross-Video_Contrast_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1155",
      "paper_id": "",
      "title": "Exploring the Effect of Primitives for Compositional Generalization in Vision-and-Language",
      "authors": "Chuanhao Li, Zhen Li, Chenchen Jing, Yunde Jia, Yuwei Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Exploring_the_Effect_of_Primitives_for_Compositional_Generalization_in_Vision-and-Language_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_926",
      "paper_id": "",
      "title": "FACT: Frame-Action Cross-Attention Temporal Modeling for Efficient Action Segmentation",
      "authors": "Zijia Lu, Ehsan Elhamifar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_FACT_Frame-Action_Cross-Attention_Temporal_Modeling_for_Efficient_Action_Segmentation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_185",
      "paper_id": "",
      "title": "Fast and Unsupervised Action Boundary Detection for Action Segmentation",
      "authors": "Zexing Du, Xue Wang, Guoqing Zhou, Qing Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Du_Fast_and_Unsupervised_Action_Boundary_Detection_for_Action_Segmentation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_576",
      "paper_id": "",
      "title": "Few-Shot Referring Relationships in Videos",
      "authors": "Yogesh Kumar, Anand Mishra",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kumar_Few-Shot_Referring_Relationships_in_Videos_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_1449",
      "paper_id": "",
      "title": "Few-Shot Transformation of Common Actions Into Time and Space",
      "authors": "Pengwan Yang, Pascal Mettes, Cees G. M. Snoek",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Few-Shot_Transformation_of_Common_Actions_Into_Time_and_Space_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1018",
      "paper_id": "",
      "title": "Fine-Grained Audible Video Description",
      "authors": "Xuyang Shen, Dong Li, Jinxing Zhou, Zhen Qin, Bowen He, Xiaodong Han, Aixuan Li, Yuchao Dai, Lingpeng Kong, Meng Wang, Yu Qiao, Yiran Zhong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Fine-Grained_Audible_Video_Description_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_564",
      "paper_id": "",
      "title": "Fine-Grained Temporal Contrastive Learning for Weakly-Supervised Temporal Action Localization",
      "authors": "Junyu Gao, Mengyuan Chen, Changsheng Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_Fine-Grained_Temporal_Contrastive_Learning_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_2113",
      "paper_id": "",
      "title": "Fine-Tuned CLIP Models Are Efficient Video Learners",
      "authors": "Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Rasheed_Fine-Tuned_CLIP_Models_Are_Efficient_Video_Learners_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_329",
      "paper_id": "",
      "title": "FineDiving: A Fine-Grained Dataset for Procedure-Aware Action Quality Assessment",
      "authors": "Jinglin Xu, Yongming Rao, Xumin Yu, Guangyi Chen, Jie Zhou, Jiwen Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_FineDiving_A_Fine-Grained_Dataset_for_Procedure-Aware_Action_Quality_Assessment_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1282",
      "paper_id": "",
      "title": "FineParser: A Fine-grained Spatio-temporal Action Parser for Human-centric Action Quality Assessment",
      "authors": "Jinglin Xu, Sibo Yin, Guohao Zhao, Zishuo Wang, Yuxin Peng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_FineParser_A_Fine-grained_Spatio-temporal_Action_Parser_for_Human-centric_Action_Quality_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_819",
      "paper_id": "",
      "title": "FineSports: A Multi-person Hierarchical Sports Video Dataset for Fine-grained Action Understanding",
      "authors": "Jinglin Xu, Guohao Zhao, Sibo Yin, Wenhao Zhou, Yuxin Peng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_FineSports_A_Multi-person_Hierarchical_Sports_Video_Dataset_for_Fine-grained_Action_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_639",
      "paper_id": "",
      "title": "Flexible Frame Selection for Efficient Video Reasoning",
      "authors": "Shyamal Buch, Arsha Nagrani, Anurag Arnab, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Buch_Flexible_Frame_Selection_for_Efficient_Video_Reasoning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_4",
      "paper_id": "",
      "title": "Frame Flexible Network",
      "authors": "Yitian Zhang, Yue Bai, Chang Liu, Huan Wang, Sheng Li, Yun Fu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Frame_Flexible_Network_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1397",
      "paper_id": "",
      "title": "Frame-Wise Action Representations for Long Videos via Sequence Contrastive Learning",
      "authors": "Minghao Chen, Fangyun Wei, Chong Li, Deng Cai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Frame-Wise_Action_Representations_for_Long_Videos_via_Sequence_Contrastive_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_953",
      "paper_id": "",
      "title": "FrameExit: Conditional Early Exiting for Efficient Video Recognition",
      "authors": "Amir Ghodrati, Babak Ehteshami Bejnordi, Amirhossein Habibian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Ghodrati_FrameExit_Conditional_Early_Exiting_for_Efficient_Video_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_749",
      "paper_id": "",
      "title": "From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding",
      "authors": "Yong-Lu Li, Xiaoqian Wu, Xinpeng Liu, Zehao Wang, Yiming Dou, Yikun Ji, Junyi Zhang, Yixing Li, Xudong Lu, Jingru Tan, Cewu Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_From_Isolated_Islands_to_Pangea_Unifying_Semantic_Space_for_Human_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_2042",
      "paper_id": "",
      "title": "From Representation to Reasoning: Towards Both Evidence and Commonsense Reasoning for Video Question-Answering",
      "authors": "Jiangtong Li, Li Niu, Liqing Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_From_Representation_to_Reasoning_Towards_Both_Evidence_and_Commonsense_Reasoning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_177",
      "paper_id": "",
      "title": "FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding",
      "authors": "Rong Gao, Xin Liu, Zhuozhao Hu, Bohao Xing, Baiqiang Xia, Zitong Yu, Heikki Kälviäinen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Gao_FSBench_A_Figure_Skating_Benchmark_for_Advancing_Artistic_Sports_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1890",
      "paper_id": "",
      "title": "Future Transformer for Long-Term Action Anticipation",
      "authors": "Dayoung Gong, Joonseok Lee, Manjin Kim, Seong Jong Ha, Minsu Cho",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Gong_Future_Transformer_for_Long-Term_Action_Anticipation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_206",
      "paper_id": "",
      "title": "GateHUB: Gated History Unit With Background Suppression for Online Action Detection",
      "authors": "Junwen Chen, Gaurav Mittal, Ye Yu, Yu Kong, Mei Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_GateHUB_Gated_History_Unit_With_Background_Suppression_for_Online_Action_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2248",
      "paper_id": "",
      "title": "Gazing Into Missteps: Leveraging Eye-Gaze for Unsupervised Mistake Detection in Egocentric Videos of Skilled Human Activities",
      "authors": "Michele Mazzamuto, Antonino Furnari, Yoichi Sato, Giovanni Maria Farinella",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Mazzamuto_Gazing_Into_Missteps_Leveraging_Eye-Gaze_for_Unsupervised_Mistake_Detection_in_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1637",
      "paper_id": "",
      "title": "Global2Local: Efficient Structure Search for Video Action Segmentation",
      "authors": "Shang-Hua Gao, Qi Han, Zhong-Yu Li, Pai Peng, Liang Wang, Ming-Ming Cheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_Global2Local_Efficient_Structure_Search_for_Video_Action_Segmentation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1963",
      "paper_id": "",
      "title": "GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation",
      "authors": "Lang Lin, Xueyang Yu, Ziqi Pang, Yu-Xiong Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_GLUS_Global-Local_Reasoning_Unified_into_A_Single_Large_Language_Model_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_580",
      "paper_id": "",
      "title": "Gradient Forward-Propagation for Large-Scale Temporal Video Modelling",
      "authors": "Mateusz Malinowski, Dimitrios Vytiniotis, Grzegorz Swirszcz, Viorica Patraucean, Joao Carreira",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Malinowski_Gradient_Forward-Propagation_for_Large-Scale_Temporal_Video_Modelling_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1099",
      "paper_id": "",
      "title": "Graph Representation for Order-Aware Visual Transformation",
      "authors": "Yue Qiu, Yanjun Sun, Fumiya Matsuzawa, Kenji Iwata, Hirokatsu Kataoka",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_Graph_Representation_for_Order-Aware_Visual_Transformation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_438",
      "paper_id": "",
      "title": "Graph-Based High-Order Relation Modeling for Long-Term Action Recognition",
      "authors": "Jiaming Zhou, Kun-Yu Lin, Haoxin Li, Wei-Shi Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Graph-Based_High-Order_Relation_Modeling_for_Long-Term_Action_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_691",
      "paper_id": "",
      "title": "Grounded Question-Answering in Long Egocentric Videos",
      "authors": "Shangzhe Di, Weidi Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Di_Grounded_Question-Answering_in_Long_Egocentric_Videos_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1962",
      "paper_id": "",
      "title": "Group Contextualization for Video Recognition",
      "authors": "Yanbin Hao, Hao Zhang, Chong-Wah Ngo, Xiangnan He",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Hao_Group_Contextualization_for_Video_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2377",
      "paper_id": "",
      "title": "Groupwise Query Specialization and Quality-Aware Multi-Assignment for Transformer-based Visual Relationship Detection",
      "authors": "Jongha Kim, Jihwan Park, Jinyoung Park, Jinyoung Kim, Sehyung Kim, Hyunwoo J. Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Groupwise_Query_Specialization_and_Quality-Aware_Multi-Assignment_for_Transformer-based_Visual_Relationship_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_908",
      "paper_id": "",
      "title": "HD-EPIC: A Highly-Detailed Egocentric Video Dataset",
      "authors": "Toby Perrett, Ahmad Darkhalil, Saptarshi Sinha, Omar Emara, Sam Pollard, Kranti Kumar Parida, Kaiting Liu, Prajwal Gatti, Siddhant Bansal, Kevin Flanagan, Jacob Chalk, Zhifan Zhu, Rhodri Guerrier, Fahd Abdelazim, Bin Zhu, Davide Moltisanti, Michael Wray, Hazel Doughty, Dima Damen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Perrett_HD-EPIC_A_Highly-Detailed_Egocentric_Video_Dataset_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1253",
      "paper_id": "",
      "title": "Hierarchical Modular Network for Video Captioning",
      "authors": "Hanhua Ye, Guorong Li, Yuankai Qi, Shuhui Wang, Qingming Huang, Ming-Hsuan Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ye_Hierarchical_Modular_Network_for_Video_Captioning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_851",
      "paper_id": "",
      "title": "Hierarchical Self-Supervised Representation Learning for Movie Understanding",
      "authors": "Fanyi Xiao, Kaustav Kundu, Joseph Tighe, Davide Modolo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xiao_Hierarchical_Self-Supervised_Representation_Learning_for_Movie_Understanding_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_367",
      "paper_id": "",
      "title": "Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding",
      "authors": "Chaolei Tan, Zihang Lin, Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Hierarchical_Semantic_Correspondence_Networks_for_Video_Paragraph_Grounding_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_117",
      "paper_id": "",
      "title": "Hierarchical Video-Moment Retrieval and Step-Captioning",
      "authors": "Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, Mohit Bansal",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zala_Hierarchical_Video-Moment_Retrieval_and_Step-Captioning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2127",
      "paper_id": "",
      "title": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding",
      "authors": "Shehreen Azad, Vibhav Vineet, Yogesh Singh Rawat",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Azad_HierarQ_Task-Aware_Hierarchical_Q-Former_for_Enhanced_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_823",
      "paper_id": "",
      "title": "HierVL: Learning Hierarchical Video-Language Embeddings",
      "authors": "Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, Kristen Grauman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ashutosh_HierVL_Learning_Hierarchical_Video-Language_Embeddings_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_32",
      "paper_id": "",
      "title": "HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding",
      "authors": "Trong-Thuan Nguyen, Pha Nguyen, Khoa Luu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_HIG_Hierarchical_Interlacement_Graph_Approach_to_Scene_Graph_Generation_in_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_589",
      "paper_id": "",
      "title": "Holistic Features are almost Sufficient for Text-to-Video Retrieval",
      "authors": "Kaibin Tian, Ruixiang Zhao, Zijie Xin, Bangxiang Lan, Xirong Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Tian_Holistic_Features_are_almost_Sufficient_for_Text-to-Video_Retrieval_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2005",
      "paper_id": "",
      "title": "Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity",
      "authors": "Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Xiaonan Huang, Changxin Gao, Shanjun Zhang, Li Yu, Nong Sang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_806",
      "paper_id": "",
      "title": "Home Action Genome: Cooperative Compositional Action Understanding",
      "authors": "Nishant Rai, Haofeng Chen, Jingwei Ji, Rishi Desai, Kazuki Kozuka, Shun Ishizaka, Ehsan Adeli, Juan Carlos Niebles",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Rai_Home_Action_Genome_Cooperative_Compositional_Action_Understanding_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_2066",
      "paper_id": "",
      "title": "How Can Objects Help Action Recognition?",
      "authors": "Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1949",
      "paper_id": "",
      "title": "How Do You Do It? Fine-Grained Action Understanding With Pseudo-Adverbs",
      "authors": "Hazel Doughty, Cees G. M. Snoek",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Doughty_How_Do_You_Do_It_Fine-Grained_Action_Understanding_With_Pseudo-Adverbs_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1393",
      "paper_id": "",
      "title": "Hybrid Relation Guided Set Matching for Few-Shot Action Recognition",
      "authors": "Xiang Wang, Shiwei Zhang, Zhiwu Qing, Mingqian Tang, Zhengrong Zuo, Changxin Gao, Rong Jin, Nong Sang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Hybrid_Relation_Guided_Set_Matching_for_Few-Shot_Action_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_389",
      "paper_id": "",
      "title": "HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation",
      "authors": "Trong-Thuan Nguyen, Pha Nguyen, Jackson Cothren, Alper Yilmaz, Khoa Luu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Nguyen_HyperGLM_HyperGraph_for_Video_Scene_Graph_Generation_and_Anticipation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_478",
      "paper_id": "",
      "title": "Improving Video Model Transfer With Dynamic Representation Learning",
      "authors": "Yi Li, Nuno Vasconcelos",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Improving_Video_Model_Transfer_With_Dynamic_Representation_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1844",
      "paper_id": "",
      "title": "Improving Weakly Supervised Temporal Action Localization by Bridging Train-Test Gap in Pseudo Labels",
      "authors": "Jingqiu Zhou, Linjiang Huang, Liang Wang, Si Liu, Hongsheng Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Improving_Weakly_Supervised_Temporal_Action_Localization_by_Bridging_Train-Test_Gap_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1201",
      "paper_id": "",
      "title": "IntentVizor: Towards Generic Query Guided Interactive Video Summarization",
      "authors": "Guande Wu, Jianzhe Lin, Claudio T. Silva",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_IntentVizor_Towards_Generic_Query_Guided_Interactive_Video_Summarization_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_985",
      "paper_id": "",
      "title": "Interventional Video Grounding With Dual Contrastive Learning",
      "authors": "Guoshun Nan, Rui Qiao, Yao Xiao, Jun Liu, Sicong Leng, Hao Zhang, Wei Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Nan_Interventional_Video_Grounding_With_Dual_Contrastive_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_133",
      "paper_id": "",
      "title": "Invariant Grounding for Video Question Answering",
      "authors": "Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, Tat-Seng Chua",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Invariant_Grounding_for_Video_Question_Answering_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_400",
      "paper_id": "",
      "title": "It's About Time: Analog Clock Reading in the Wild",
      "authors": "Charig Yang, Weidi Xie, Andrew Zisserman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Its_About_Time_Analog_Clock_Reading_in_the_Wild_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_512",
      "paper_id": "",
      "title": "Iterative Proposal Refinement for Weakly-Supervised Video Grounding",
      "authors": "Meng Cao, Fangyun Wei, Can Xu, Xiubo Geng, Long Chen, Can Zhang, Yuexian Zou, Tao Shen, Daxin Jiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_641",
      "paper_id": "",
      "title": "Joint Video Summarization and Moment Localization by Cross-Task Sample Transfer",
      "authors": "Hao Jiang, Yadong Mu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Joint_Video_Summarization_and_Moment_Localization_by_Cross-Task_Sample_Transfer_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1700",
      "paper_id": "",
      "title": "Just Add π! Pose Induced Video Transformers for Understanding Activities of Daily Living",
      "authors": "Dominick Reilly, Srijan Das",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Reilly_Just_Add__Pose_Induced_Video_Transformers_for_Understanding_Activities_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_878",
      "paper_id": "",
      "title": "Koala: Key Frame-Conditioned Long Video-LLM",
      "authors": "Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan A. Plummer, Bryan Russell, Kate Saenko",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Tan_Koala_Key_Frame-Conditioned_Long_Video-LLM_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1131",
      "paper_id": "",
      "title": "Language As Queries for Referring Video Object Segmentation",
      "authors": "Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, Ping Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Language_As_Queries_for_Referring_Video_Object_Segmentation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1166",
      "paper_id": "",
      "title": "Language Model Guided Interpretable Video Action Reasoning",
      "authors": "Ning Wang, Guangming Zhu, HS Li, Liang Zhang, Syed Afaq Ali Shah, Mohammed Bennamoun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Language_Model_Guided_Interpretable_Video_Action_Reasoning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2326",
      "paper_id": "",
      "title": "Language-aware Visual Semantic Distillation for Video Question Answering",
      "authors": "Bo Zou, Chao Yang, Yu Qiao, Chengbin Quan, Youjian Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zou_Language-aware_Visual_Semantic_Distillation_for_Video_Question_Answering_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1395",
      "paper_id": "",
      "title": "Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation",
      "authors": "Zihan Ding, Tianrui Hui, Junshi Huang, Xiaoming Wei, Jizhong Han, Si Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Language-Bridged_Spatial-Temporal_Interaction_for_Referring_Video_Object_Segmentation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_586",
      "paper_id": "",
      "title": "Language-Guided Audio-Visual Learning for Long-Term Sports Assessment",
      "authors": "Huangbiao Xu, Xiao Ke, Huanqi Wu, Rui Xu, Yuezhou Li, Wenzhong Guo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_Language-Guided_Audio-Visual_Learning_for_Long-Term_Sports_Assessment_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_822",
      "paper_id": "",
      "title": "Latency Matters: Real-Time Action Forecasting Transformer",
      "authors": "Harshayu Girase, Nakul Agarwal, Chiho Choi, Karttikeya Mangalam",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Girase_Latency_Matters_Real-Time_Action_Forecasting_Transformer_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1414",
      "paper_id": "",
      "title": "LAVENDER: Unifying Video-Language Understanding As Masked Language Modeling",
      "authors": "Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, Lijuan Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_LAVENDER_Unifying_Video-Language_Understanding_As_Masked_Language_Modeling_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1377",
      "paper_id": "",
      "title": "Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos",
      "authors": "Saghir Alfasly, Jian Lu, Chen Xu, Yuru Zou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Alfasly_Learnable_Irrelevant_Modality_Dropout_for_Multimodal_Action_Recognition_on_Modality-Specific_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1410",
      "paper_id": "",
      "title": "Learning Action Changes by Measuring Verb-Adverb Textual Relationships",
      "authors": "Davide Moltisanti, Frank Keller, Hakan Bilen, Laura Sevilla-Lara",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Moltisanti_Learning_Action_Changes_by_Measuring_Verb-Adverb_Textual_Relationships_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_167",
      "paper_id": "",
      "title": "Learning Asynchronous and Sparse Human-Object Interaction in Videos",
      "authors": "Romero Morais, Vuong Le, Svetha Venkatesh, Truyen Tran",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Morais_Learning_Asynchronous_and_Sparse_Human-Object_Interaction_in_Videos_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1727",
      "paper_id": "",
      "title": "Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval",
      "authors": "Boseung Jeong, Jicheol Park, Sungyeon Kim, Suha Kwak",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_317",
      "paper_id": "",
      "title": "Learning by Aligning Videos in Time",
      "authors": "Sanjay Haresh, Sateesh Kumar, Huseyin Coskun, Shahram N. Syed, Andrey Konin, Zeeshan Zia, Quoc-Huy Tran",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Haresh_Learning_by_Aligning_Videos_in_Time_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1226",
      "paper_id": "",
      "title": "Learning Discriminative Prototypes With Dynamic Time Warping",
      "authors": "Xiaobin Chang, Frederick Tung, Greg Mori",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chang_Learning_Discriminative_Prototypes_With_Dynamic_Time_Warping_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1117",
      "paper_id": "",
      "title": "Learning from Streaming Video with Orthogonal Gradients",
      "authors": "Tengda Han, Dilara Gokay, Joseph Heyward, Chuhan Zhang, Daniel Zoran, Viorica Patraucean, Joao Carreira, Dima Damen, Andrew Zisserman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Han_Learning_from_Streaming_Video_with_Orthogonal_Gradients_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_527",
      "paper_id": "",
      "title": "Learning From Temporal Gradient for Semi-Supervised Action Recognition",
      "authors": "Junfei Xiao, Longlong Jing, Lin Zhang, Ju He, Qi She, Zongwei Zhou, Alan Yuille, Yingwei Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xiao_Learning_From_Temporal_Gradient_for_Semi-Supervised_Action_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1077",
      "paper_id": "",
      "title": "Learning From Untrimmed Videos: Self-Supervised Video Representation Learning With Hierarchical Consistency",
      "authors": "Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Yi Xu, Xiang Wang, Mingqian Tang, Changxin Gao, Rong Jin, Nong Sang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Qing_Learning_From_Untrimmed_Videos_Self-Supervised_Video_Representation_Learning_With_Hierarchical_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_310",
      "paper_id": "",
      "title": "Learning Group Activity Features Through Person Attribute Prediction",
      "authors": "Chihiro Nakatani, Hiroaki Kawashima, Norimichi Ukita",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Nakatani_Learning_Group_Activity_Features_Through_Person_Attribute_Prediction_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_516",
      "paper_id": "",
      "title": "Learning Object State Changes in Videos: An Open-World Perspective",
      "authors": "Zihui Xue, Kumar Ashutosh, Kristen Grauman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xue_Learning_Object_State_Changes_in_Videos_An_Open-World_Perspective_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_513",
      "paper_id": "",
      "title": "Learning Pixel Trajectories With Multiscale Contrastive Random Walks",
      "authors": "Zhangxing Bian, Allan Jabri, Alexei A. Efros, Andrew Owens",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Bian_Learning_Pixel_Trajectories_With_Multiscale_Contrastive_Random_Walks_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1786",
      "paper_id": "",
      "title": "Learning Pixel-Level Distinctions for Video Highlight Detection",
      "authors": "Fanyue Wei, Biao Wang, Tiezheng Ge, Yuning Jiang, Wen Li, Lixin Duan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wei_Learning_Pixel-Level_Distinctions_for_Video_Highlight_Detection_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_336",
      "paper_id": "",
      "title": "Learning Procedure-Aware Video Representation From Instructional Videos and Their Narrations",
      "authors": "Yiwu Zhong, Licheng Yu, Yang Bai, Shangwen Li, Xueting Yan, Yin Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Learning_Procedure-Aware_Video_Representation_From_Instructional_Videos_and_Their_Narrations_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_891",
      "paper_id": "",
      "title": "Learning Program Representations for Food Images and Cooking Recipes",
      "authors": "Dim P. Papadopoulos, Enrique Mora, Nadiia Chepurko, Kuan Wei Huang, Ferda Ofli, Antonio Torralba",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Papadopoulos_Learning_Program_Representations_for_Food_Images_and_Cooking_Recipes_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_38",
      "paper_id": "",
      "title": "Learning Salient Boundary Feature for Anchor-free Temporal Action Localization",
      "authors": "Chuming Lin, Chengming Xu, Donghao Luo, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Yanwei Fu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Learning_Salient_Boundary_Feature_for_Anchor-free_Temporal_Action_Localization_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_2062",
      "paper_id": "",
      "title": "Learning Situation Hyper-Graphs for Video Question Answering",
      "authors": "Aisha Urooj, Hilde Kuehne, Bo Wu, Kim Chheu, Walid Bousselham, Chuang Gan, Niels Lobo, Mubarak Shah",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Urooj_Learning_Situation_Hyper-Graphs_for_Video_Question_Answering_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_311",
      "paper_id": "",
      "title": "Learning the Best Pooling Strategy for Visual Semantic Embedding",
      "authors": "Jiacheng Chen, Hexiang Hu, Hao Wu, Yuning Jiang, Changhu Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Learning_the_Best_Pooling_Strategy_for_Visual_Semantic_Embedding_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_1711",
      "paper_id": "",
      "title": "Learning To Align Sequential Actions in the Wild",
      "authors": "Weizhe Liu, Bugra Tekin, Huseyin Coskun, Vibhav Vineet, Pascal Fua, Marc Pollefeys",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Learning_To_Align_Sequential_Actions_in_the_Wild_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_2463",
      "paper_id": "",
      "title": "Learning to Predict Activity Progress by Self-Supervised Video Alignment",
      "authors": "Gerard Donahue, Ehsan Elhamifar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Donahue_Learning_to_Predict_Activity_Progress_by_Self-Supervised_Video_Alignment_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_452",
      "paper_id": "",
      "title": "Learning To Recognize Procedural Activities With Distant Supervision",
      "authors": "Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang, Lorenzo Torresani",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_Learning_To_Recognize_Procedural_Activities_With_Distant_Supervision_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_124",
      "paper_id": "",
      "title": "Learning To Refactor Action and Co-Occurrence Features for Temporal Action Localization",
      "authors": "Kun Xia, Le Wang, Sanping Zhou, Nanning Zheng, Wei Tang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xia_Learning_To_Refactor_Action_and_Co-Occurrence_Features_for_Temporal_Action_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_208",
      "paper_id": "",
      "title": "Learning To Segment Actions From Visual and Language Instructions via Differentiable Weak Sequence Alignment",
      "authors": "Yuhan Shen, Lu Wang, Ehsan Elhamifar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_Learning_To_Segment_Actions_From_Visual_and_Language_Instructions_via_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_371",
      "paper_id": "",
      "title": "Learning to Segment Referred Objects from Narrated Egocentric Videos",
      "authors": "Yuhan Shen, Huiyu Wang, Xitong Yang, Matt Feiszli, Ehsan Elhamifar, Lorenzo Torresani, Effrosyni Mavroudi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Shen_Learning_to_Segment_Referred_Objects_from_Narrated_Egocentric_Videos_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1368",
      "paper_id": "",
      "title": "Learning Transferable Spatiotemporal Representations From Natural Script Knowledge",
      "authors": "Ziyun Zeng, Yuying Ge, Xihui Liu, Bin Chen, Ping Luo, Shu-Tao Xia, Yixiao Ge",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_Learning_Transferable_Spatiotemporal_Representations_From_Natural_Script_Knowledge_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1543",
      "paper_id": "",
      "title": "Learning Video Representations From Large Language Models",
      "authors": "Yue Zhao, Ishan Misra, Philipp Krähenbühl, Rohit Girdhar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Learning_Video_Representations_From_Large_Language_Models_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_814",
      "paper_id": "",
      "title": "Less Is More: ClipBERT for Video-and-Language Learning via Sparse Sampling",
      "authors": "Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, Jingjing Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Lei_Less_Is_More_ClipBERT_for_Video-and-Language_Learning_via_Sparse_Sampling_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_2083",
      "paper_id": "",
      "title": "Leveraging Temporal Context in Low Representational Power Regimes",
      "authors": "Camilo L. Fosco, SouYoung Jin, Emilie Josephs, Aude Oliva",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Fosco_Leveraging_Temporal_Context_in_Low_Representational_Power_Regimes_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1586",
      "paper_id": "",
      "title": "Likert Scoring With Grade Decoupling for Long-Term Action Assessment",
      "authors": "Angchi Xu, Ling-An Zeng, Wei-Shi Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Likert_Scoring_With_Grade_Decoupling_for_Long-Term_Action_Assessment_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_526",
      "paper_id": "",
      "title": "LION-FS: Fast & Slow Video-Language Thinker as Online Video Assistant",
      "authors": "Wei Li, Bing Hu, Rui Shao, Leyang Shen, Liqiang Nie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_LION-FS_Fast__Slow_Video-Language_Thinker_as_Online_Video_Assistant_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_378",
      "paper_id": "",
      "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
      "authors": "Joya Chen, Ziyun Zeng, Yiqi Lin, Wei Li, Zejun Ma, Mike Zheng Shou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_LiveCC_Learning_Video_LLM_with_Streaming_Speech_Transcription_at_Scale_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1718",
      "paper_id": "",
      "title": "LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding",
      "authors": "Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, Si Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_LLaVA-ST_A_Multimodal_Large_Language_Model_for_Fine-Grained_Spatial-Temporal_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_829",
      "paper_id": "",
      "title": "LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living",
      "authors": "Dominick Reilly, Rajatsubhra Chakraborty, Arkaprava Sinha, Manish Kumar Govind, Pu Wang, Francois Bremond, Le Xue, Srijan Das",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Reilly_LLAVIDAL_A_Large_LAnguage_VIsion_Model_for_Daily_Activities_of_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_536",
      "paper_id": "",
      "title": "Locality-Aware Inter- and Intra-Video Reconstruction for Self-Supervised Correspondence Learning",
      "authors": "Liulei Li, Tianfei Zhou, Wenguan Wang, Lu Yang, Jianwu Li, Yi Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Locality-Aware_Inter-_and_Intra-Video_Reconstruction_for_Self-Supervised_Correspondence_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_812",
      "paper_id": "",
      "title": "Localizing Events in Videos with Multimodal Queries",
      "authors": "Gengyuan Zhang, Mang Ling Ada Fok, Jialu Ma, Yan Xia, Daniel Cremers, Philip Torr, Volker Tresp, Jindong Gu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Localizing_Events_in_Videos_with_Multimodal_Queries_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_77",
      "paper_id": "",
      "title": "LOGO: A Long-Form Video Dataset for Group Action Quality Assessment",
      "authors": "Shiyi Zhang, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, Yansong Tang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_LOGO_A_Long-Form_Video_Dataset_for_Group_Action_Quality_Assessment_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_61",
      "paper_id": "",
      "title": "Long-Short Temporal Contrastive Learning of Video Transformers",
      "authors": "Jue Wang, Gedas Bertasius, Du Tran, Lorenzo Torresani",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Long-Short_Temporal_Contrastive_Learning_of_Video_Transformers_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_801",
      "paper_id": "",
      "title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos",
      "authors": "Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, Feng Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Geng_LongVALE_Vision-Audio-Language-Event_Benchmark_Towards_Time-Aware_Omni-Modal_Perception_of_Long_Videos_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1111",
      "paper_id": "",
      "title": "Look Before You Speak: Visually Contextualized Utterances",
      "authors": "Paul Hongsuck Seo, Arsha Nagrani, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Seo_Look_Before_You_Speak_Visually_Contextualized_Utterances_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_306",
      "paper_id": "",
      "title": "Look for the Change: Learning Object States and State-Modifying Actions From Untrimmed Web Videos",
      "authors": "Tomáš Souček, Jean-Baptiste Alayrac, Antoine Miech, Ivan Laptev, Josef Sivic",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Soucek_Look_for_the_Change_Learning_Object_States_and_State-Modifying_Actions_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_483",
      "paper_id": "",
      "title": "LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation",
      "authors": "Linfeng Yuan, Miaojing Shi, Zijie Yue, Qijun Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_LoSh_Long-Short_Text_Joint_Prediction_Network_for_Referring_Video_Object_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1158",
      "paper_id": "",
      "title": "M-LLM Based Video Frame Selection for Efficient Video Understanding",
      "authors": "Kai Hu, Feng Gao, Xiaohan Nie, Peng Zhou, Son Tran, Tal Neiman, Lingyun Wang, Mubarak Shah, Raffay Hamid, Bing Yin, Trishul Chilimbi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hu_M-LLM_Based_Video_Frame_Selection_for_Efficient_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1763",
      "paper_id": "",
      "title": "MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding",
      "authors": "Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/He_MA-LMM_Memory-Augmented_Large_Multimodal_Model_for_Long-Term_Video_Understanding_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_419",
      "paper_id": "",
      "title": "MAD: A Scalable Dataset for Language Grounding in Videos From Movie Audio Descriptions",
      "authors": "Mattia Soldan, Alejandro Pardo, Juan León Alcázar, Fabian Caba, Chen Zhao, Silvio Giancola, Bernard Ghanem",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Soldan_MAD_A_Scalable_Dataset_for_Language_Grounding_in_Videos_From_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1092",
      "paper_id": "",
      "title": "MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term Dense Action Anticipation",
      "authors": "Olga Zatsarynna, Emad Bahrami, Yazan Abu Farha, Gianpiero Francesca, Juergen Gall",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zatsarynna_MANTA_Diffusion_Mamba_for_Efficient_and_Effective_Stochastic_Long-Term_Dense_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_974",
      "paper_id": "",
      "title": "Masked Feature Prediction for Self-Supervised Visual Pre-Training",
      "authors": "Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, Christoph Feichtenhofer",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wei_Masked_Feature_Prediction_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1950",
      "paper_id": "",
      "title": "Masked Motion Encoding for Self-Supervised Video Representation Learning",
      "authors": "Xinyu Sun, Peihao Chen, Liangwei Chen, Changhao Li, Thomas H. Li, Mingkui Tan, Chuang Gan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Masked_Motion_Encoding_for_Self-Supervised_Video_Representation_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_860",
      "paper_id": "",
      "title": "Masked Video Distillation: Rethinking Masked Feature Modeling for Self-Supervised Video Representation Learning",
      "authors": "Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, Yu-Gang Jiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Masked_Video_Distillation_Rethinking_Masked_Feature_Modeling_for_Self-Supervised_Video_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_224",
      "paper_id": "",
      "title": "Measuring Compositional Consistency for Video Question Answering",
      "authors": "Mona Gandhi, Mustafa Omer Gul, Eva Prakash, Madeleine Grunde-McLaughlin, Ranjay Krishna, Maneesh Agrawala",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Gandhi_Measuring_Compositional_Consistency_for_Video_Question_Answering_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1671",
      "paper_id": "",
      "title": "MED-VT: Multiscale Encoder-Decoder Video Transformer With Application To Object Segmentation",
      "authors": "Rezaul Karim, He Zhao, Richard P. Wildes, Mennatullah Siam",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Karim_MED-VT_Multiscale_Encoder-Decoder_Video_Transformer_With_Application_To_Object_Segmentation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1582",
      "paper_id": "",
      "title": "MELTR: Meta Loss Transformer for Learning To Fine-Tune Video Foundation Models",
      "authors": "Dohwan Ko, Joonmyung Choi, Hyeong Kyu Choi, Kyoung-Woon On, Byungseok Roh, Hyunwoo J. Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ko_MELTR_Meta_Loss_Transformer_for_Learning_To_Fine-Tune_Video_Foundation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_563",
      "paper_id": "",
      "title": "MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition",
      "authors": "Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_MeMViT_Memory-Augmented_Multiscale_Vision_Transformer_for_Efficient_Long-Term_Video_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1459",
      "paper_id": "",
      "title": "MERLOT Reserve: Neural Script Knowledge Through Vision and Language and Sound",
      "authors": "Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, Yejin Choi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zellers_MERLOT_Reserve_Neural_Script_Knowledge_Through_Vision_and_Language_and_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1584",
      "paper_id": "",
      "title": "Meta-Personalizing Vision-Language Models To Find Named Instances in Video",
      "authors": "Chun-Hsiao Yeh, Bryan Russell, Josef Sivic, Fabian Caba Heilbron, Simon Jenni",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yeh_Meta-Personalizing_Vision-Language_Models_To_Find_Named_Instances_in_Video_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_665",
      "paper_id": "",
      "title": "MICap: A Unified Model for Identity-Aware Movie Descriptions",
      "authors": "Haran Raajesh, Naveen Reddy Desanur, Zeeshan Khan, Makarand Tapaswi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Raajesh_MICap_A_Unified_Model_for_Identity-Aware_Movie_Descriptions_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_333",
      "paper_id": "",
      "title": "Mining Better Samples for Contrastive Learning of Temporal Correspondence",
      "authors": "Sangryul Jeon, Dongbo Min, Seungryong Kim, Kwanghoon Sohn",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Jeon_Mining_Better_Samples_for_Contrastive_Learning_of_Temporal_Correspondence_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_1138",
      "paper_id": "",
      "title": "Mirasol3B: A Multimodal Autoregressive Model for Time-Aligned and Contextual Modalities",
      "authors": "AJ Piergiovanni, Isaac Noble, Dahun Kim, Michael S. Ryoo, Victor Gomes, Anelia Angelova",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Piergiovanni_Mirasol3B_A_Multimodal_Autoregressive_Model_for_Time-Aligned_and_Contextual_Modalities_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1612",
      "paper_id": "",
      "title": "MIST: Multi-Modal Iterative Spatial-Temporal Transformer for Long-Form Video Question Answering",
      "authors": "Difei Gao, Luowei Zhou, Lei Ji, Linchao Zhu, Yi Yang, Mike Zheng Shou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_MIST_Multi-Modal_Iterative_Spatial-Temporal_Transformer_for_Long-Form_Video_Question_Answering_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1124",
      "paper_id": "",
      "title": "MLP-3D: A MLP-Like 3D Architecture With Grouped Time Mixing",
      "authors": "Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Tao Mei",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Qiu_MLP-3D_A_MLP-Like_3D_Architecture_With_Grouped_Time_Mixing_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1299",
      "paper_id": "",
      "title": "MLVU: Benchmarking Multi-task Long Video Understanding",
      "authors": "Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, Zheng Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_MLVU_Benchmarking_Multi-task_Long_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_317",
      "paper_id": "",
      "title": "MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning",
      "authors": "Chaoyi Zhang, Kevin Lin, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Chung-Ching Lin, Zicheng Liu, Lijuan Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_MM-Narrator_Narrating_Long-form_Videos_with_Multimodal_In-Context_Learning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1229",
      "paper_id": "",
      "title": "MMG-Ego4D: Multimodal Generalization in Egocentric Action Recognition",
      "authors": "Xinyu Gong, Sreyas Mohan, Naina Dhingra, Jean-Charles Bazin, Yilei Li, Zhangyang Wang, Rakesh Ranjan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Gong_MMG-Ego4D_Multimodal_Generalization_in_Egocentric_Action_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2392",
      "paper_id": "",
      "title": "MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos",
      "authors": "Jielin Qiu, Jiacheng Zhu, William Han, Aditesh Kumar, Karthik Mittal, Claire Jin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Ding Zhao, Bo Li, Lijuan Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Qiu_MMSum_A_Dataset_for_Multimodal_Summarization_and_Thumbnail_Generation_of_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_456",
      "paper_id": "",
      "title": "Modeling Motion With Multi-Modal Features for Text-Based Video Segmentation",
      "authors": "Wangbo Zhao, Kai Wang, Xiangxiang Chu, Fuzhao Xue, Xinchao Wang, Yang You",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Modeling_Motion_With_Multi-Modal_Features_for_Text-Based_Video_Segmentation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_1590",
      "paper_id": "",
      "title": "Modeling Multi-Label Action Dependencies for Temporal Action Localization",
      "authors": "Praveen Tirupattur, Kevin Duarte, Yogesh S Rawat, Mubarak Shah",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Tirupattur_Modeling_Multi-Label_Action_Dependencies_for_Temporal_Action_Localization_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2549",
      "paper_id": "",
      "title": "Modeling Multiple Normal Action Representations for Error Detection in Procedural Tasks",
      "authors": "Wei-Jin Huang, Yuan-Ming Li, Zhi-Wei Xia, Yu-Ming Tang, Kun-Yu Lin, Jian-Fang Hu, Wei-Shi Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Modeling_Multiple_Normal_Action_Representations_for_Error_Detection_in_Procedural_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1362",
      "paper_id": "",
      "title": "Modeling Video As Stochastic Processes for Fine-Grained Video Representation Learning",
      "authors": "Heng Zhang, Daqing Liu, Qi Zheng, Bing Su",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Modeling_Video_As_Stochastic_Processes_for_Fine-Grained_Video_Representation_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_2008",
      "paper_id": "",
      "title": "Modular Memorability: Tiered Representations for Video Memorability Prediction",
      "authors": "Théo Dumont, Juan Segundo Hevia, Camilo L. Fosco",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Dumont_Modular_Memorability_Tiered_Representations_for_Video_Memorability_Prediction_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_21",
      "paper_id": "",
      "title": "MoLo: Motion-Augmented Long-Short Contrastive Learning for Few-Shot Action Recognition",
      "authors": "Xiang Wang, Shiwei Zhang, Zhiwu Qing, Changxin Gao, Yingya Zhang, Deli Zhao, Nong Sang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MoLo_Motion-Augmented_Long-Short_Contrastive_Learning_for_Few-Shot_Action_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_841",
      "paper_id": "",
      "title": "MoReVQA: Exploring Modular Reasoning Models for Video Question Answering",
      "authors": "Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Min_MoReVQA_Exploring_Modular_Reasoning_Models_for_Video_Question_Answering_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_41",
      "paper_id": "",
      "title": "Motion-Aware Contrastive Video Representation Learning via Foreground-Background Merging",
      "authors": "Shuangrui Ding, Maomao Li, Tianyu Yang, Rui Qian, Haohang Xu, Qingyi Chen, Jue Wang, Hongkai Xiong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Motion-Aware_Contrastive_Video_Representation_Learning_via_Foreground-Background_Merging_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_529",
      "paper_id": "",
      "title": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level",
      "authors": "Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, Yapeng Tian, Ajmal Saeed Mian, Mohit Bansal, Chen Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Deng_Motion-Grounded_Video_Reasoning_Understanding_and_Perceiving_Motion_at_Pixel_Level_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1069",
      "paper_id": "",
      "title": "Motion-Modulated Temporal Fragment Alignment Network for Few-Shot Action Recognition",
      "authors": "Jiamin Wu, Tianzhu Zhang, Zhe Zhang, Feng Wu, Yongdong Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Motion-Modulated_Temporal_Fragment_Alignment_Network_for_Few-Shot_Action_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_574",
      "paper_id": "",
      "title": "MovieChat: From Dense Token to Sparse Memory for Long Video Understanding",
      "authors": "Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, Yan Lu, Jenq-Neng Hwang, Gaoang Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Song_MovieChat_From_Dense_Token_to_Sparse_Memory_for_Long_Video_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1306",
      "paper_id": "",
      "title": "Movies2Scenes: Using Movie Metadata To Learn Scene Representation",
      "authors": "Shixing Chen, Chun-Hao Liu, Xiang Hao, Xiaohan Nie, Maxim Arap, Raffay Hamid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Movies2Scenes_Using_Movie_Metadata_To_Learn_Scene_Representation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_1642",
      "paper_id": "",
      "title": "MoViNets: Mobile Video Networks for Efficient Video Recognition",
      "authors": "Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew Brown, Boqing Gong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Kondratyuk_MoViNets_Mobile_Video_Networks_for_Efficient_Video_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_257",
      "paper_id": "",
      "title": "MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection",
      "authors": "Rui Dai, Srijan Das, Kumara Kahatapitiya, Michael S. Ryoo, François Brémond",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Dai_MS-TCT_Multi-Scale_Temporal_ConvTransformer_for_Action_Detection_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_1070",
      "paper_id": "",
      "title": "Multi-Label Activity Recognition Using Activity-Specific Features and Activity Correlations",
      "authors": "Yanyi Zhang, Xinyu Li, Ivan Marsic",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Multi-Label_Activity_Recognition_Using_Activity-Specific_Features_and_Activity_Correlations_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_1543",
      "paper_id": "",
      "title": "Multi-Level Representation Learning With Semantic Alignment for Referring Video Object Segmentation",
      "authors": "Dongming Wu, Xingping Dong, Ling Shao, Jianbing Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Multi-Level_Representation_Learning_With_Semantic_Alignment_for_Referring_Video_Object_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_567",
      "paper_id": "",
      "title": "Multi-Modal Relational Graph for Cross-Modal Video Moment Retrieval",
      "authors": "Yawen Zeng, Da Cao, Xiaochi Wei, Meng Liu, Zhou Zhao, Zheng Qin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zeng_Multi-Modal_Relational_Graph_for_Cross-Modal_Video_Moment_Retrieval_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1047",
      "paper_id": "",
      "title": "Multi-Shot Temporal Event Localization: A Benchmark",
      "authors": "Xiaolong Liu, Yao Hu, Song Bai, Fei Ding, Xiang Bai, Philip H. S. Torr",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Multi-Shot_Temporal_Event_Localization_A_Benchmark_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1317",
      "paper_id": "",
      "title": "Multi-Stage Aggregated Transformer Network for Temporal Language Localization in Videos",
      "authors": "Mingxing Zhang, Yang Yang, Xinghan Chen, Yanli Ji, Xing Xu, Jingjing Li, Heng Tao Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Multi-Stage_Aggregated_Transformer_Network_for_Temporal_Language_Localization_in_Videos_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2447",
      "paper_id": "",
      "title": "Multiscale Vision Transformers Meet Bipartite Matching for Efficient Single-stage Action Localization",
      "authors": "Ioanna Ntinou, Enrique Sanchez, Georgios Tzimiropoulos",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ntinou_Multiscale_Vision_Transformers_Meet_Bipartite_Matching_for_Efficient_Single-stage_Action_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1295",
      "paper_id": "",
      "title": "MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval",
      "authors": "Reno Kriz, Kate Sanders, David Etter, Kenton Murray, Cameron Carpenter, Hannah Recknor, Jimena Guallar-Blasco, Alexander Martin, Eugene Yang, Benjamin Van Durme",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kriz_MultiVENT_2.0_A_Massive_Multilingual_Benchmark_for_Event-Centric_Video_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1232",
      "paper_id": "",
      "title": "Multiview Transformers for Video Recognition",
      "authors": "Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_Multiview_Transformers_for_Video_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1698",
      "paper_id": "",
      "title": "MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval",
      "authors": "Xiaojie Jin, Bowen Zhang, Weibo Gong, Kai Xu, Xueqing Deng, Peng Wang, Zhao Zhang, Xiaohui Shen, Jiashi Feng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Jin_MV-Adapter_Multimodal_Video_Transfer_Learning_for_Video_Text_Retrieval_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_432",
      "paper_id": "",
      "title": "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark",
      "authors": "Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, Yu Qiao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MVBench_A_Comprehensive_Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_764",
      "paper_id": "",
      "title": "NaQ: Leveraging Narrations As Queries To Supervise Episodic Memory",
      "authors": "Santhosh Kumar Ramakrishnan, Ziad Al-Halah, Kristen Grauman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ramakrishnan_NaQ_Leveraging_Narrations_As_Queries_To_Supervise_Episodic_Memory_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_25",
      "paper_id": "",
      "title": "Narrating the Video: Boosting Text-Video Retrieval via Comprehensive Utilization of Frame-Level Captions",
      "authors": "Chan Hur, Jeong-hun Hong, Dong-hun Lee, Dabin Kang, Semin Myeong, Sang-hyo Park, Hyeyoung Park",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hur_Narrating_the_Video_Boosting_Text-Video_Retrieval_via_Comprehensive_Utilization_of_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2209",
      "paper_id": "",
      "title": "Narrative Action Evaluation with Prompt-Guided Multimodal Interaction",
      "authors": "Shiyi Zhang, Sule Bai, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Narrative_Action_Evaluation_with_Prompt-Guided_Multimodal_Interaction_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1312",
      "paper_id": "",
      "title": "Neighbor Relations Matter in Video Scene Detection",
      "authors": "Jiawei Tan, Hongxing Wang, Jiaxin Li, Zhilong Ou, Zhangbin Qian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Tan_Neighbor_Relations_Matter_in_Video_Scene_Detection_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1210",
      "paper_id": "",
      "title": "NewsNet: A Novel Dataset for Hierarchical Temporal Segmentation",
      "authors": "Haoqian Wu, Keyu Chen, Haozhe Liu, Mingchen Zhuge, Bing Li, Ruizhi Qiao, Xiujun Shu, Bei Gan, Liangsheng Xu, Bo Ren, Mengmeng Xu, Wentian Zhang, Raghavendra Ramachandra, Chia-Wen Lin, Bernard Ghanem",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_NewsNet_A_Novel_Dataset_for_Hierarchical_Temporal_Segmentation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_157",
      "paper_id": "",
      "title": "NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions",
      "authors": "Junbin Xiao, Xindi Shang, Angela Yao, Tat-Seng Chua",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Xiao_NExT-QA_Next_Phase_of_Question-Answering_to_Explaining_Temporal_Actions_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_907",
      "paper_id": "",
      "title": "No Frame Left Behind: Full Video Action Recognition",
      "authors": "Xin Liu, Silvia L. Pintea, Fatemeh Karimi Nejadasl, Olaf Booij, Jan C. van Gemert",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_No_Frame_Left_Behind_Full_Video_Action_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_662",
      "paper_id": "",
      "title": "Number it: Temporal Grounding Videos like Flipping Manga",
      "authors": "Yongliang Wu, Xinting Hu, Yuyang Sun, Yizhou Zhou, Wenbo Zhu, Fengyun Rao, Bernt Schiele, Xu Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Number_it_Temporal_Grounding_Videos_like_Flipping_Manga_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1008",
      "paper_id": "",
      "title": "Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food",
      "authors": "Quin Thames, Arjun Karpur, Wade Norris, Fangting Xia, Liviu Panait, Tobias Weyand, Jack Sim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Thames_Nutrition5k_Towards_Automatic_Nutritional_Understanding_of_Generic_Food_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_389",
      "paper_id": "",
      "title": "Object-Aware Video-Language Pre-Training for Retrieval",
      "authors": "Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu Qie, Mike Zheng Shou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Object-Aware_Video-Language_Pre-Training_for_Retrieval_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1257",
      "paper_id": "",
      "title": "Object-Region Video Transformers",
      "authors": "Roei Herzig, Elad Ben-Avraham, Karttikeya Mangalam, Amir Bar, Gal Chechik, Anna Rohrbach, Trevor Darrell, Amir Globerson",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Herzig_Object-Region_Video_Transformers_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_977",
      "paper_id": "",
      "title": "Object-Relation Reasoning Graph for Action Recognition",
      "authors": "Yangjun Ou, Li Mi, Zhenzhong Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ou_Object-Relation_Reasoning_Graph_for_Action_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1091",
      "paper_id": "",
      "title": "Object-Shot Enhanced Grounding Network for Egocentric Video",
      "authors": "Yisen Feng, Haoyu Zhang, Meng Liu, Weili Guan, Liqiang Nie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Feng_Object-Shot_Enhanced_Grounding_Network_for_Egocentric_Video_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_679",
      "paper_id": "",
      "title": "OCSampler: Compressing Videos to One Clip With Single-Step Sampling",
      "authors": "Jintao Lin, Haodong Duan, Kai Chen, Dahua Lin, Limin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_OCSampler_Compressing_Videos_to_One_Clip_With_Single-Step_Sampling_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1875",
      "paper_id": "",
      "title": "Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks",
      "authors": "Miran Heo, Min-Hung Chen, De-An Huang, Sifei Liu, Subhashree Radhakrishnan, Seon Joo Kim, Yu-Chiang Frank Wang, Ryo Hachiuma",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Heo_Omni-RGPT_Unifying_Image_and_Video_Region-level_Understanding_via_Token_Marks_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1486",
      "paper_id": "",
      "title": "Omnia de EgoTempo: Benchmarking Temporal Understanding of Multi-Modal LLMs in Egocentric Videos",
      "authors": "Chiara Plizzari, Alessio Tonioni, Yongqin Xian, Achin Kulshrestha, Federico Tombari",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Plizzari_Omnia_de_EgoTempo_Benchmarking_Temporal_Understanding_of_Multi-Modal_LLMs_in_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2072",
      "paper_id": "",
      "title": "OmniViD: A Generative Framework for Universal Video Understanding",
      "authors": "Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_OmniViD_A_Generative_Framework_for_Universal_Video_Understanding_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_682",
      "paper_id": "",
      "title": "On Semantic Similarity in Video Retrieval",
      "authors": "Michael Wray, Hazel Doughty, Dima Damen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wray_On_Semantic_Similarity_in_Video_Retrieval_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2606",
      "paper_id": "",
      "title": "On the Consistency of Video Large Language Models in Temporal Comprehension",
      "authors": "Minjoon Jung, Junbin Xiao, Byoung-Tak Zhang, Angela Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jung_On_the_Consistency_of_Video_Large_Language_Models_in_Temporal_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1843",
      "paper_id": "",
      "title": "Online Video Understanding: OVBench and VideoChat-Online",
      "authors": "Zhenpeng Huang, Xinhao Li, Jiaqi Li, Jing Wang, Xiangyu Zeng, Cheng Liang, Tao Wu, Xi Chen, Liang Li, Limin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Online_Video_Understanding_OVBench_and_VideoChat-Online_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_2130",
      "paper_id": "",
      "title": "Open Set Action Recognition via Multi-Label Evidential Learning",
      "authors": "Chen Zhao, Dawei Du, Anthony Hoogs, Christopher Funk",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Open_Set_Action_Recognition_via_Multi-Label_Evidential_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_770",
      "paper_id": "",
      "title": "Open-Book Video Captioning With Retrieve-Copy-Generate Network",
      "authors": "Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Ying Shan, Bing Li, Ying Deng, Weiming Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Open-Book_Video_Captioning_With_Retrieve-Copy-Generate_Network_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2185",
      "paper_id": "",
      "title": "OpenEQA: Embodied Question Answering in the Era of Foundation Models",
      "authors": "Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Alexander Sax, Aravind Rajeswaran",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1632",
      "paper_id": "",
      "title": "OpenTAL: Towards Open Set Temporal Action Localization",
      "authors": "Wentao Bao, Qi Yu, Yu Kong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Bao_OpenTAL_Towards_Open_Set_Temporal_Action_Localization_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_784",
      "paper_id": "",
      "title": "OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for General Video Recognition",
      "authors": "Tongjia Chen, Hongshan Yu, Zhengeng Yang, Zechuan Li, Wei Sun, Chen Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_OST_Refining_Text_Knowledge_with_Optimal_Spatio-Temporal_Descriptor_for_General_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_392",
      "paper_id": "",
      "title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?",
      "authors": "Junbo Niu, Yifei Li, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Niu_OVO-Bench_How_Far_is_Your_Video-LLMs_from_Real-World_Online_Video_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1432",
      "paper_id": "",
      "title": "P3IV: Probabilistic Procedure Planning From Instructional Videos With Weak Supervision",
      "authors": "He Zhao, Isma Hadji, Nikita Dvornik, Konstantinos G. Derpanis, Richard P. Wildes, Allan D. Jepson",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_P3IV_Probabilistic_Procedure_Planning_From_Instructional_Videos_With_Weak_Supervision_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_379",
      "paper_id": "",
      "title": "PairDETR : Joint Detection and Association of Human Bodies and Faces",
      "authors": "Ammar Ali, Georgii Gaikov, Denis Rybalchenko, Alexander Chigorin, Ivan Laptev, Sergey Zagoruyko",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ali_PairDETR__Joint_Detection_and_Association_of_Human_Bodies_and_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2688",
      "paper_id": "",
      "title": "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers",
      "authors": "Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, Sergey Tulyakov",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Panda-70M_Captioning_70M_Videos_with_Multiple_Cross-Modality_Teachers_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1038",
      "paper_id": "",
      "title": "PAVE: Patching and Adapting Video Large Language Models",
      "authors": "Zhuoming Liu, Yiquan Li, Khoi Duc Nguyen, Yiwu Zhong, Yin Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_PAVE_Patching_and_Adapting_Video_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_863",
      "paper_id": "",
      "title": "PDPP:Projected Diffusion for Procedure Planning in Instructional Videos",
      "authors": "Hanlin Wang, Yilu Wu, Sheng Guo, Limin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_PDPPProjected_Diffusion_for_Procedure_Planning_in_Instructional_Videos_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_1018",
      "paper_id": "",
      "title": "PGT: A Progressive Method for Training Models on Long Videos",
      "authors": "Bo Pang, Gao Peng, Yizhuo Li, Cewu Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Pang_PGT_A_Progressive_Method_for_Training_Models_on_Long_Videos_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_758",
      "paper_id": "",
      "title": "PHGC: Procedural Heterogeneous Graph Completion for Natural Language Task Verification in Egocentric Videos",
      "authors": "Xun Jiang, Zhiyi Huang, Xing Xu, Jingkuan Song, Fumin Shen, Heng Tao Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jiang_PHGC_Procedural_Heterogeneous_Graph_Completion_for_Natural_Language_Task_Verification_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_292",
      "paper_id": "",
      "title": "PivoTAL: Prior-Driven Supervision for Weakly-Supervised Temporal Action Localization",
      "authors": "Mamshad Nayeem Rizve, Gaurav Mittal, Ye Yu, Matthew Hall, Sandra Sajeev, Mubarak Shah, Mei Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Rizve_PivoTAL_Prior-Driven_Supervision_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1030",
      "paper_id": "",
      "title": "Post-Processing Temporal Action Detection",
      "authors": "Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, Tao Xiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Nag_Post-Processing_Temporal_Action_Detection_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_824",
      "paper_id": "",
      "title": "Precise Event Spotting in Sports Videos: Solving Long-Range Dependency and Class Imbalance",
      "authors": "Sanchayan Santra, Vishal Chudasama, Pankaj Wasnik, Vineeth N Balasubramanian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Santra_Precise_Event_Spotting_in_Sports_Videos_Solving_Long-Range_Dependency_and_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1978",
      "paper_id": "",
      "title": "PREGO: Online Mistake Detection in PRocedural EGOcentric Videos",
      "authors": "Alessandro Flaborea, Guido Maria D'Amely di Melendugno, Leonardo Plini, Luca Scofano, Edoardo De Matteis, Antonino Furnari, Giovanni Maria Farinella, Fabio Galasso",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Flaborea_PREGO_Online_Mistake_Detection_in_PRocedural_EGOcentric_Videos_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_462",
      "paper_id": "",
      "title": "Previously on ... From Recaps to Story Summarization",
      "authors": "Aditya Kumar Singh, Dhruv Srivastava, Makarand Tapaswi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Singh_Previously_on_..._From_Recaps_to_Story_Summarization_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_944",
      "paper_id": "",
      "title": "Probabilistic Representations for Video Contrastive Learning",
      "authors": "Jungin Park, Jiyoung Lee, Ig-Jae Kim, Kwanghoon Sohn",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Park_Probabilistic_Representations_for_Video_Contrastive_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1016",
      "paper_id": "",
      "title": "Procedure-Aware Pretraining for Instructional Video Understanding",
      "authors": "Honglu Zhou, Roberto Martín-Martín, Mubbasir Kapadia, Silvio Savarese, Juan Carlos Niebles",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Procedure-Aware_Pretraining_for_Instructional_Video_Understanding_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1749",
      "paper_id": "",
      "title": "Progress-Aware Online Action Segmentation for Egocentric Procedural Task Videos",
      "authors": "Yuhan Shen, Ehsan Elhamifar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Shen_Progress-Aware_Online_Action_Segmentation_for_Egocentric_Procedural_Task_Videos_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2592",
      "paper_id": "",
      "title": "Progress-Aware Video Frame Captioning",
      "authors": "Zihui Xue, Joungbin An, Xitong Yang, Kristen Grauman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xue_Progress-Aware_Video_Frame_Captioning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_541",
      "paper_id": "",
      "title": "Progressive Attention on Multi-Level Dense Difference Maps for Generic Event Boundary Detection",
      "authors": "Jiaqi Tang, Zhaoyang Liu, Chen Qian, Wayne Wu, Limin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Progressive_Attention_on_Multi-Level_Dense_Difference_Maps_for_Generic_Event_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_2048",
      "paper_id": "",
      "title": "Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization",
      "authors": "Huan Ren, Wenfei Yang, Tianzhu Zhang, Yongdong Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Proposal-Based_Multiple_Instance_Learning_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_865",
      "paper_id": "",
      "title": "ProTeGe: Untrimmed Pretraining for Video Temporal Grounding by Video Temporal Grounding",
      "authors": "Lan Wang, Gaurav Mittal, Sandra Sajeev, Ye Yu, Matthew Hall, Vishnu Naresh Boddeti, Mei Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_ProTeGe_Untrimmed_Pretraining_for_Video_Temporal_Grounding_by_Video_Temporal_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_42",
      "paper_id": "",
      "title": "Query-Dependent Video Representation for Moment Retrieval and Highlight Detection",
      "authors": "WonJun Moon, Sangeek Hyun, SangUk Park, Dongchan Park, Jae-Pil Heo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Moon_Query-Dependent_Video_Representation_for_Moment_Retrieval_and_Highlight_Detection_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2335",
      "paper_id": "",
      "title": "Question-Aware Gaussian Experts for Audio-Visual Question Answering",
      "authors": "Hongyeob Kim, Inyoung Jung, Dayoon Suh, Youjia Zhang, Sangmin Lee, Sungeun Hong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_822",
      "paper_id": "",
      "title": "Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels",
      "authors": "Tianming Liang, Chaolei Tan, Beihao Xia, Wei-Shi Zheng, Jian-Fang Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Ranking_Distillation_for_Open-Ended_Video_Question_Answering_with_Insufficient_Labels_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_901",
      "paper_id": "",
      "title": "RCL: Recurrent Continuous Localization for Temporal Action Detection",
      "authors": "Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_RCL_Recurrent_Continuous_Localization_for_Temporal_Action_Detection_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2810",
      "paper_id": "",
      "title": "Re-thinking Temporal Search for Long-Form Video Understanding",
      "authors": "Jinhui Ye, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, Jiajun Wu, Manling Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_Re-thinking_Temporal_Search_for_Long-Form_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_2011",
      "paper_id": "",
      "title": "Re2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization",
      "authors": "Chen Zhao, Shuming Liu, Karttikeya Mangalam, Bernard Ghanem",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Re2TAL_Rewiring_Pretrained_Video_Backbones_for_Reversible_Temporal_Action_Localization_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2641",
      "paper_id": "",
      "title": "Realigning Confidence with Temporal Saliency Information for Point-Level Weakly-Supervised Temporal Action Localization",
      "authors": "Ziying Xia, Jian Cheng, Siyu Liu, Yongxiang Hu, Shiguang Wang, Yijie Zhang, Liwan Dang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xia_Realigning_Confidence_with_Temporal_Saliency_Information_for_Point-Level_Weakly-Supervised_Temporal_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_652",
      "paper_id": "",
      "title": "Recurring the Transformer for Video Action Recognition",
      "authors": "Jiewen Yang, Xingbo Dong, Liujun Liu, Chao Zhang, Jiajun Shen, Dahai Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Recurring_the_Transformer_for_Video_Action_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1808",
      "paper_id": "",
      "title": "Reducing the Label Bias for Timestamp Supervised Temporal Action Segmentation",
      "authors": "Kaiyuan Liu, Yunheng Li, Shenglan Liu, Chenwei Tan, Zihang Shao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Reducing_the_Label_Bias_for_Timestamp_Supervised_Temporal_Action_Segmentation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1373",
      "paper_id": "",
      "title": "Relational Space-Time Query in Long-Form Videos",
      "authors": "Xitong Yang, Fu-Jen Chu, Matt Feiszli, Raghav Goyal, Lorenzo Torresani, Du Tran",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Relational_Space-Time_Query_in_Long-Form_Videos_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1133",
      "paper_id": "",
      "title": "RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations",
      "authors": "Savya Khosla, Sethuraman T V, Alexander Schwing, Derek Hoiem",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Khosla_RELOCATE_A_Simple_Training-Free_Baseline_for_Visual_Query_Localization_Using_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_572",
      "paper_id": "",
      "title": "RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition",
      "authors": "Jun Chen, Aniket Agarwal, Sherif Abdelkarim, Deyao Zhu, Mohamed Elhoseiny",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_RelTransformer_A_Transformer-Based_Long-Tail_Visual_Relationship_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_1534",
      "paper_id": "",
      "title": "Removing the Background by Adding the Background: Towards Background Robust Self-Supervised Video Representation Learning",
      "authors": "Jinpeng Wang, Yuting Gao, Ke Li, Yiqi Lin, Andy J. Ma, Hao Cheng, Pai Peng, Feiyue Huang, Rongrong Ji, Xing Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Removing_the_Background_by_Adding_the_Background_Towards_Background_Robust_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_519",
      "paper_id": "",
      "title": "Repetitive Activity Counting by Sight and Sound",
      "authors": "Yunhua Zhang, Ling Shao, Cees G. M. Snoek",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Repetitive_Activity_Counting_by_Sight_and_Sound_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_735",
      "paper_id": "",
      "title": "Representation Learning via Global Temporal Alignment and Cycle-Consistency",
      "authors": "Isma Hadji, Konstantinos G. Derpanis, Allan D. Jepson",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Hadji_Representation_Learning_via_Global_Temporal_Alignment_and_Cycle-Consistency_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_584",
      "paper_id": "",
      "title": "Representing Videos As Discriminative Sub-Graphs for Action Recognition",
      "authors": "Dong Li, Zhaofan Qiu, Yingwei Pan, Ting Yao, Houqiang Li, Tao Mei",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Representing_Videos_As_Discriminative_Sub-Graphs_for_Action_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1418",
      "paper_id": "",
      "title": "ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams",
      "authors": "Chris Dongjoo Kim, Jihwan Moon, Sangwoo Moon, Heeseung Yun, Sihaeng Lee, Aniruddha Kembhavi, Soonyoung Lee, Gunhee Kim, Sangho Lee, Christopher Clark",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_ReSpec_Relevance_and_Specificity_Grounded_Online_Filtering_for_Learning_on_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2205",
      "paper_id": "",
      "title": "Rethinking Noisy Video-Text Retrieval via Relation-aware Alignment",
      "authors": "Huakai Lai, Guoxin Xiong, Huayu Mai, Xiang Liu, Tianzhu Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lai_Rethinking_Noisy_Video-Text_Retrieval_via_Relation-aware_Alignment_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_2179",
      "paper_id": "",
      "title": "Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning",
      "authors": "AJ Piergiovanni, Weicheng Kuo, Anelia Angelova",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Piergiovanni_Rethinking_Video_ViTs_Sparse_Video_Tubes_for_Joint_Image_and_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_991",
      "paper_id": "",
      "title": "Retrieval-Augmented Egocentric Video Captioning",
      "authors": "Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Retrieval-Augmented_Egocentric_Video_Captioning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2673",
      "paper_id": "",
      "title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos",
      "authors": "Tanveer Hannan, Md Mohaiminul Islam, Jindong Gu, Thomas Seidl, Gedas Bertasius",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hannan_ReVisionLLM_Recursive_Vision-Language_Model_for_Temporal_Grounding_in_Hour-Long_Videos_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_413",
      "paper_id": "",
      "title": "Revisiting Temporal Modeling for CLIP-Based Image-to-Video Knowledge Transferring",
      "authors": "Ruyang Liu, Jingjia Huang, Ge Li, Jiashi Feng, Xinglong Wu, Thomas H. Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Revisiting_Temporal_Modeling_for_CLIP-Based_Image-to-Video_Knowledge_Transferring_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1572",
      "paper_id": "",
      "title": "Revisiting the \"Video\" in Video-Language Understanding",
      "authors": "Shyamal Buch, Cristóbal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, Juan Carlos Niebles",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Buch_Revisiting_the_Video_in_Video-Language_Understanding_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2011",
      "paper_id": "",
      "title": "ReWind: Understanding Long Videos with Instructed Learnable Memory",
      "authors": "Anxhelo Diko, Tinghuai Wang, Wassim Swaileh, Shiyan Sun, Ioannis Patras",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Diko_ReWind_Understanding_Long_Videos_with_Instructed_Learnable_Memory_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_404",
      "paper_id": "",
      "title": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives",
      "authors": "Chirag Parikh, Deepti Rawat, Rakshitha R. T., Tathagata Ghosh, Ravi Kiran Sarvadevabhatla",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Parikh_RoadSocial_A_Diverse_VideoQA_Dataset_and_Benchmark_for_Road_Event_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_264",
      "paper_id": "",
      "title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis",
      "authors": "Junho Kim, Hyunjun Kim, Hosu Lee, Yong Man Ro",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_SALOVA_Segment-Augmented_Long_Video_Assistant_for_Targeted_Retrieval_and_Routing_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_105",
      "paper_id": "",
      "title": "SAM2-LOVE: Segment Anything Model 2 in Language-aided Audio-Visual Scenes",
      "authors": "Yuji Wang, Haoran Xu, Yong Liu, Jiaze Li, Yansong Tang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_SAM2-LOVE_Segment_Anything_Model_2_in_Language-aided_Audio-Visual_Scenes_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1476",
      "paper_id": "",
      "title": "SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation",
      "authors": "Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, Giuseppe Averta",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_58",
      "paper_id": "",
      "title": "Scaling Up Video Summarization Pretraining with Large Language Models",
      "authors": "Dawit Mureja Argaw, Seunghyun Yoon, Fabian Caba Heilbron, Hanieh Deilamsalehy, Trung Bui, Zhaowen Wang, Franck Dernoncourt, Joon Son Chung",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Argaw_Scaling_Up_Video_Summarization_Pretraining_with_Large_Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_369",
      "paper_id": "",
      "title": "Scene Consistency Representation Learning for Video Scene Segmentation",
      "authors": "Haoqian Wu, Keyu Chen, Yanan Luo, Ruizhi Qiao, Bo Ren, Haozhe Liu, Weicheng Xie, Linlin Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Scene_Consistency_Representation_Learning_for_Video_Scene_Segmentation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1277",
      "paper_id": "",
      "title": "SEAL: Semantic Attention Learning for Long Video Representation",
      "authors": "Lan Wang, Yujia Chen, Du Tran, Vishnu Naresh Boddeti, Wen-Sheng Chu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_170",
      "paper_id": "",
      "title": "Search-Map-Search: A Frame Selection Paradigm for Action Recognition",
      "authors": "Mingjun Zhao, Yakun Yu, Xiaoli Wang, Lei Yang, Di Niu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Search-Map-Search_A_Frame_Selection_Paradigm_for_Action_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_2327",
      "paper_id": "",
      "title": "Selective Structured State-Spaces for Long-Form Video Understanding",
      "authors": "Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, Raffay Hamid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Selective_Structured_State-Spaces_for_Long-Form_Video_Understanding_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2700",
      "paper_id": "",
      "title": "Self-Supervised Cross-View Correspondence with Predictive Cycle Consistency",
      "authors": "Alan Baade, Changan Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1296",
      "paper_id": "",
      "title": "Self-Supervised Learning for Semi-Supervised Temporal Action Proposal",
      "authors": "Xiang Wang, Shiwei Zhang, Zhiwu Qing, Yuanjie Shao, Changxin Gao, Nong Sang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Self-Supervised_Learning_for_Semi-Supervised_Temporal_Action_Proposal_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1554",
      "paper_id": "",
      "title": "Self-Supervised Motion Learning From Static Images",
      "authors": "Ziyuan Huang, Shiwei Zhang, Jianwen Jiang, Mingqian Tang, Rong Jin, Marcelo H. Ang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Self-Supervised_Motion_Learning_From_Static_Images_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_92",
      "paper_id": "",
      "title": "Self-Supervised Video Hashing via Bidirectional Transformers",
      "authors": "Shuyan Li, Xiu Li, Jiwen Lu, Jie Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Self-Supervised_Video_Hashing_via_Bidirectional_Transformers_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1593",
      "paper_id": "",
      "title": "Self-Supervised Video Representation Learning by Context and Motion Decoupling",
      "authors": "Lianghua Huang, Yu Liu, Bin Wang, Pan Pan, Yinghui Xu, Rong Jin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Self-Supervised_Video_Representation_Learning_by_Context_and_Motion_Decoupling_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_1893",
      "paper_id": "",
      "title": "Self-Supervised Video Transformer",
      "authors": "Kanchana Ranasinghe, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, Michael S. Ryoo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ranasinghe_Self-Supervised_Video_Transformer_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_477",
      "paper_id": "",
      "title": "Semantic and Sequential Alignment for Referring Video Object Segmentation",
      "authors": "Feiyu Pan, Hao Fang, Fangkai Li, Yanyu Xu, Yawei Li, Luca Benini, Xiankai Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Pan_Semantic_and_Sequential_Alignment_for_Referring_Video_Object_Segmentation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_891",
      "paper_id": "",
      "title": "Semi-Supervised Action Recognition With Temporal Contrastive Learning",
      "authors": "Ankit Singh, Omprakash Chakraborty, Ashutosh Varshney, Rameswar Panda, Rogerio Feris, Kate Saenko, Abir Das",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Singh_Semi-Supervised_Action_Recognition_With_Temporal_Contrastive_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_1438",
      "paper_id": "",
      "title": "Semi-Supervised Video Paragraph Grounding With Contrastive Encoder",
      "authors": "Xun Jiang, Xing Xu, Jingran Zhang, Fumin Shen, Zuo Cao, Heng Tao Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Semi-Supervised_Video_Paragraph_Grounding_With_Contrastive_Encoder_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_704",
      "paper_id": "",
      "title": "Semi-Weakly-Supervised Learning of Complex Actions From Instructional Task Videos",
      "authors": "Yuhan Shen, Ehsan Elhamifar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Shen_Semi-Weakly-Supervised_Learning_of_Complex_Actions_From_Instructional_Task_Videos_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_955",
      "paper_id": "",
      "title": "Seq2Time: Sequential Knowledge Transfer for Video LLM Temporal Grounding",
      "authors": "Andong Deng, Zhongpai Gao, Anwesa Choudhuri, Benjamin Planche, Meng Zheng, Bin Wang, Terrence Chen, Chen Chen, Ziyan Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Deng_Seq2Time_Sequential_Knowledge_Transfer_for_Video_LLM_Temporal_Grounding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_704",
      "paper_id": "",
      "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding",
      "authors": "Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, ShaoGuo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_SeriesBench_A_Benchmark_for_Narrative-Driven_Drama_Series_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1710",
      "paper_id": "",
      "title": "Set-Supervised Action Learning in Procedural Task Videos via Pairwise Order Consistency",
      "authors": "Zijia Lu, Ehsan Elhamifar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Set-Supervised_Action_Learning_in_Procedural_Task_Videos_via_Pairwise_Order_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2296",
      "paper_id": "",
      "title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding",
      "authors": "Yangliu Hu, Zikai Song, Na Feng, Yawei Luo, Junqing Yu, Yi-Ping Phoebe Chen, Wei Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hu_SF2T_Self-supervised_Fragment_Finetuning_of_Video-LLMs_for_Fine-Grained_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1475",
      "paper_id": "",
      "title": "Shot Contrastive Self-Supervised Learning for Scene Boundary Detection",
      "authors": "Shixing Chen, Xiaohan Nie, David Fan, Dongqing Zhang, Vimal Bhat, Raffay Hamid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Shot_Contrastive_Self-Supervised_Learning_for_Scene_Boundary_Detection_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_1565",
      "paper_id": "",
      "title": "Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding",
      "authors": "Chaolei Tan, Jianhuang Lai, Wei-Shi Zheng, Jian-Fang Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Tan_Siamese_Learning_with_Joint_Alignment_and_Regression_for_Weakly-Supervised_Video_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_161",
      "paper_id": "",
      "title": "Sketch, Ground, and Refine: Top-Down Dense Video Captioning",
      "authors": "Chaorui Deng, Shizhe Chen, Da Chen, Yuan He, Qi Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_Sketch_Ground_and_Refine_Top-Down_Dense_Video_Captioning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_2051",
      "paper_id": "",
      "title": "SLIC: Self-Supervised Learning With Iterative Clustering for Human Action Videos",
      "authors": "Salar Hosseini Khorasgani, Yuxuan Chen, Florian Shkurti",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Khorasgani_SLIC_Self-Supervised_Learning_With_Iterative_Clustering_for_Human_Action_Videos_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_264",
      "paper_id": "",
      "title": "SmartAdapt: Multi-Branch Object Detection Framework for Videos on Mobiles",
      "authors": "Ran Xu, Fangzhou Mu, Jayoung Lee, Preeti Mukherjee, Somali Chaterji, Saurabh Bagchi, Yin Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_SmartAdapt_Multi-Branch_Object_Detection_Framework_for_Videos_on_Mobiles_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1066",
      "paper_id": "",
      "title": "SMILE: Infusing Spatial and Motion Semantics in Masked Video Learning",
      "authors": "Fida Mohammad Thoker, Letian Jiang, Chen Zhao, Bernard Ghanem",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Thoker_SMILE_Infusing_Spatial_and_Motion_Semantics_in_Masked_Video_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2593",
      "paper_id": "",
      "title": "SMTPD: A New Benchmark for Temporal Prediction of Social Media Popularity",
      "authors": "Yijie Xu, Bolun Zheng, Wei Zhu, Hangjia Pan, Yuchen Yao, Ning Xu, Anan Liu, Quan Zhang, Chenggang Yan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_SMTPD_A_New_Benchmark_for_Temporal_Prediction_of_Social_Media_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2436",
      "paper_id": "",
      "title": "SnAG: Scalable and Accurate Video Grounding",
      "authors": "Fangzhou Mu, Sicheng Mo, Yin Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Mu_SnAG_Scalable_and_Accurate_Video_Grounding_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1599",
      "paper_id": "",
      "title": "Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks",
      "authors": "Hyolim Kang, Hanjung Kim, Joungbin An, Minsu Cho, Seon Joo Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Soft-Landing_Strategy_for_Alleviating_the_Task_Discrepancy_Problem_in_Temporal_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_391",
      "paper_id": "",
      "title": "SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge",
      "authors": "Andong Wang, Bo Wu, Sunli Chen, Zhenfang Chen, Haotian Guan, Wei-Ning Lee, Li Erran Li, Chuang Gan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_SOK-Bench_A_Situated_Video_Reasoning_Benchmark_with_Aligned_Open-World_Knowledge_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1756",
      "paper_id": "",
      "title": "Sound Bridge: Associating Egocentric and Exocentric Videos via Audio Cues",
      "authors": "Sihong Huang, Jiaxin Wu, Xiaoyong Wei, Yi Cai, Dongmei Jiang, Yaowei Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Sound_Bridge_Associating_Egocentric_and_Exocentric_Videos_via_Audio_Cues_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_469",
      "paper_id": "",
      "title": "Spatial-Temporal Concept Based Explanation of 3D ConvNets",
      "authors": "Ying Ji, Yu Wang, Jien Kato",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Spatial-Temporal_Concept_Based_Explanation_of_3D_ConvNets_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1344",
      "paper_id": "",
      "title": "Spatial-Then-Temporal Self-Supervised Learning for Video Correspondence",
      "authors": "Rui Li, Dong Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Spatial-Then-Temporal_Self-Supervised_Learning_for_Video_Correspondence_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1792",
      "paper_id": "",
      "title": "Spatio-Temporal Relation Modeling for Few-Shot Action Recognition",
      "authors": "Anirudh Thatipelli, Sanath Narayan, Salman Khan, Rao Muhammad Anwer, Fahad Shahbaz Khan, Bernard Ghanem",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Thatipelli_Spatio-Temporal_Relation_Modeling_for_Few-Shot_Action_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_1250",
      "paper_id": "",
      "title": "Spatiotemporal Contrastive Video Representation Learning",
      "authors": "Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, Yin Cui",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Qian_Spatiotemporal_Contrastive_Video_Representation_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1470",
      "paper_id": "",
      "title": "Spoken Moments: Learning Joint Audio-Visual Representations From Video Descriptions",
      "authors": "Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass, Aude Oliva",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Monfort_Spoken_Moments_Learning_Joint_Audio-Visual_Representations_From_Video_Descriptions_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_1094",
      "paper_id": "",
      "title": "SportsHHI: A Dataset for Human-Human Interaction Detection in Sports Videos",
      "authors": "Tao Wu, Runyu He, Gangshan Wu, Limin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_SportsHHI_A_Dataset_for_Human-Human_Interaction_Detection_in_Sports_Videos_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1008",
      "paper_id": "",
      "title": "SRTube: Video-Language Pre-Training with Action-Centric Video Tube Features and Semantic Role Labeling",
      "authors": "Ju-Hee Lee, Je-Won Kang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_SRTube_Video-Language_Pre-Training_with_Action-Centric_Video_Tube_Features_and_Semantic_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_53",
      "paper_id": "",
      "title": "SSAN: Separable Self-Attention Network for Video Representation Learning",
      "authors": "Xudong Guo, Xun Guo, Yan Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_SSAN_Separable_Self-Attention_Network_for_Video_Representation_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_963",
      "paper_id": "",
      "title": "Stand-Alone Inter-Frame Attention in Video Models",
      "authors": "Fuchen Long, Zhaofan Qiu, Yingwei Pan, Ting Yao, Jiebo Luo, Tao Mei",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Long_Stand-Alone_Inter-Frame_Attention_in_Video_Models_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_799",
      "paper_id": "",
      "title": "Step Differences in Instructional Video",
      "authors": "Tushar Nagarajan, Lorenzo Torresani",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Nagarajan_Step_Differences_in_Instructional_Video_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1382",
      "paper_id": "",
      "title": "STEP: Enhancing Video-LLMs' Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training",
      "authors": "Haiyi Qiu, Minghe Gao, Long Qian, Kaihang Pan, Qifan Yu, Juncheng Li, Wenjie Wang, Siliang Tang, Yueting Zhuang, Tat-Seng Chua",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Qiu_STEP_Enhancing_Video-LLMs_Compositional_Reasoning_by_Spatio-Temporal_Graph-guided_Self-Training_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_335",
      "paper_id": "",
      "title": "StepFormer: Self-Supervised Step Discovery and Localization in Instructional Videos",
      "authors": "Nikita Dvornik, Isma Hadji, Ran Zhang, Konstantinos G. Derpanis, Richard P. Wildes, Allan D. Jepson",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Dvornik_StepFormer_Self-Supervised_Step_Discovery_and_Localization_in_Instructional_Videos_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_937",
      "paper_id": "",
      "title": "STMixer: A One-Stage Sparse Action Detector",
      "authors": "Tao Wu, Mengqi Cao, Ziteng Gao, Gangshan Wu, Limin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_STMixer_A_One-Stage_Sparse_Action_Detector_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_199",
      "paper_id": "",
      "title": "STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding",
      "authors": "Zichen Liu, Kunlun Xu, Bing Su, Xu Zou, Yuxin Peng, Jiahuan Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_STOP_Integrated_Spatial-Temporal_Dynamic_Prompting_for_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_669",
      "paper_id": "",
      "title": "STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding",
      "authors": "Aaryan Garg, Akash Kumar, Yogesh S Rawat",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Garg_STPro_Spatial_and_Temporal_Progressive_Learning_for_Weakly_Supervised_Spatio-Temporal_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_25",
      "paper_id": "",
      "title": "Streaming Dense Video Captioning",
      "authors": "Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Streaming_Dense_Video_Captioning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1940",
      "paper_id": "",
      "title": "Streaming Video Model",
      "authors": "Yucheng Zhao, Chong Luo, Chuanxin Tang, Dongdong Chen, Noel Codella, Zheng-Jun Zha",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Streaming_Video_Model_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_634",
      "paper_id": "",
      "title": "Structured Multi-Level Interaction Network for Video Moment Localization via Language Query",
      "authors": "Hao Wang, Zheng-Jun Zha, Liang Li, Dong Liu, Jiebo Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Structured_Multi-Level_Interaction_Network_for_Video_Moment_Localization_via_Language_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_473",
      "paper_id": "",
      "title": "Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation",
      "authors": "Razvan-George Pasca, Alexey Gavryushin, Muhammad Hamza, Yen-Ling Kuo, Kaichun Mo, Luc Van Gool, Otmar Hilliges, Xi Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Pasca_Summarize_the_Past_to_Predict_the_Future_Natural_Language_Descriptions_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_263",
      "paper_id": "",
      "title": "SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning Over Traffic Events",
      "authors": "Li Xu, He Huang, Jun Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_SUTD-TrafficQA_A_Question_Answering_Benchmark_and_an_Efficient_Network_for_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_756",
      "paper_id": "",
      "title": "SVFormer: Semi-Supervised Video Transformer for Action Recognition",
      "authors": "Zhen Xing, Qi Dai, Han Hu, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Xing_SVFormer_Semi-Supervised_Video_Transformer_for_Action_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1482",
      "paper_id": "",
      "title": "SVIP: Sequence VerIfication for Procedures in Videos",
      "authors": "Yicheng Qian, Weixin Luo, Dongze Lian, Xu Tang, Peilin Zhao, Shenghua Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Qian_SVIP_Sequence_VerIfication_for_Procedures_in_Videos_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_381",
      "paper_id": "",
      "title": "SViTT: Temporal Learning of Sparse Video-Text Transformers",
      "authors": "Yi Li, Kyle Min, Subarna Tripathi, Nuno Vasconcelos",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SViTT_Temporal_Learning_of_Sparse_Video-Text_Transformers_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1442",
      "paper_id": "",
      "title": "SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situation",
      "authors": "Hao Du, Bo Wu, Yan Lu, Zhendong Mao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Du_SVLTA_Benchmarking_Vision-Language_Temporal_Alignment_via_Synthetic_Video_Situation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1312",
      "paper_id": "",
      "title": "SwinBERT: End-to-End Transformers With Sparse Attention for Video Captioning",
      "authors": "Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, Lijuan Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_SwinBERT_End-to-End_Transformers_With_Sparse_Attention_for_Video_Captioning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1355",
      "paper_id": "",
      "title": "System-Status-Aware Adaptive Network for Online Streaming Video Understanding",
      "authors": "Lin Geng Foo, Jia Gong, Zhipeng Fan, Jun Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Foo_System-Status-Aware_Adaptive_Network_for_Online_Streaming_Video_Understanding_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_264",
      "paper_id": "",
      "title": "T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval",
      "authors": "Xiaohan Wang, Linchao Zhu, Yi Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_T2VLAD_Global-Local_Sequence_Alignment_for_Text-Video_Retrieval_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_568",
      "paper_id": "",
      "title": "TAMT: Temporal-Aware Model Tuning for Cross-Domain Few-Shot Action Recognition",
      "authors": "Yilong Wang, Zilin Gao, Qilong Wang, Zhaofeng Chen, Peihua Li, Qinghua Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_TAMT_Temporal-Aware_Model_Tuning_for_Cross-Domain_Few-Shot_Action_Recognition_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1600",
      "paper_id": "",
      "title": "Task-Driven Exploration: Decoupling and Inter-Task Feedback for Joint Moment Retrieval and Highlight Detection",
      "authors": "Jin Yang, Ping Wei, Huan Li, Ziyang Ren",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Task-Driven_Exploration_Decoupling_and_Inter-Task_Feedback_for_Joint_Moment_Retrieval_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_144",
      "paper_id": "",
      "title": "TDN: Temporal Difference Networks for Efficient Action Recognition",
      "authors": "Limin Wang, Zhan Tong, Bin Ji, Gangshan Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_TDN_Temporal_Difference_Networks_for_Efficient_Action_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_995",
      "paper_id": "",
      "title": "TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression",
      "authors": "Ho-Joong Kim, Jung-Ho Hong, Heejo Kong, Seong-Whan Lee",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_TE-TAD_Towards_Full_End-to-End_Temporal_Action_Detection_via_Time-Aligned_Coordinate_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1367",
      "paper_id": "",
      "title": "Temporal Action Detection Model Compression by Progressive Block Drop",
      "authors": "Xiaoyong Chen, Yong Guo, Jiaming Liang, Sitong Zhuang, Runhao Zeng, Xiping Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Temporal_Action_Detection_Model_Compression_by_Progressive_Block_Drop_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1092",
      "paper_id": "",
      "title": "Temporal Action Segmentation From Timestamp Supervision",
      "authors": "Zhe Li, Yazan Abu Farha, Jurgen Gall",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Temporal_Action_Segmentation_From_Timestamp_Supervision_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_147",
      "paper_id": "",
      "title": "Temporal Alignment Networks for Long-Term Video",
      "authors": "Tengda Han, Weidi Xie, Andrew Zisserman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Temporal_Alignment_Networks_for_Long-Term_Video_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1005",
      "paper_id": "",
      "title": "Temporal Alignment-Free Video Matching for Few-shot Action Recognition",
      "authors": "SuBeen Lee, WonJun Moon, Hyun Seok Seong, Jae-Pil Heo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_474",
      "paper_id": "",
      "title": "Temporal Context Aggregation Network for Temporal Action Proposal Refinement",
      "authors": "Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang, Wei Wu, Xiang Wang, Yu Qiao, Junjie Yan, Changxin Gao, Nong Sang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Qing_Temporal_Context_Aggregation_Network_for_Temporal_Action_Proposal_Refinement_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_942",
      "paper_id": "",
      "title": "Temporal Query Networks for Fine-Grained Video Understanding",
      "authors": "Chuhan Zhang, Ankush Gupta, Andrew Zisserman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Temporal_Query_Networks_for_Fine-Grained_Video_Understanding_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1564",
      "paper_id": "",
      "title": "Temporal-Relational CrossTransformers for Few-Shot Action Recognition",
      "authors": "Toby Perrett, Alessandro Masullo, Tilo Burghardt, Majid Mirmehdi, Dima Damen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Perrett_Temporal-Relational_CrossTransformers_for_Few-Shot_Action_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1371",
      "paper_id": "",
      "title": "Temporally Consistent Object-Centric Learning by Contrasting Slots",
      "authors": "Anna Manasyan, Maximilian Seitzer, Filip Radovic, Georg Martius, Andrii Zadaianchuk",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_583",
      "paper_id": "",
      "title": "Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation",
      "authors": "Ming Xu, Stephen Gould",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Temporally_Consistent_Unbalanced_Optimal_Transport_for_Unsupervised_Action_Segmentation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1255",
      "paper_id": "",
      "title": "Temporally-Weighted Hierarchical Clustering for Unsupervised Action Segmentation",
      "authors": "Saquib Sarfraz, Naila Murray, Vivek Sharma, Ali Diba, Luc Van Gool, Rainer Stiefelhagen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Sarfraz_Temporally-Weighted_Hierarchical_Clustering_for_Unsupervised_Action_Segmentation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_503",
      "paper_id": "",
      "title": "Tencent-MVSE: A Large-Scale Benchmark Dataset for Multi-Modal Video Similarity Evaluation",
      "authors": "Zhaoyang Zeng, Yongsheng Luo, Zhenhua Liu, Fengyun Rao, Dian Li, Weidong Guo, Zhen Wen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_Tencent-MVSE_A_Large-Scale_Benchmark_Dataset_for_Multi-Modal_Video_Similarity_Evaluation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1302",
      "paper_id": "",
      "title": "Test of Time: Instilling Video-Language Models With a Sense of Time",
      "authors": "Piyush Bagad, Makarand Tapaswi, Cees G. M. Snoek",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bagad_Test_of_Time_Instilling_Video-Language_Models_With_a_Sense_of_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_927",
      "paper_id": "",
      "title": "Test-Time Zero-Shot Temporal Action Localization",
      "authors": "Benedetta Liberatori, Alessandro Conti, Paolo Rota, Yiming Wang, Elisa Ricci",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liberatori_Test-Time_Zero-Shot_Temporal_Action_Localization_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_969",
      "paper_id": "",
      "title": "Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval",
      "authors": "Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Text_Is_MASS_Modeling_as_Stochastic_Embedding_for_Text-Video_Retrieval_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1193",
      "paper_id": "",
      "title": "Text With Knowledge Graph Augmented Transformer for Video Captioning",
      "authors": "Xin Gu, Guang Chen, Yufei Wang, Libo Zhang, Tiejian Luo, Longyin Wen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Text_With_Knowledge_Graph_Augmented_Transformer_for_Video_Captioning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1962",
      "paper_id": "",
      "title": "Text-Visual Prompting for Efficient 2D Temporal Video Grounding",
      "authors": "Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Text-Visual_Prompting_for_Efficient_2D_Temporal_Video_Grounding_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_1328",
      "paper_id": "",
      "title": "The Blessings of Unlabeled Background in Untrimmed Videos",
      "authors": "Yuan Liu, Jingyuan Chen, Zhenfang Chen, Bing Deng, Jianqiang Huang, Hanwang Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_The_Blessings_of_Unlabeled_Background_in_Untrimmed_Videos_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1100",
      "paper_id": "",
      "title": "The Devil is in Temporal Token: High Quality Video Reasoning Segmentation",
      "authors": "Sitong Gong, Yunzhi Zhuge, Lu Zhang, Zongxin Yang, Pingping Zhang, Huchuan Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Gong_The_Devil_is_in_Temporal_Token_High_Quality_Video_Reasoning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_305",
      "paper_id": "",
      "title": "The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction",
      "authors": "Alexandros Stergiou, Dima Damen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Stergiou_The_Wisdom_of_Crowds_Temporal_Progressive_Attention_for_Early_Action_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1055",
      "paper_id": "",
      "title": "Therbligs in Action: Video Understanding Through Motion Primitives",
      "authors": "Eadom Dessalene, Michael Maynord, Cornelia Fermüller, Yiannis Aloimonos",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Dessalene_Therbligs_in_Action_Video_Understanding_Through_Motion_Primitives_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_662",
      "paper_id": "",
      "title": "Three Birds with One Stone: Multi-Task Temporal Action Detection via Recycling Temporal Annotations",
      "authors": "Zhihui Li, Lina Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Three_Birds_with_One_Stone_Multi-Task_Temporal_Action_Detection_via_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2488",
      "paper_id": "",
      "title": "TIM: A Time Interval Machine for Audio-Visual Action Recognition",
      "authors": "Jacob Chalk, Jaesung Huh, Evangelos Kazakos, Andrew Zisserman, Dima Damen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chalk_TIM_A_Time_Interval_Machine_for_Audio-Visual_Action_Recognition_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1864",
      "paper_id": "",
      "title": "TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition",
      "authors": "Ishan Rajendrakumar Dave, Mamshad Nayeem Rizve, Chen Chen, Mubarak Shah",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Dave_TimeBalance_Temporally-Invariant_and_Temporally-Distinctive_Video_Representations_for_Semi-Supervised_Action_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1213",
      "paper_id": "",
      "title": "TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding",
      "authors": "Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ren_TimeChat_A_Time-sensitive_Multimodal_Large_Language_Model_for_Long_Video_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1509",
      "paper_id": "",
      "title": "Token Turing Machines",
      "authors": "Michael S. Ryoo, Keerthana Gopalakrishnan, Kumara Kahatapitiya, Ted Xiao, Kanishka Rao, Austin Stone, Yao Lu, Julian Ibarz, Anurag Arnab",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ryoo_Token_Turing_Machines_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2314",
      "paper_id": "",
      "title": "Towards Automated Movie Trailer Generation",
      "authors": "Dawit Mureja Argaw, Mattia Soldan, Alejandro Pardo, Chen Zhao, Fabian Caba Heilbron, Joon Son Chung, Bernard Ghanem",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Argaw_Towards_Automated_Movie_Trailer_Generation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1284",
      "paper_id": "",
      "title": "Towards Bridging Event Captioner and Sentence Localizer for Weakly Supervised Dense Event Captioning",
      "authors": "Shaoxiang Chen, Yu-Gang Jiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Towards_Bridging_Event_Captioner_and_Sentence_Localizer_for_Weakly_Supervised_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_506",
      "paper_id": "",
      "title": "Towards Diverse Paragraph Captioning for Untrimmed Videos",
      "authors": "Yuqing Song, Shizhe Chen, Qin Jin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Towards_Diverse_Paragraph_Captioning_for_Untrimmed_Videos_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_924",
      "paper_id": "",
      "title": "Towards End-to-End Generative Modeling of Long Videos With Memory-Efficient Bidirectional Transformers",
      "authors": "Jaehoon Yoo, Semin Kim, Doyup Lee, Chiheon Kim, Seunghoon Hong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yoo_Towards_End-to-End_Generative_Modeling_of_Long_Videos_With_Memory-Efficient_Bidirectional_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1718",
      "paper_id": "",
      "title": "Towards Fast Adaptation of Pretrained Contrastive Models for Multi-Channel Video-Language Retrieval",
      "authors": "Xudong Lin, Simran Tiwari, Shiyuan Huang, Manling Li, Mike Zheng Shou, Heng Ji, Shih-Fu Chang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Towards_Fast_Adaptation_of_Pretrained_Contrastive_Models_for_Multi-Channel_Video-Language_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_313",
      "paper_id": "",
      "title": "Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training",
      "authors": "Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, Yang Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Towards_Generalisable_Video_Moment_Retrieval_Visual-Dynamic_Injection_to_Image-Text_Pre-Training_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_1025",
      "paper_id": "",
      "title": "Towards Long-Form Video Understanding",
      "authors": "Chao-Yuan Wu, Philipp Krahenbuhl",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Towards_Long-Form_Video_Understanding_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1429",
      "paper_id": "",
      "title": "Towards Unified Surgical Skill Assessment",
      "authors": "Daochang Liu, Qiyue Li, Tingting Jiang, Yizhou Wang, Rulin Miao, Fei Shan, Ziyu Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Towards_Unified_Surgical_Skill_Assessment_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2538",
      "paper_id": "",
      "title": "Towards Universal Soccer Video Understanding",
      "authors": "Jiayuan Rao, Haoning Wu, Hao Jiang, Ya Zhang, Yanfeng Wang, Weidi Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Rao_Towards_Universal_Soccer_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1369",
      "paper_id": "",
      "title": "Transitional Adaptation of Pretrained Models for Visual Storytelling",
      "authors": "Youngjae Yu, Jiwan Chung, Heeseung Yun, Jongseok Kim, Gunhee Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yu_Transitional_Adaptation_of_Pretrained_Models_for_Visual_Storytelling_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_666",
      "paper_id": "",
      "title": "TransRAC: Encoding Multi-Scale Temporal Correlation With Transformers for Repetitive Action Counting",
      "authors": "Huazhang Hu, Sixun Dong, Yiqun Zhao, Dongze Lian, Zhengxin Li, Shenghua Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_TransRAC_Encoding_Multi-Scale_Temporal_Correlation_With_Transformers_for_Repetitive_Action_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_131",
      "paper_id": "",
      "title": "TransRank: Self-Supervised Video Representation Learning via Ranking-Based Transformation Recognition",
      "authors": "Haodong Duan, Nanxuan Zhao, Kai Chen, Dahua Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Duan_TransRank_Self-Supervised_Video_Representation_Learning_via_Ranking-Based_Transformation_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_2209",
      "paper_id": "",
      "title": "TriDet: Temporal Action Detection With Relative Boundary Modeling",
      "authors": "Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, Dacheng Tao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_TriDet_Temporal_Action_Detection_With_Relative_Boundary_Modeling_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_296",
      "paper_id": "",
      "title": "TSAM: Temporal SAM Augmented with Multimodal Prompts for Referring Audio-Visual Segmentation",
      "authors": "Abduljalil Radman, Jorma Laaksonen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Radman_TSAM_Temporal_SAM_Augmented_with_Multimodal_Prompts_for_Referring_Audio-Visual_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_2049",
      "paper_id": "",
      "title": "TubeDETR: Spatio-Temporal Video Grounding With Transformers",
      "authors": "Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_TubeDETR_Spatio-Temporal_Video_Grounding_With_Transformers_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1909",
      "paper_id": "",
      "title": "TubeR: Tubelet Transformer for Video Action Detection",
      "authors": "Jiaojiao Zhao, Yanyi Zhang, Xinyu Li, Hao Chen, Bing Shuai, Mingze Xu, Chunhui Liu, Kaustav Kundu, Yuanjun Xiong, Davide Modolo, Ivan Marsic, Cees G. M. Snoek, Joseph Tighe",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_TubeR_Tubelet_Transformer_for_Video_Action_Detection_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_195",
      "paper_id": "",
      "title": "TULIP: Multi-camera 3D Precision Assessment of Parkinson's Disease",
      "authors": "Kyungdo Kim, Sihan Lyu, Sneha Mantri, Timothy W. Dunn",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_TULIP_Multi-camera_3D_Precision_Assessment_of_Parkinsons_Disease_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1694",
      "paper_id": "",
      "title": "Two-Stream Networks for Weakly-Supervised Temporal Action Localization With Semantic-Aware Mechanisms",
      "authors": "Yu Wang, Yadong Li, Hongbin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Two-Stream_Networks_for_Weakly-Supervised_Temporal_Action_Localization_With_Semantic-Aware_Mechanisms_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_776",
      "paper_id": "",
      "title": "UBoCo: Unsupervised Boundary Contrastive Learning for Generic Event Boundary Detection",
      "authors": "Hyolim Kang, Jinwoo Kim, Taehyun Kim, Seon Joo Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kang_UBoCo_Unsupervised_Boundary_Contrastive_Learning_for_Generic_Event_Boundary_Detection_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1760",
      "paper_id": "",
      "title": "UMT: Unified Multi-Modal Transformers for Joint Video Moment Retrieval and Highlight Detection",
      "authors": "Ye Liu, Siyuan Li, Yang Wu, Chang-Wen Chen, Ying Shan, Xiaohu Qie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_UMT_Unified_Multi-Modal_Transformers_for_Joint_Video_Moment_Retrieval_and_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1030",
      "paper_id": "",
      "title": "Unbiasing through Textual Descriptions: Mitigating Representation Bias in Video Benchmarks",
      "authors": "Nina Shvetsova, Arsha Nagrani, Bernt Schiele, Hilde Kuehne, Christian Rupprecht",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Shvetsova_Unbiasing_through_Textual_Descriptions_Mitigating_Representation_Bias_in_Video_Benchmarks_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1595",
      "paper_id": "",
      "title": "Uncertainty Guided Collaborative Training for Weakly Supervised Temporal Action Detection",
      "authors": "Wenfei Yang, Tianzhu Zhang, Xiaoyuan Yu, Tian Qi, Yongdong Zhang, Feng Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Uncertainty_Guided_Collaborative_Training_for_Weakly_Supervised_Temporal_Action_Detection_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_592",
      "paper_id": "",
      "title": "Uncertainty-aware Action Decoupling Transformer for Action Anticipation",
      "authors": "Hongji Guo, Nakul Agarwal, Shao-Yuan Lo, Kwonjoon Lee, Qiang Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_Uncertainty-aware_Action_Decoupling_Transformer_for_Action_Anticipation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_410",
      "paper_id": "",
      "title": "Uncertainty-Guided Probabilistic Transformer for Complex Action Recognition",
      "authors": "Hongji Guo, Hanjing Wang, Qiang Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Uncertainty-Guided_Probabilistic_Transformer_for_Complex_Action_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1597",
      "paper_id": "",
      "title": "Uncovering What Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly",
      "authors": "Hang Du, Sicheng Zhang, Binzhu Xie, Guoshun Nan, Jiayang Zhang, Junrui Xu, Hangyu Liu, Sicong Leng, Jiangming Liu, Hehe Fan, Dajiu Huang, Jing Feng, Linli Chen, Can Zhang, Xuhuan Li, Hao Zhang, Jianhang Chen, Qimei Cui, Xiaofeng Tao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Du_Uncovering_What_Why_and_How_A_Comprehensive_Benchmark_for_Causation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_942",
      "paper_id": "",
      "title": "Understanding Multi-Task Activities from Single-Task Videos",
      "authors": "Yuhan Shen, Ehsan Elhamifar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2335",
      "paper_id": "",
      "title": "Understanding Video Transformers via Universal Concept Discovery",
      "authors": "Matthew Kowal, Achal Dave, Rares Ambrus, Adrien Gaidon, Konstantinos G. Derpanis, Pavel Tokmakov",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kowal_Understanding_Video_Transformers_via_Universal_Concept_Discovery_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_752",
      "paper_id": "",
      "title": "UniSTD: Towards Unified Spatio-Temporal Learning across Diverse Disciplines",
      "authors": "Chen Tang, Xinzhu Ma, Encheng Su, Xiufeng Song, Xiaohong Liu, Wei-Hong Li, Lei Bai, Wanli Ouyang, Xiangyu Yue",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_UniSTD_Towards_Unified_Spatio-Temporal_Learning_across_Diverse_Disciplines_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_2011",
      "paper_id": "",
      "title": "Unsupervised Action Segmentation by Joint Representation Learning and Online Clustering",
      "authors": "Sateesh Kumar, Sanjay Haresh, Awais Ahmed, Andrey Konin, M. Zeeshan Zia, Quoc-Huy Tran",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kumar_Unsupervised_Action_Segmentation_by_Joint_Representation_Learning_and_Online_Clustering_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1219",
      "paper_id": "",
      "title": "Unsupervised Pre-Training for Temporal Action Localization Tasks",
      "authors": "Can Zhang, Tianyu Yang, Junwu Weng, Meng Cao, Jue Wang, Yuexian Zou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Unsupervised_Pre-Training_for_Temporal_Action_Localization_Tasks_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_1346",
      "paper_id": "",
      "title": "Unsupervised Visual Representation Learning by Tracking Patches in Video",
      "authors": "Guangting Wang, Yizhou Zhou, Chong Luo, Wenxuan Xie, Wenjun Zeng, Zhiwei Xiong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Unsupervised_Visual_Representation_Learning_by_Tracking_Patches_in_Video_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_825",
      "paper_id": "",
      "title": "UnweaveNet: Unweaving Activity Stories",
      "authors": "Will Price, Carl Vondrick, Dima Damen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Price_UnweaveNet_Unweaving_Activity_Stories_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1859",
      "paper_id": "",
      "title": "Use Your Head: Improving Long-Tail Video Recognition",
      "authors": "Toby Perrett, Saptarshi Sinha, Tilo Burghardt, Majid Mirmehdi, Dima Damen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Perrett_Use_Your_Head_Improving_Long-Tail_Video_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1320",
      "paper_id": "",
      "title": "VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict Entailment",
      "authors": "Darshana Saravanan, Varun Gupta, Darshan Singh, Zeeshan Khan, Vineet Gandhi, Makarand Tapaswi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Saravanan_VELOCITI_Benchmarking_Video-Language_Compositional_Reasoning_with_Strict_Entailment_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2626",
      "paper_id": "",
      "title": "VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models",
      "authors": "Muchao Ye, Weiyang Liu, Pan He",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_VERA_Explainable_Video_Anomaly_Detection_via_Verbalized_Learning_of_Vision-Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2464",
      "paper_id": "",
      "title": "VicTR: Video-conditioned Text Representations for Activity Recognition",
      "authors": "Kumara Kahatapitiya, Anurag Arnab, Arsha Nagrani, Michael S. Ryoo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kahatapitiya_VicTR_Video-conditioned_Text_Representations_for_Activity_Recognition_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1325",
      "paper_id": "",
      "title": "vid-TLDR: Training Free Token Merging for Light-weight Video Transformer",
      "authors": "Joonmyung Choi, Sanghyeok Lee, Jaewon Chu, Minhyuk Choi, Hyunwoo J. Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Choi_vid-TLDR_Training_Free_Token_Merging_for_Light-weight_Video_Transformer_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_217",
      "paper_id": "",
      "title": "Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning",
      "authors": "Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Vid2Seq_Large-Scale_Pretraining_of_a_Visual_Language_Model_for_Dense_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1067",
      "paper_id": "",
      "title": "Video Language Model Pretraining with Spatio-temporal Masking",
      "authors": "Yue Wu, Zhaobo Qi, Junshu Sun, Yaowei Wang, Qingming Huang, Shuhui Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Video_Language_Model_Pretraining_with_Spatio-temporal_Masking_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1185",
      "paper_id": "",
      "title": "Video ReCap: Recursive Captioning of Hour-Long Videos",
      "authors": "Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Islam_Video_ReCap_Recursive_Captioning_of_Hour-Long_Videos_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1283",
      "paper_id": "",
      "title": "Video Recognition in Portrait Mode",
      "authors": "Mingfei Han, Linjie Yang, Xiaojie Jin, Jiashi Feng, Xiaojun Chang, Heng Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Han_Video_Recognition_in_Portrait_Mode_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2861",
      "paper_id": "",
      "title": "Video Summarization with Large Language Models",
      "authors": "Min Jung Lee, Dayoung Gong, Minsu Cho",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Video_Summarization_with_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1413",
      "paper_id": "",
      "title": "Video Swin Transformer",
      "authors": "Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Video_Swin_Transformer_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2107",
      "paper_id": "",
      "title": "Video-ColBERT: Contextualized Late Interaction for Text-to-Video Retrieval",
      "authors": "Arun Reddy, Alexander Martin, Eugene Yang, Andrew Yates, Kate Sanders, Kenton Murray, Reno Kriz, Celso M. de Melo, Benjamin Van Durme, Rama Chellappa",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Reddy_Video-ColBERT_Contextualized_Late_Interaction_for_Text-to-Video_Retrieval_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_602",
      "paper_id": "",
      "title": "Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models",
      "authors": "Jinhui Yi, Syed Talal Wasim, Yanan Luo, Muzammal Naseer, Juergen Gall",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yi_Video-Panda_Parameter-efficient_Alignment_for_Encoder-free_Video-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_996",
      "paper_id": "",
      "title": "Video-Text As Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning",
      "authors": "Peng Jin, Jinfa Huang, Pengfei Xiong, Shangxuan Tian, Chang Liu, Xiangyang Ji, Li Yuan, Jie Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Video-Text_As_Game_Players_Hierarchical_Banzhaf_Interaction_for_Cross-Modal_Representation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_287",
      "paper_id": "",
      "title": "Video-Text Representation Learning via Differentiable Weak Temporal Alignment",
      "authors": "Dohwan Ko, Joonmyung Choi, Juyeon Ko, Shinyeong Noh, Kyoung-Woon On, Eun-Sol Kim, Hyunwoo J. Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ko_Video-Text_Representation_Learning_via_Differentiable_Weak_Temporal_Alignment_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1835",
      "paper_id": "",
      "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding",
      "authors": "Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, Bo Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_395",
      "paper_id": "",
      "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models",
      "authors": "Dahun Kim, AJ Piergiovanni, Ganesh Mallya, Anelia Angelova",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_VideoComp_Advancing_Fine-Grained_Compositional_and_Temporal_Alignment_in_Video-Text_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1604",
      "paper_id": "",
      "title": "VideoCon: Robust Video-Language Alignment via Contrast Captions",
      "authors": "Hritik Bansal, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang, Aditya Grover",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Bansal_VideoCon_Robust_Video-Language_Alignment_via_Contrast_Captions_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1508",
      "paper_id": "",
      "title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection",
      "authors": "Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, Si Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2177",
      "paper_id": "",
      "title": "VideoGEM: Training-free Action Grounding in Videos",
      "authors": "Felix Vogel, Walid Bousselham, Anna Kukleva, Nina Shvetsova, Hilde Kuehne",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Vogel_VideoGEM_Training-free_Action_Grounding_in_Videos_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_143",
      "paper_id": "",
      "title": "VideoGrounding-DINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding",
      "authors": "Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wasim_VideoGrounding-DINO_Towards_Open-Vocabulary_Spatio-Temporal_Video_Grounding_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_187",
      "paper_id": "",
      "title": "VideoLLM-online: Online Video Large Language Model for Streaming Video",
      "authors": "Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, Mike Zheng Shou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_VideoLLM-online_Online_Video_Large_Language_Model_for_Streaming_Video_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_778",
      "paper_id": "",
      "title": "VideoMAC: Video Masked Autoencoders Meet ConvNets",
      "authors": "Gensheng Pei, Tao Chen, Xiruo Jiang, Huafeng Liu, Zeren Sun, Yazhou Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Pei_VideoMAC_Video_Masked_Autoencoders_Meet_ConvNets_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_207",
      "paper_id": "",
      "title": "VideoMAE V2: Scaling Video Masked Autoencoders With Dual Masking",
      "authors": "Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, Yu Qiao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_VideoMAE_V2_Scaling_Video_Masked_Autoencoders_With_Dual_Masking_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_58",
      "paper_id": "",
      "title": "VideoMoCo: Contrastive Video Representation Learning With Temporally Adversarial Examples",
      "authors": "Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, Wei Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_VideoMoCo_Contrastive_Video_Representation_Learning_With_Temporally_Adversarial_Examples_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2788",
      "paper_id": "",
      "title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM",
      "authors": "Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, Yueting Zhuang, Jianke Zhu, Lidong Bing",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yuan_VideoRefer_Suite_Advancing_Spatial-Temporal_Object_Understanding_with_Video_LLM_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1809",
      "paper_id": "",
      "title": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos",
      "authors": "Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, Mohit Bansal",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_VideoTree_Adaptive_Tree-based_Video_Representation_for_LLM_Reasoning_on_Long_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1239",
      "paper_id": "",
      "title": "VidLA: Video-Language Alignment at Scale",
      "authors": "Mamshad Nayeem Rizve, Fan Fei, Jayakrishnan Unnikrishnan, Son Tran, Benjamin Z. Yao, Belinda Zeng, Mubarak Shah, Trishul Chilimbi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Rizve_VidLA_Video-Language_Alignment_at_Scale_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2840",
      "paper_id": "",
      "title": "Viewpoint Rosetta Stone: Unlocking Unpaired Ego-Exo Videos for View-invariant Representation Learning",
      "authors": "Mi Luo, Zihui Xue, Alex Dimakis, Kristen Grauman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_2037",
      "paper_id": "",
      "title": "VindLU: A Recipe for Effective Video-and-Language Pretraining",
      "authors": "Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit Bansal, Gedas Bertasius",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_VindLU_A_Recipe_for_Effective_Video-and-Language_Pretraining_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1798",
      "paper_id": "",
      "title": "Vision Transformers Are Parameter-Efficient Audio-Visual Learners",
      "authors": "Yan-Bo Lin, Yi-Lin Sung, Jie Lei, Mohit Bansal, Gedas Bertasius",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1816",
      "paper_id": "",
      "title": "VISTA-LLAMA: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens",
      "authors": "Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, Yi Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_VISTA-LLAMA_Reducing_Hallucination_in_Video_Language_Models_via_Equal_Distance_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_183",
      "paper_id": "",
      "title": "VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation",
      "authors": "Weiming Ren, Huan Yang, Jie Min, Cong Wei, Wenhu Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ren_VISTA_Enhancing_Long-Duration_and_High-Resolution_Video_Understanding_by_Video_Spatiotemporal_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_910",
      "paper_id": "",
      "title": "Visual Objectification in Films: Towards a New AI Task for Video Interpretation",
      "authors": "Julie Tores, Lucile Sassatelli, Hui-Yin Wu, Clement Bergman, Léa Andolfi, Victor Ecrement, Frédéric Precioso, Thierry Devars, Magali Guaresi, Virginie Julliard, Sarah Lecossais",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Tores_Visual_Objectification_in_Films_Towards_a_New_AI_Task_for_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1232",
      "paper_id": "",
      "title": "Visual Semantic Role Labeling for Video Understanding",
      "authors": "Arka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia, Aniruddha Kembhavi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Sadhu_Visual_Semantic_Role_Labeling_for_Video_Understanding_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_2044",
      "paper_id": "",
      "title": "Vita-CLIP: Video and Text Adaptive CLIP via Multimodal Prompting",
      "authors": "Syed Talal Wasim, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, Mubarak Shah",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wasim_Vita-CLIP_Video_and_Text_Adaptive_CLIP_via_Multimodal_Prompting_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_491",
      "paper_id": "",
      "title": "VITED: Video Temporal Evidence Distillation",
      "authors": "Yujie Lu, Yale Song, William Wang, Lorenzo Torresani, Tushar Nagarajan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lu_VITED_Video_Temporal_Evidence_Distillation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1008",
      "paper_id": "",
      "title": "VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary",
      "authors": "Kevin Qinghong Lin, Mike Zheng Shou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_VLog_Video-Language_Models_by_Generative_Retrieval_of_Narration_Vocabulary_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_106",
      "paper_id": "",
      "title": "VoP: Text-Video Co-Operative Prompt Tuning for Cross-Modal Retrieval",
      "authors": "Siteng Huang, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, Donglin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_VoP_Text-Video_Co-Operative_Prompt_Tuning_for_Cross-Modal_Retrieval_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1781",
      "paper_id": "",
      "title": "VRDFormer: End-to-End Video Visual Relation Detection With Transformers",
      "authors": "Sipeng Zheng, Shizhe Chen, Qin Jin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_VRDFormer_End-to-End_Video_Visual_Relation_Detection_With_Transformers_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_427",
      "paper_id": "",
      "title": "VTimeLLM: Empower LLM to Grasp Video Moments",
      "authors": "Bin Huang, Xin Wang, Hong Chen, Zihan Song, Wenwu Zhu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_VTimeLLM_Empower_LLM_to_Grasp_Video_Moments_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_546",
      "paper_id": "",
      "title": "Vx2Text: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs",
      "authors": "Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh, Lorenzo Torresani",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Vx2Text_End-to-End_Learning_of_Video-Based_Text_Generation_From_Multimodal_Inputs_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_200",
      "paper_id": "",
      "title": "Weakly Supervised Action Selection Learning in Video",
      "authors": "Junwei Ma, Satya Krishna Gorti, Maksims Volkovs, Guangwei Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Ma_Weakly_Supervised_Action_Selection_Learning_in_Video_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_705",
      "paper_id": "",
      "title": "Weakly Supervised Temporal Action Localization via Dual-Prior Collaborative Learning Guided by Multimodal Large Language Models",
      "authors": "Quan Zhang, Jinwei Fang, Rui Yuan, Xi Tang, Yuxin Qi, Ke Zhang, Chun Yuan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Weakly_Supervised_Temporal_Action_Localization_via_Dual-Prior_Collaborative_Learning_Guided_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_466",
      "paper_id": "",
      "title": "Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation",
      "authors": "Linjiang Huang, Liang Wang, Hongsheng Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_Weakly_Supervised_Temporal_Action_Localization_via_Representative_Snippet_Knowledge_Propagation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1483",
      "paper_id": "",
      "title": "Weakly Supervised Temporal Sentence Grounding With Gaussian-Based Contrastive Proposal Learning",
      "authors": "Minghang Zheng, Yanjie Huang, Qingchao Chen, Yuxin Peng, Yang Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Weakly_Supervised_Temporal_Sentence_Grounding_With_Gaussian-Based_Contrastive_Proposal_Learning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_2224",
      "paper_id": "",
      "title": "Weakly Supervised Temporal Sentence Grounding With Uncertainty-Guided Self-Training",
      "authors": "Yifei Huang, Lijin Yang, Yoichi Sato",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Weakly_Supervised_Temporal_Sentence_Grounding_With_Uncertainty-Guided_Self-Training_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_253",
      "paper_id": "",
      "title": "Weakly Supervised Video Representation Learning With Unaligned Text for Sequential Videos",
      "authors": "Sixun Dong, Huazhang Hu, Dongze Lian, Weixin Luo, Yicheng Qian, Shenghua Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Weakly_Supervised_Video_Representation_Learning_With_Unaligned_Text_for_Sequential_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1346",
      "paper_id": "",
      "title": "Weakly-Supervised Online Action Segmentation in Multi-View Instructional Videos",
      "authors": "Reza Ghoddoosian, Isht Dwivedi, Nakul Agarwal, Chiho Choi, Behzad Dariush",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ghoddoosian_Weakly-Supervised_Online_Action_Segmentation_in_Multi-View_Instructional_Videos_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_234",
      "paper_id": "",
      "title": "What When and Where? Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions",
      "authors": "Brian Chen, Nina Shvetsova, Andrew Rouditchenko, Daniel Kondermann, Samuel Thomas, Shih-Fu Chang, Rogerio Feris, James Glass, Hilde Kuehne",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_What_When_and_Where_Self-Supervised_Spatio-Temporal_Grounding_in_Untrimmed_Multi-Action_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1410",
      "paper_id": "",
      "title": "When the Future Becomes the Past: Taming Temporal Correspondence for Self-supervised Video Representation Learning",
      "authors": "Yang Liu, Qianqian Xu, Peisong Wen, Siran Dai, Qingming Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_When_the_Future_Becomes_the_Past_Taming_Temporal_Correspondence_for_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_2169",
      "paper_id": "",
      "title": "Where Is My Wallet? Modeling Object Proposal Sets for Egocentric Visual Query Localization",
      "authors": "Mengmeng Xu, Yanghao Li, Cheng-Yang Fu, Bernard Ghanem, Tao Xiang, Juan-Manuel Pérez-Rúa",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Where_Is_My_Wallet_Modeling_Object_Proposal_Sets_for_Egocentric_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_939",
      "paper_id": "",
      "title": "Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos",
      "authors": "Sagnik Majumder, Tushar Nagarajan, Ziad Al-Halah, Reina Pradhan, Kristen Grauman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1508",
      "paper_id": "",
      "title": "Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos",
      "authors": "Kumaranage Ravindu Yasas Nagasinghe, Honglu Zhou, Malitha Gunawardhana, Martin Renqiang Min, Daniel Harari, Muhammad Haris Khan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Nagasinghe_Why_Not_Use_Your_Textbook_Knowledge-Enhanced_Procedure_Planning_of_Instructional_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_478",
      "paper_id": "",
      "title": "WINNER: Weakly-Supervised hIerarchical decompositioN and aligNment for Spatio-tEmporal Video gRounding",
      "authors": "Mengze Li, Han Wang, Wenqiao Zhang, Jiaxu Miao, Zhou Zhao, Shengyu Zhang, Wei Ji, Fei Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_WINNER_Weakly-Supervised_hIerarchical_decompositioN_and_aligNment_for_Spatio-tEmporal_Video_gRounding_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_910",
      "paper_id": "",
      "title": "WOAD: Weakly Supervised Online Action Detection in Untrimmed Videos",
      "authors": "Mingfei Gao, Yingbo Zhou, Ran Xu, Richard Socher, Caiming Xiong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_WOAD_Weakly_Supervised_Online_Action_Detection_in_Untrimmed_Videos_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_367",
      "paper_id": "",
      "title": "X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization",
      "authors": "Anna Kukleva, Fadime Sener, Edoardo Remelli, Bugra Tekin, Eric Sauser, Bernt Schiele, Shugao Ma",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kukleva_X-MIC_Cross-Modal_Instance_Conditioning_for_Egocentric_Action_Generalization_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1858",
      "paper_id": "",
      "title": "X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval",
      "authors": "Satya Krishna Gorti, Noël Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, Guangwei Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Gorti_X-Pool_Cross-Modal_Language-Video_Attention_for_Text-Video_Retrieval_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_930",
      "paper_id": "",
      "title": "You Can Ground Earlier Than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos",
      "authors": "Xiang Fang, Daizong Liu, Pan Zhou, Guoshun Nan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_You_Can_Ground_Earlier_Than_See_An_Effective_and_Efficient_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    }
  ]
}