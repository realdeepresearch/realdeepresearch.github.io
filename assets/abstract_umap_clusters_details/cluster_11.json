{
  "cluster_id": 11,
  "papers": [
    {
      "id": "cvpr21_326",
      "paper_id": "",
      "title": "A Multiplexed Network for End-to-End, Multilingual OCR",
      "authors": "Jing Huang, Guan Pang, Rama Kovvuri, Mandy Toh, Kevin J Liang, Praveen Krishnan, Xi Yin, Tal Hassner",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_A_Multiplexed_Network_for_End-to-End_Multilingual_OCR_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2142",
      "paper_id": "",
      "title": "A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions",
      "authors": "Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, Adriana Romero-Soriano",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Urbanek_A_Picture_is_Worth_More_Than_77_Text_Tokens_Evaluating_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1162",
      "paper_id": "",
      "title": "A Probabilistic Graphical Model Based on Neural-Symbolic Reasoning for Visual Relationship Detection",
      "authors": "Dongran Yu, Bo Yang, Qianhao Wei, Anchen Li, Shirui Pan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_A_Probabilistic_Graphical_Model_Based_on_Neural-Symbolic_Reasoning_for_Visual_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1639",
      "paper_id": "",
      "title": "A Simple yet Effective Layout Token in Large Language Models for Document Understanding",
      "authors": "Zhaoqing Zhu, Chuwei Luo, Zirui Shao, Feiyu Gao, Hangdi Xing, Qi Zheng, Ji Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_A_Simple_yet_Effective_Layout_Token_in_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1351",
      "paper_id": "",
      "title": "A Vision Check-up for Language Models",
      "authors": "Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Sharma_A_Vision_Check-up_for_Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_612",
      "paper_id": "",
      "title": "Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution",
      "authors": "Chi Zhang, Baoxiong Jia, Song-Chun Zhu, Yixin Zhu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Abstract_Spatial-Temporal_Reasoning_via_Probabilistic_Abduction_and_Execution_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1996",
      "paper_id": "",
      "title": "Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's Progressive Matrices",
      "authors": "Jingyi Xu, Tushar Vaidya, Yufei Wu, Saket Chandra, Zhangsheng Lai, Kai Fong Ernest Chong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Abstract_Visual_Reasoning_An_Algebraic_Approach_for_Solving_Ravens_Progressive_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2604",
      "paper_id": "",
      "title": "Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation",
      "authors": "Andrea Maracani, Savas Ozkan, Sijun Cho, Hyowon Kim, Eunchung Noh, Jeongwon Min, Cho Jung Min, Dookun Park, Mete Ozay",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Maracani_Accurate_Scene_Text_Recognition_with_Efficient_Model_Scaling_and_Cloze_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_142",
      "paper_id": "",
      "title": "ACRE: Abstract Causal REasoning Beyond Covariation",
      "authors": "Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, Yixin Zhu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_ACRE_Abstract_Causal_REasoning_Beyond_Covariation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_165",
      "paper_id": "",
      "title": "Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding",
      "authors": "Han Xiao, Yina Xie, Guanxin Tan, Yinghao Chen, Rui Hu, Ke Wang, Aojun Zhou, Hao Li, Hao Shao, Xudong Lu, Peng Gao, Yafei Wen, Xiaoxin Chen, Shuai Ren, Hongsheng Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xiao_Adaptive_Markup_Language_Generation_for_Contextually-Grounded_Visual_Document_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_2172",
      "paper_id": "",
      "title": "Advancing Visual Grounding With Scene Knowledge: Benchmark and Method",
      "authors": "Zhihong Chen, Ruifei Zhang, Yibing Song, Xiang Wan, Guanbin Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Advancing_Visual_Grounding_With_Scene_Knowledge_Benchmark_and_Method_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_552",
      "paper_id": "",
      "title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages",
      "authors": "Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kukreja, Mykola Maslych, Wafa Al Ghallabi, Mihail Minkov Mihaylov, Chao Qin, Abdelrahman M. Shaker, Mike Zhang, Mahardika Krisna Ihsani, Amiel Gian Esplana, Monil Gokani, Shachar Mirkin, Harsh Singh, Ashay Srivastava, Endre Hamerlik, Fathinah Asma Izzati, Fadillah Adamsyah Maani, Sebastian Cavada, Jenny Chim, Rohit Gupta, Sanjay Manjunath, Kamila Zhumakhanova, Feno Heriniaina Rabevohitra, Azril Hafizi Amirudin, Muhammad Ridzuan, Daniya Najiha Abdul Kareem, Ketan Pravin More, Kunyang Li, Pramesh Shakya, Muhammad Saad, Amirpouya Ghasemaghaei, Amirbek Djanibekov, Dilshod Azizov, Branislava Jankovic, Naman Bhatia, Alvaro Cabrera, Johan Obando-Ceron, Olympiah Otieno, Febian Farestam, Muztoba Rabbani, Sanoojan Ballah, Santosh Sanjeev, Abduragim Shtanchaev, Maheen Fatima, Thao Nguyen, Amrin Kareem, Toluwani Aremu, Nathan Augusto Zacarias Xavier, Amit Bhatkal, Hawau Olamide Toyin, Aman Chadha, Hisham Cholakkal, Rao Muhammad Anwer, Michael Felsberg, Jorma Laaksonen, Thamar Solorio, Monojit Choudhury, Ivan Laptev, Mubarak Shah, Salman Khan, Fahad Shahbaz Khan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_42",
      "paper_id": "",
      "title": "An Empirical Study of Scaling Law for Scene Text Recognition",
      "authors": "Miao Rang, Zhenni Bi, Chuanjian Liu, Yunhe Wang, Kai Han",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Rang_An_Empirical_Study_of_Scaling_Law_for_Scene_Text_Recognition_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_585",
      "paper_id": "",
      "title": "Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception",
      "authors": "Yuanchen Wu, Lu Zhang, Hang Yao, Junlong Du, Ke Yan, Shouhong Ding, Yunsheng Wu, Xiaoqiang Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Antidote_A_Unified_Framework_for_Mitigating_LVLM_Hallucinations_in_Counterfactual_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1640",
      "paper_id": "",
      "title": "Are Deep Neural Networks SMARTer Than Second Graders?",
      "authors": "Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, Kevin A. Smith, Joshua B. Tenenbaum",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cherian_Are_Deep_Neural_Networks_SMARTer_Than_Second_Graders_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1559",
      "paper_id": "",
      "title": "Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought",
      "authors": "Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang, Zhiding Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Man_Argus_Vision-Centric_Reasoning_with_Grounded_Chain-of-Thought_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_264",
      "paper_id": "",
      "title": "Atom-Level Optical Chemical Structure Recognition with Limited Supervision",
      "authors": "Martijn Oldenhof, Edward De Brouwer, Adam Arany, Yves Moreau",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Oldenhof_Atom-Level_Optical_Chemical_Structure_Recognition_with_Limited_Supervision_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2383",
      "paper_id": "",
      "title": "Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation",
      "authors": "Yuhui Zhang, Yuchang Su, Yiming Liu, Xiaohan Wang, James Burgess, Elaine Sui, Chenyu Wang, Josiah Aklilu, Alejandro Lozano, Anjiang Wei, Ludwig Schmidt, Serena Yeung-Levy",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Automated_Generation_of_Challenging_Multiple-Choice_Questions_for_Vision_Language_Model_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_709",
      "paper_id": "",
      "title": "BACON: Improving Clarity of Image Captions via Bag-of-Concept Graphs",
      "authors": "Zhantao Yang, Ruili Feng, Keyu Yan, Huangji Wang, Zhicai Wang, Shangwen Zhu, Han Zhang, Jie Xiao, Pingyu Wu, Kai Zhu, Jixuan Chen, Chen-Wei Xie, Yue Yang, Hongyang Zhang, Yu Liu, Fan Cheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_BACON_Improving_Clarity_of_Image_Captions_via_Bag-of-Concept_Graphs_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1195",
      "paper_id": "",
      "title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning",
      "authors": "Fan Lu, Wei Wu, Kecheng Zheng, Shuailei Ma, Biao Gong, Jiawei Liu, Wei Zhai, Yang Cao, Yujun Shen, Zheng-Jun Zha",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lu_Benchmarking_Large_Vision-Language_Models_via_Directed_Scene_Graph_for_Comprehensive_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_2039",
      "paper_id": "",
      "title": "Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning",
      "authors": "Chia-Wen Kuo, Zsolt Kira",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kuo_Beyond_a_Pre-Trained_Object_Detector_Cross-Modal_Textual_and_Visual_Context_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1621",
      "paper_id": "",
      "title": "Beyond Sight: Towards Cognitive Alignment in LVLM via Enriched Visual Knowledge",
      "authors": "Yaqi Zhao, Yuanyang Yin, Lin Li, Mingan Lin, Victor Shea-Jay Huang, Siwei Chen, Weipeng Chen, Baoqun Yin, Zenan Zhou, Wentao Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_Beyond_Sight_Towards_Cognitive_Alignment_in_LVLM_via_Enriched_Visual_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_793",
      "paper_id": "",
      "title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension",
      "authors": "Lei Zhu, Fangyun Wei, Yanye Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Beyond_Text_Frozen_Large_Language_Models_in_Visual_Signal_Comprehension_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_498",
      "paper_id": "",
      "title": "Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events",
      "authors": "Aditya Chinchure, Sahithya Ravi, Raymond Ng, Vered Shwartz, Boyang Li, Leonid Sigal",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chinchure_Black_Swan_Abductive_and_Defeasible_Video_Reasoning_in_Unpredictable_Events_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_192",
      "paper_id": "",
      "title": "Bottom-Up Shift and Reasoning for Referring Image Segmentation",
      "authors": "Sibei Yang, Meng Xia, Guanbin Li, Hong-Yu Zhou, Yizhou Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Bottom-Up_Shift_and_Reasoning_for_Referring_Image_Segmentation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_1274",
      "paper_id": "",
      "title": "Bridging the Gap Between End-to-End and Two-Step Text Spotting",
      "authors": "Mingxin Huang, Hongliang Li, Yuliang Liu, Xiang Bai, Lianwen Jin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_Bridging_the_Gap_Between_End-to-End_and_Two-Step_Text_Spotting_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1237",
      "paper_id": "",
      "title": "BTS: A Bi-Lingual Benchmark for Text Segmentation in the Wild",
      "authors": "Xixi Xu, Zhongang Qi, Jianqi Ma, Honglun Zhang, Ying Shan, Xiaohu Qie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_BTS_A_Bi-Lingual_Benchmark_for_Text_Segmentation_in_the_Wild_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_8",
      "paper_id": "",
      "title": "CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models",
      "authors": "Kiet A. Nguyen, Adheesh Juvekar, Tianjiao Yu, Muntasir Wahed, Ismini Lourentzou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Nguyen_CALICO_Part-Focused_Semantic_Co-Segmentation_with_Large_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1363",
      "paper_id": "",
      "title": "Can Large Vision-Language Models Correct Semantic Grounding Errors By Themselves?",
      "authors": "Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liao_Can_Large_Vision-Language_Models_Correct_Semantic_Grounding_Errors_By_Themselves_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_902",
      "paper_id": "",
      "title": "Can Machines Understand Composition? Dataset and Benchmark for Photographic Image Composition Embedding and Understanding",
      "authors": "Zhaoran Zhao, Peng Lu, Anran Zhang, Peipei Li, Xia Li, Xuannan Liu, Yang Hu, Shiyi Chen, Liwei Wang, Wenhao Guo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1607",
      "paper_id": "",
      "title": "Causal Attention for Vision-Language Tasks",
      "authors": "Xu Yang, Hanwang Zhang, Guojun Qi, Jianfei Cai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Causal_Attention_for_Vision-Language_Tasks_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2568",
      "paper_id": "",
      "title": "Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models",
      "authors": "Shitian Zhao, Zhuowan Li, Yadong Lu, Alan Yuille, Yan Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Causal-CoG_A_Causal-Effect_Look_at_Context_Generation_for_Boosting_Multi-modal_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2090",
      "paper_id": "",
      "title": "Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding",
      "authors": "Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, Li Yuan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Jin_Chat-UniVi_Unified_Visual_Representation_Empowers_Large_Language_Models_with_Image_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2422",
      "paper_id": "",
      "title": "Choose What You Need: Disentangled Representation Learning for Scene Text Recognition Removal and Editing",
      "authors": "Boqiang Zhang, Hongtao Xie, Zuan Gao, Yuxin Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Choose_What_You_Need_Disentangled_Representation_Learning_for_Scene_Text_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_224",
      "paper_id": "",
      "title": "ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models",
      "authors": "Hao Yin, Guangzong Si, Zilei Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yin_ClearSight_Visual_Signal_Enhancement_for_Object_Hallucination_Mitigation_in_Multimodal_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_341",
      "paper_id": "",
      "title": "CLIP is Almost All You Need: Towards Parameter-Efficient Scene Text Retrieval without OCR",
      "authors": "Xugong Qin, Peng Zhang, Jun Jie Ou Yang, Gangyan Zeng, Yubo Li, Yuanyuan Wang, Wanqian Zhang, Pengwen Dai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Qin_CLIP_is_Almost_All_You_Need_Towards_Parameter-Efficient_Scene_Text_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_421",
      "paper_id": "",
      "title": "CLOVA: A Closed-LOop Visual Assistant with Tool Usage and Update",
      "authors": "Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, Qing Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_CLOVA_A_Closed-LOop_Visual_Assistant_with_Tool_Usage_and_Update_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_354",
      "paper_id": "",
      "title": "CoDi-2: In-Context Interleaved and Interactive Any-to-Any Generation",
      "authors": "Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, Mohit Bansal",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Tang_CoDi-2_In-Context_Interleaved_and_Interactive_Any-to-Any_Generation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2600",
      "paper_id": "",
      "title": "CoG-DQA: Chain-of-Guiding Learning with Large Language Models for Diagram Question Answering",
      "authors": "Shaowei Wang, Lingling Zhang, Longji Zhu, Tao Qin, Kim-Hui Yap, Xinyu Zhang, Jun Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_CoG-DQA_Chain-of-Guiding_Learning_with_Large_Language_Models_for_Diagram_Question_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_872",
      "paper_id": "",
      "title": "Collaborative Transformers for Grounded Situation Recognition",
      "authors": "Junhyeong Cho, Youngseok Yoon, Suha Kwak",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Cho_Collaborative_Transformers_for_Grounded_Situation_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1776",
      "paper_id": "",
      "title": "CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation",
      "authors": "Wei Chen, Lin Li, Yongqi Yang, Bin Wen, Fan Yang, Tingting Gao, Yu Wu, Long Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_993",
      "paper_id": "",
      "title": "Composing Photos Like a Photographer",
      "authors": "Chaoyi Hong, Shuaiyuan Du, Ke Xian, Hao Lu, Zhiguo Cao, Weicai Zhong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Composing_Photos_Like_a_Photographer_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2710",
      "paper_id": "",
      "title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
      "authors": "Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Mitra_Compositional_Chain-of-Thought_Prompting_for_Large_Multimodal_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_910",
      "paper_id": "",
      "title": "Comprehending and Ordering Semantics for Image Captioning",
      "authors": "Yehao Li, Yingwei Pan, Ting Yao, Tao Mei",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Comprehending_and_Ordering_Semantics_for_Image_Captioning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_219",
      "paper_id": "",
      "title": "Connecting What To Say With Where To Look by Modeling Human Attention Traces",
      "authors": "Zihang Meng, Licheng Yu, Ning Zhang, Tamara L. Berg, Babak Damavandi, Vikas Singh, Amy Bearman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Meng_Connecting_What_To_Say_With_Where_To_Look_by_Modeling_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_253",
      "paper_id": "",
      "title": "Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering",
      "authors": "Zaid Khan, Yun Fu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Khan_Consistency_and_Uncertainty_Identifying_Unreliable_Responses_From_Black-Box_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_479",
      "paper_id": "",
      "title": "Continual SFT Matches Multimodal RLHF with Negative Supervision",
      "authors": "Ke Zhu, Yu Wang, Yanpeng Sun, Qiang Chen, Jiangjiang Liu, Gang Zhang, Jingdong Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Continual_SFT_Matches_Multimodal_RLHF_with_Negative_Supervision_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1790",
      "paper_id": "",
      "title": "Contrastive Grouping With Transformer for Referring Image Segmentation",
      "authors": "Jiajin Tang, Ge Zheng, Cheng Shi, Sibei Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Contrastive_Grouping_With_Transformer_for_Referring_Image_Segmentation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2764",
      "paper_id": "",
      "title": "CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models",
      "authors": "Yiqi Zhu, Ziyue Wang, Can Zhang, Peng Li, Yang Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_CoSpace_Benchmarking_Continuous_Space_Perception_Ability_for_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_938",
      "paper_id": "",
      "title": "Counterfactual VQA: A Cause-Effect Look at Language Bias",
      "authors": "Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, Ji-Rong Wen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Niu_Counterfactual_VQA_A_Cause-Effect_Look_at_Language_Bias_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2418",
      "paper_id": "",
      "title": "CountLLM: Towards Generalizable Repetitive Action Counting via Large Language Model",
      "authors": "Ziyu Yao, Xuxin Cheng, Zhiqi Huang, Lei Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yao_CountLLM_Towards_Generalizable_Repetitive_Action_Counting_via_Large_Language_Model_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1400",
      "paper_id": "",
      "title": "CREPE: Can Vision-Language Foundation Models Reason Compositionally?",
      "authors": "Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, Ranjay Krishna",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_CREPE_Can_Vision-Language_Foundation_Models_Reason_Compositionally_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_48",
      "paper_id": "",
      "title": "CRIS: CLIP-Driven Referring Image Segmentation",
      "authors": "Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, Tongliang Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_329",
      "paper_id": "",
      "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
      "authors": "Di Zhang, Jingdi Lei, Junxian Li, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Critic-V_VLM_Critics_Help_Catch_VLM_Errors_in_Multimodal_Reasoning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2841",
      "paper_id": "",
      "title": "Cross-modal Information Flow in Multimodal Large Language Models",
      "authors": "Zhi Zhang, Srishti Yadav, Fengze Han, Ekaterina Shutova",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Cross-modal_Information_Flow_in_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_93",
      "paper_id": "",
      "title": "Curriculum Point Prompting for Weakly-Supervised Referring Image Segmentation",
      "authors": "Qiyuan Dai, Sibei Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Dai_Curriculum_Point_Prompting_for_Weakly-Supervised_Referring_Image_Segmentation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_104",
      "paper_id": "",
      "title": "Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization",
      "authors": "Zefeng Zhang, Hengzhu Tang, Jiawei Sheng, Zhenyu Zhang, Yiming Ren, Zhenyang Li, Dawei Yin, Duohe Ma, Tingwen Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Debiasing_Multimodal_Large_Language_Models_via_Noise-Aware_Preference_Optimization_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1258",
      "paper_id": "",
      "title": "DeeCap: Dynamic Early Exiting for Efficient Image Captioning",
      "authors": "Zhengcong Fei, Xu Yan, Shuhui Wang, Qi Tian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Fei_DeeCap_Dynamic_Early_Exiting_for_Efficient_Image_Captioning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_1537",
      "paper_id": "",
      "title": "DeepSolo: Let Transformer Decoder With Explicit Points Solo for Text Spotting",
      "authors": "Maoyuan Ye, Jing Zhang, Shanshan Zhao, Juhua Liu, Tongliang Liu, Bo Du, Dacheng Tao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_DeepSolo_Let_Transformer_Decoder_With_Explicit_Points_Solo_for_Text_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2688",
      "paper_id": "",
      "title": "Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens",
      "authors": "Zhangqi Jiang, Junkai Chen, Beier Zhu, Tingjin Luo, Yankun Shen, Xu Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jiang_Devils_in_Middle_Layers_of_Large_Vision-Language_Models_Interpreting_Detecting_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1149",
      "paper_id": "",
      "title": "Dictionary-Guided Scene Text Recognition",
      "authors": "Nguyen Nguyen, Thu Nguyen, Vinh Tran, Minh-Triet Tran, Thanh Duc Ngo, Thien Huu Nguyen, Minh Hoai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Nguyen_Dictionary-Guided_Scene_Text_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_124",
      "paper_id": "",
      "title": "DIEM: Decomposition-Integration Enhancing Multimodal Insights",
      "authors": "Xinyi Jiang, Guoming Wang, Junhao Guo, Juncheng Li, Wenqiao Zhang, Rongxing Lu, Siliang Tang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_DIEM_Decomposition-Integration_Enhancing_Multimodal_Insights_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_501",
      "paper_id": "",
      "title": "DIFNet: Boosting Visual Information Flow for Image Captioning",
      "authors": "Mingrui Wu, Xuying Zhang, Xiaoshuai Sun, Yiyi Zhou, Chao Chen, Jiaxin Gu, Xing Sun, Rongrong Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_DIFNet_Boosting_Visual_Information_Flow_for_Image_Captioning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_162",
      "paper_id": "",
      "title": "DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery",
      "authors": "Utkarsh Mall, Cheng Perng Phoo, Mia Chiquier, Bharath Hariharan, Kavita Bala, Carl Vondrick",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Mall_DiSciPLE_Learning_Interpretable_Programs_for_Scientific_Visual_Discovery_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_451",
      "paper_id": "",
      "title": "Divide and Conquer: Answering Questions With Object Factorization and Compositional Reasoning",
      "authors": "Shi Chen, Qi Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Divide_and_Conquer_Answering_Questions_With_Object_Factorization_and_Compositional_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_126",
      "paper_id": "",
      "title": "DocLayLLM: An Efficient Multi-modal Extension of Large Language Models for Text-rich Document Understanding",
      "authors": "Wenhui Liao, Jiapeng Wang, Hongliang Li, Chengyu Wang, Jun Huang, Lianwen Jin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liao_DocLayLLM_An_Efficient_Multi-modal_Extension_of_Large_Language_Models_for_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_906",
      "paper_id": "",
      "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding",
      "authors": "Yuchen Duan, Zhe Chen, Yusong Hu, Weiyun Wang, Shenglong Ye, Botian Shi, Lewei Lu, Qibin Hou, Tong Lu, Hongsheng Li, Jifeng Dai, Wenhai Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Duan_Docopilot_Improving_Multimodal_Models_for_Document-Level_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_251",
      "paper_id": "",
      "title": "DocSAM: Unified Document Image Segmentation via Query Decomposition and Heterogeneous Mixed Learning",
      "authors": "Xiao-Hui Li, Fei Yin, Cheng-Lin Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_DocSAM_Unified_Document_Image_Segmentation_via_Query_Decomposition_and_Heterogeneous_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1672",
      "paper_id": "",
      "title": "Document Haystacks:  Vision-Language Reasoning Over Piles of 1000+ Documents",
      "authors": "Jun Chen, Dannong Xu, Junjie Fei, Chun-Mei Feng, Mohamed Elhoseiny",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Document_Haystacks__Vision-Language_Reasoning_Over_Piles_of_1000_Documents_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1262",
      "paper_id": "",
      "title": "DocVLM: Make Your VLM an Efficient Reader",
      "authors": "Mor Shpigel Nacson, Aviad Aberdam, Roy Ganz, Elad Ben Avraham, Alona Golts, Yair Kittenplon, Shai Mazor, Ron Litman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Nacson_DocVLM_Make_Your_VLM_an_Efficient_Reader_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1107",
      "paper_id": "",
      "title": "Domain-Robust VQA With Diverse Datasets and Methods but No Target Labels",
      "authors": "Mingda Zhang, Tristan Maidment, Ahmad Diab, Adriana Kovashka, Rebecca Hwa",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Domain-Robust_VQA_With_Diverse_Datasets_and_Methods_but_No_Target_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_1326",
      "paper_id": "",
      "title": "DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback",
      "authors": "Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_DRESS_Instructing_Large_Vision-Language_Models_to_Align_and_Interact_with_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_93",
      "paper_id": "",
      "title": "DViN: Dynamic Visual Routing Network for Weakly Supervised Referring Expression Comprehension",
      "authors": "Xiaofu Chen, Yaxin Luo, Gen Luo, Jiayi Ji, Henghui Ding, Yiyi Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_DViN_Dynamic_Visual_Routing_Network_for_Weakly_Supervised_Referring_Expression_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2252",
      "paper_id": "",
      "title": "DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding",
      "authors": "Geng Li, Jinglin Xu, Yunzhen Zhao, Yuxin Peng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1422",
      "paper_id": "",
      "title": "DynRefer: Delving into Region-level Multimodal Tasks via Dynamic Resolution",
      "authors": "Yuzhong Zhao, Feng Liu, Yue Liu, Mingxiang Liao, Chen Gong, Qixiang Ye, Fang Wan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_DynRefer_Delving_into_Region-level_Multimodal_Tasks_via_Dynamic_Resolution_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1227",
      "paper_id": "",
      "title": "E-GPS: Explainable Geometry Problem Solving via Top-Down Solver and Bottom-Up Generator",
      "authors": "Wenjun Wu, Lingling Zhang, Jun Liu, Xi Tang, Yaxian Wang, Shaowei Wang, Qianying Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_E-GPS_Explainable_Geometry_Problem_Solving_via_Top-Down_Solver_and_Bottom-Up_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_672",
      "paper_id": "",
      "title": "EEE-Bench: A Comprehensive Multimodal Electrical And Electronics Engineering Benchmark",
      "authors": "Ming Li, Jike Zhong, Tianle Chen, Yuxiang Lai, Konstantinos Psounis",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_EEE-Bench_A_Comprehensive_Multimodal_Electrical_And_Electronics_Engineering_Benchmark_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2472",
      "paper_id": "",
      "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
      "authors": "Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Lanqing Hong, Lu Hou, Hang Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_EMOVA_Empowering_Language_Models_to_See_Hear_and_Speak_with_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2122",
      "paper_id": "",
      "title": "EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning",
      "authors": "Hongxia Xie, Chu-Jun Peng, Yu-Wen Tseng, Hung-Jen Chen, Chan-Feng Hsu, Hong-Han Shuai, Wen-Huang Cheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_EmoVIT_Revolutionizing_Emotion_Insights_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1953",
      "paper_id": "",
      "title": "EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching",
      "authors": "Yaya Shi, Xu Yang, Haiyang Xu, Chunfeng Yuan, Bing Li, Weiming Hu, Zheng-Jun Zha",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_EMScore_Evaluating_Video_Captioning_via_Coarse-Grained_and_Fine-Grained_Embedding_Matching_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_4",
      "paper_id": "",
      "title": "Encoder Fusion Network With Co-Attention Embedding for Referring Image Segmentation",
      "authors": "Guang Feng, Zhiwei Hu, Lihe Zhang, Huchuan Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Feng_Encoder_Fusion_Network_With_Co-Attention_Embedding_for_Referring_Image_Segmentation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2477",
      "paper_id": "",
      "title": "Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models",
      "authors": "Xin Li, Yunfei Wu, Xinghua Jiang, Zhihao Guo, Mingming Gong, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Enhancing_Visual_Document_Understanding_with_Contrastive_Learning_in_Large_Visual-Language_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_691",
      "paper_id": "",
      "title": "Evaluating Model Perception of Color Illusions in Photorealistic Scenes",
      "authors": "Lingjun Mao, Zineng Tang, Alane Suhr",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Mao_Evaluating_Model_Perception_of_Color_Illusions_in_Photorealistic_Scenes_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_36",
      "paper_id": "",
      "title": "Evaluating Vision-Language Models as Evaluators in Path Planning",
      "authors": "Mohamed Aghzal, Xiang Yue, Erion Plaku, Ziyu Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1066",
      "paper_id": "",
      "title": "Explicit Knowledge Incorporation for Visual Reasoning",
      "authors": "Yifeng Zhang, Ming Jiang, Qi Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Explicit_Knowledge_Incorporation_for_Visual_Reasoning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1375",
      "paper_id": "",
      "title": "Exploring Contextual Attribute Density in Referring Expression Counting",
      "authors": "Zhicheng Wang, Zhiyu Pan, Zhan Peng, Jian Cheng, Liwen Xiao, Wei Jiang, Zhiguo Cao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Exploring_Contextual_Attribute_Density_in_Referring_Expression_Counting_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_943",
      "paper_id": "",
      "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
      "authors": "Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Tong_Eyes_Wide_Shut_Exploring_the_Visual_Shortcomings_of_Multimodal_LLMs_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2304",
      "paper_id": "",
      "title": "F-LMM: Grounding Frozen Large Multimodal Models",
      "authors": "Size Wu, Sheng Jin, Wenwei Zhang, Lumin Xu, Wentao Liu, Wei Li, Chen Change Loy",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_F-LMM_Grounding_Frozen_Large_Multimodal_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2372",
      "paper_id": "",
      "title": "FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs",
      "authors": "Xiaoqin Wang, Xusen Ma, Xianxu Hou, Meidan Ding, Yudong Li, Junliang Chen, Wenting Chen, Xiaoyang Peng, Linlin Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_FaceBench_A_Multi-View_Multi-Level_Facial_Attribute_VQA_Dataset_for_Benchmarking_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1408",
      "paper_id": "",
      "title": "FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models",
      "authors": "Alice Heiman, Xiaoman Zhang, Emma Chen, Sung Eun Kim, Pranav Rajpurkar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Heiman_FactCheXcker_Mitigating_Measurement_Hallucinations_in_Chest_X-ray_Report_Generation_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_703",
      "paper_id": "",
      "title": "FAIEr: Fidelity and Adequacy Ensured Image Caption Evaluation",
      "authors": "Sijin Wang, Ziwei Yao, Ruiping Wang, Zhongqin Wu, Xilin Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_FAIEr_Fidelity_and_Adequacy_Ensured_Image_Caption_Evaluation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_925",
      "paper_id": "",
      "title": "Few Could Be Better Than All: Feature Sampling and Grouping for Scene Text Detection",
      "authors": "Jingqun Tang, Wenqing Zhang, Hongye Liu, MingKun Yang, Bo Jiang, Guanglong Hu, Xiang Bai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Few_Could_Be_Better_Than_All_Feature_Sampling_and_Grouping_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_504",
      "paper_id": "",
      "title": "Filter Images First, Generate Instructions Later: Pre-Instruction Data Selection for Visual Instruction Tuning",
      "authors": "Bardia Safaei, Faizan Siddiqui, Jiacong Xu, Vishal M. Patel, Shao-Yuan Lo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2164",
      "paper_id": "",
      "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity",
      "authors": "Hang Hua, Qing Liu, Lingzhi Zhang, Jing Shi, Soo Ye Kim, Zhifei Zhang, Yilin Wang, Jianming Zhang, Zhe Lin, Jiebo Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hua_FINECAPTION_Compositional_Image_Captioning_Focusing_on_Wherever_You_Want_at_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_953",
      "paper_id": "",
      "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion",
      "authors": "Jiuhai Chen, Jianwei Yang, Haiping Wu, Dianqi Li, Jianfeng Gao, Tianyi Zhou, Bin Xiao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Florence-VL_Enhancing_Vision-Language_Models_with_Generative_Vision_Encoder_and_Depth-Breadth_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1416",
      "paper_id": "",
      "title": "Found a Reason for me? Weakly-supervised Grounded Visual Question Answering using Capsules",
      "authors": "Aisha Urooj, Hilde Kuehne, Kevin Duarte, Chuang Gan, Niels Lobo, Mubarak Shah",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Urooj_Found_a_Reason_for_me_Weakly-supervised_Grounded_Visual_Question_Answering_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1380",
      "paper_id": "",
      "title": "Fourier Contour Embedding for Arbitrary-Shaped Text Detection",
      "authors": "Yiqin Zhu, Jianyong Chen, Lingyu Liang, Zhanghui Kuang, Lianwen Jin, Wayne Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Fourier_Contour_Embedding_for_Arbitrary-Shaped_Text_Detection_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1920",
      "paper_id": "",
      "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering",
      "authors": "Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_FRAMES-VQA_Benchmarking_Fine-Tuning_Robustness_across_Multi-Modal_Shifts_in_Visual_Question_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1696",
      "paper_id": "",
      "title": "From Images to Textual Prompts: Zero-Shot Visual Question Answering With Frozen Large Language Models",
      "authors": "Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, Steven Hoi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_From_Images_to_Textual_Prompts_Zero-Shot_Visual_Question_Answering_With_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1581",
      "paper_id": "",
      "title": "Fusing Pre-Trained Language Models With Multimodal Prompts Through Reinforcement Learning",
      "authors": "Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, Jae Sung Park, Ximing Lu, Rowan Zellers, Prithviraj Ammanabrolu, Ronan Le Bras, Gunhee Kim, Yejin Choi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Fusing_Pre-Trained_Language_Models_With_Multimodal_Prompts_Through_Reinforcement_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_40",
      "paper_id": "",
      "title": "Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding",
      "authors": "Tianyu Chen, Xingcheng Fu, Yisen Gao, Haodong Qian, Yuecen Wei, Kun Yan, Haoyi Zhou, Jianxin Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_302",
      "paper_id": "",
      "title": "Generative Bias for Robust Visual Question Answering",
      "authors": "Jae Won Cho, Dong-Jin Kim, Hyeonggon Ryu, In So Kweon",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Generative_Bias_for_Robust_Visual_Question_Answering_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_435",
      "paper_id": "",
      "title": "Generative Multimodal Models are In-Context Learners",
      "authors": "Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_Generative_Multimodal_Models_are_In-Context_Learners_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2165",
      "paper_id": "",
      "title": "GeoChat: Grounded Large Vision-Language Model for Remote Sensing",
      "authors": "Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, Fahad Shahbaz Khan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kuckreja_GeoChat_Grounded_Large_Vision-Language_Model_for_Remote_Sensing_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_157",
      "paper_id": "",
      "title": "GeoLayoutLM: Geometric Pre-Training for Visual Information Extraction",
      "authors": "Chuwei Luo, Changxu Cheng, Qi Zheng, Cong Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_GeoLayoutLM_Geometric_Pre-Training_for_Visual_Information_Extraction_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1625",
      "paper_id": "",
      "title": "GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks",
      "authors": "Haoqiang Kang, Enna Sachdeva, Piyush Gupta, Sangjae Bae, Kwonjoon Lee",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kang_GFlowVLM_Enhancing_Multi-step_Reasoning_in_Vision-Language_Models_with_Generative_Flow_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1041",
      "paper_id": "",
      "title": "GLaMM: Pixel Grounding Large Multimodal Model",
      "authors": "Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, Fahad S. Khan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Rasheed_GLaMM_Pixel_Grounding_Large_Multimodal_Model_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1665",
      "paper_id": "",
      "title": "GlitchBench: Can Large Multimodal Models Detect Video Game Glitches?",
      "authors": "Mohammad Reza Taesiri, Tianjun Feng, Cor-Paul Bezemer, Anh Nguyen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Taesiri_GlitchBench_Can_Large_Multimodal_Models_Detect_Video_Game_Glitches_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1262",
      "paper_id": "",
      "title": "GRAM: Global Reasoning for Multi-Page VQA",
      "authors": "Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts, Roy Ganz, Elad Ben Avraham, Aviad Aberdam, Shahar Tsiper, Ron Litman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Blau_GRAM_Global_Reasoning_for_Multi-Page_VQA_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_969",
      "paper_id": "",
      "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
      "authors": "Yi Fang, Bowen Jin, Jiacheng Shen, Sirui Ding, Qiaoyu Tan, Jiawei Han",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Fang_GRAPHGPT-O_Synergistic_Multimodal_Comprehension_and_Generation_on_Graphs_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_496",
      "paper_id": "",
      "title": "GRES: Generalized Referring Expression Segmentation",
      "authors": "Chang Liu, Henghui Ding, Xudong Jiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2179",
      "paper_id": "",
      "title": "Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels",
      "authors": "Yongshuo Zong, Qin Zhang, Dongsheng An, Zhihua Li, Xiang Xu, Linghan Xu, Zhuowen Tu, Yifan Xing, Onkar Dabeer",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zong_Ground-V_Teaching_VLMs_to_Ground_Complex_Instructions_in_Pixels_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_599",
      "paper_id": "",
      "title": "GROUNDHOG: Grounding Large Language Models to Holistic Segmentation",
      "authors": "Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, Joyce Chai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_GROUNDHOG_Grounding_Large_Language_Models_to_Holistic_Segmentation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_515",
      "paper_id": "",
      "title": "Grounding Answers for Visual Questions Asked by Visually Impaired People",
      "authors": "Chongyan Chen, Samreen Anjum, Danna Gurari",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Grounding_Answers_for_Visual_Questions_Asked_by_Visually_Impaired_People_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2699",
      "paper_id": "",
      "title": "GroundingFace: Fine-grained Face Understanding via Pixel Grounding Multimodal Large Language Model",
      "authors": "Yue Han, Jiangning Zhang, Junwei Zhu, Runze Hou, Xiaozhong Ji, Chuming Lin, Xiaobin Hu, Zhucun Xue, Yong Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1910",
      "paper_id": "",
      "title": "GSVA: Generalized Segmentation via Multimodal Large Language Models",
      "authors": "Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, Gao Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xia_GSVA_Generalized_Segmentation_via_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1067",
      "paper_id": "",
      "title": "HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning",
      "authors": "Chia-Wen Kuo, Zsolt Kira",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kuo_HAAV_Hierarchical_Aggregation_of_Augmented_Views_for_Image_Captioning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2051",
      "paper_id": "",
      "title": "HalLoc: Token-level Localization of Hallucinations for Vision Language Models",
      "authors": "Eunkyu Park, Minyeong Kim, Gunhee Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Park_HalLoc_Token-level_Localization_of_Hallucinations_for_Vision_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2393",
      "paper_id": "",
      "title": "HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data",
      "authors": "Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, Yueting Zhuang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_HalluciDoctor_Mitigating_Hallucinatory_Toxicity_in_Visual_Instruction_Data_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_977",
      "paper_id": "",
      "title": "Hallucination Augmented Contrastive Learning for Multimodal Large Language Model",
      "authors": "Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, Shikun Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_Hallucination_Augmented_Contrastive_Learning_for_Multimodal_Large_Language_Model_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_880",
      "paper_id": "",
      "title": "HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models",
      "authors": "Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Guan_HallusionBench_An_Advanced_Diagnostic_Suite_for_Entangled_Language_Hallucination_and_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1657",
      "paper_id": "",
      "title": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator",
      "authors": "Fan Yang, Ru Zhen, Jianing Wang, Yanhao Zhang, Haoxiang Chen, Haonan Lu, Sicheng Zhao, Guiguang Ding",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_HEIE_MLLM-Based_Hierarchical_Explainable_AIGC_Image_Implausibility_Evaluator_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2042",
      "paper_id": "",
      "title": "Honeybee: Locality-enhanced Projector for Multimodal LLM",
      "authors": "Junbum Cha, Wooyoung Kang, Jonghwan Mun, Byungseok Roh",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Cha_Honeybee_Locality-enhanced_Projector_for_Multimodal_LLM_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2793",
      "paper_id": "",
      "title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding",
      "authors": "Chenxin Tao, Shiqian Su, Xizhou Zhu, Chenyu Zhang, Zhe Chen, Jiawen Liu, Wenhai Wang, Lewei Lu, Gao Huang, Yu Qiao, Jifeng Dai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tao_HoVLE_Unleashing_the_Power_of_Monolithic_Vision-Language_Models_with_Holistic_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_998",
      "paper_id": "",
      "title": "How Transferable Are Reasoning Patterns in VQA?",
      "authors": "Corentin Kervadec, Theo Jaunet, Grigory Antipov, Moez Baccouche, Romain Vuillemot, Christian Wolf",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Kervadec_How_Transferable_Are_Reasoning_Patterns_in_VQA_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2557",
      "paper_id": "",
      "title": "HRVDA: High-Resolution Visual Document Assistant",
      "authors": "Chaohu Liu, Kun Yin, Haoyu Cao, Xinghua Jiang, Xin Li, Yinsong Liu, Deqiang Jiang, Xing Sun, Linli Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_HRVDA_High-Resolution_Visual_Document_Assistant_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_47",
      "paper_id": "",
      "title": "Human-Like Controllable Image Captioning With Verb-Specific Semantic Roles",
      "authors": "Long Chen, Zhihong Jiang, Jun Xiao, Wei Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Human-Like_Controllable_Image_Captioning_With_Verb-Specific_Semantic_Roles_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_605",
      "paper_id": "",
      "title": "Hybrid Global-Local Representation with Augmented Spatial Guidance for Zero-Shot Referring Image Segmentation",
      "authors": "Ting Liu, Siyuan Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Hybrid_Global-Local_Representation_with_Augmented_Spatial_Guidance_for_Zero-Shot_Referring_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_441",
      "paper_id": "",
      "title": "Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models",
      "authors": "Zhihang Liu, Chen-Wei Xie, Pandeng Li, Liming Zhao, Longxiang Tang, Yun Zheng, Chuanbin Liu, Hongtao Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Hybrid-Level_Instruction_Injection_for_Video_Token_Compression_in_Multi-modal_Large_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2859",
      "paper_id": "",
      "title": "HyperSeg: Hybrid Segmentation Assistant with Fine-grained Visual Perceiver",
      "authors": "Cong Wei, Yujie Zhong, Haoxian Tan, Yong Liu, Jie Hu, Dengjie Li, Zheng Zhao, Yujiu Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wei_HyperSeg_Hybrid_Segmentation_Assistant_with_Fine-grained_Visual_Perceiver_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1359",
      "paper_id": "",
      "title": "ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models",
      "authors": "Junzhe Chen, Tianshu Zhang, Shiyu Huang, Yuwei Niu, Linfeng Zhang, Lijie Wen, Xuming Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_ICT_Image-Object_Cross-Level_Trusted_Intervention_for_Mitigating_Object_Hallucination_in_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_26",
      "paper_id": "",
      "title": "IDEA-Bench: How Far are Generative Models from Professional Designing?",
      "authors": "Chen Liang, Lianghua Huang, Jingwu Fang, Huanzhang Dou, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Junge Zhang, Xin Zhao, Yu Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_IDEA-Bench_How_Far_are_Generative_Models_from_Professional_Designing_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2659",
      "paper_id": "",
      "title": "Identifying and Mitigating Position Bias of Multi-image Vision-Language Models",
      "authors": "Xinyu Tian, Shu Zou, Zhaoyuan Yang, Jing Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_76",
      "paper_id": "",
      "title": "Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models",
      "authors": "Qirui Jiao, Daoyuan Chen, Yilun Huang, Bolin Ding, Yaliang Li, Ying Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jiao_Img-Diff_Contrastive_Data_Synthesis_for_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_319",
      "paper_id": "",
      "title": "Implicit Feature Alignment: Learn To Convert Text Recognizer to Text Spotter",
      "authors": "Tianwei Wang, Yuanzhi Zhu, Lianwen Jin, Dezhi Peng, Zhe Li, Mengchao He, Yongpan Wang, Canjie Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Implicit_Feature_Alignment_Learn_To_Convert_Text_Recognizer_to_Text_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_1915",
      "paper_id": "",
      "title": "Improved Baselines with Visual Instruction Tuning",
      "authors": "Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_642",
      "paper_id": "",
      "title": "Improved Visual Grounding through Self-Consistent Explanations",
      "authors": "Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg, Vicente Ordonez",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/He_Improved_Visual_Grounding_through_Self-Consistent_Explanations_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2137",
      "paper_id": "",
      "title": "Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles",
      "authors": "Shuquan Ye, Yujia Xie, Dongdong Chen, Yichong Xu, Lu Yuan, Chenguang Zhu, Jing Liao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Improving_Commonsense_in_Vision-Language_Models_via_Knowledge_Graph_Riddles_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_698",
      "paper_id": "",
      "title": "Improving OCR-Based Image Captioning by Incorporating Geometrical Relationship",
      "authors": "Jing Wang, Jinhui Tang, Mingkun Yang, Xiang Bai, Jiebo Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Improving_OCR-Based_Image_Captioning_by_Incorporating_Geometrical_Relationship_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_693",
      "paper_id": "",
      "title": "Improving Selective Visual Question Answering by Learning From Your Peers",
      "authors": "Corentin Dancette, Spencer Whitehead, Rishabh Maheshwary, Ramakrishna Vedantam, Stefan Scherer, Xinlei Chen, Matthieu Cord, Marcus Rohrbach",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Dancette_Improving_Selective_Visual_Question_Answering_by_Learning_From_Your_Peers_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1038",
      "paper_id": "",
      "title": "Improving Table Structure Recognition With Visual-Alignment Sequential Coordinate Modeling",
      "authors": "Yongshuai Huang, Ning Lu, Dapeng Chen, Yibo Li, Zecheng Xie, Shenggao Zhu, Liangcai Gao, Wei Peng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Improving_Table_Structure_Recognition_With_Visual-Alignment_Sequential_Coordinate_Modeling_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_1954",
      "paper_id": "",
      "title": "Improving Visual Grounding by Encouraging Consistent Gradient-Based Explanations",
      "authors": "Ziyan Yang, Kushal Kafle, Franck Dernoncourt, Vicente Ordonez",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Improving_Visual_Grounding_by_Encouraging_Consistent_Gradient-Based_Explanations_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1610",
      "paper_id": "",
      "title": "Improving Visual Grounding With Visual-Linguistic Verification and Iterative Reasoning",
      "authors": "Li Yang, Yan Xu, Chunfeng Yuan, Wei Liu, Bing Li, Weiming Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Improving_Visual_Grounding_With_Visual-Linguistic_Verification_and_Iterative_Reasoning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_1407",
      "paper_id": "",
      "title": "Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation",
      "authors": "Liwei Wang, Jing Huang, Yin Li, Kun Xu, Zhengyuan Yang, Dong Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Improving_Weakly_Supervised_Visual_Grounding_by_Contrastive_Knowledge_Distillation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_1957",
      "paper_id": "",
      "title": "Injecting Semantic Concepts Into End-to-End Image Captioning",
      "authors": "Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lin Liang, Zhe Gan, Lijuan Wang, Yezhou Yang, Zicheng Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Fang_Injecting_Semantic_Concepts_Into_End-to-End_Image_Captioning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1719",
      "paper_id": "",
      "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models",
      "authors": "Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, Ziwei Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2634",
      "paper_id": "",
      "title": "Interleaved-Modal Chain-of-Thought",
      "authors": "Jun Gao, Yongqi Li, Ziqiang Cao, Wenjie Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Gao_Interleaved-Modal_Chain-of-Thought_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_308",
      "paper_id": "",
      "title": "Intrinsic Physical Concepts Discovery With Object-Centric Predictive Models",
      "authors": "Qu Tang, Xiangyu Zhu, Zhen Lei, Zhaoxiang Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Intrinsic_Physical_Concepts_Discovery_With_Object-Centric_Predictive_Models_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2470",
      "paper_id": "",
      "title": "Investigating Compositional Challenges in Vision-Language Models for Visual Grounding",
      "authors": "Yunan Zeng, Yan Huang, Jinjin Zhang, Zequn Jie, Zhenhua Chai, Liang Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_Investigating_Compositional_Challenges_in_Vision-Language_Models_for_Visual_Grounding_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_698",
      "paper_id": "",
      "title": "Is `Right' Right? Enhancing Object Orientation Understanding in Multimodal Large Language Models through Egocentric Instruction Tuning",
      "authors": "Ji Hyeok Jung, Eun Tae Kim, Seoyeon Kim, Joo Ho Lee, Bumsoo Kim, Buru Chang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jung_Is_Right_Right_Enhancing_Object_Orientation_Understanding_in_Multimodal_Large_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_528",
      "paper_id": "",
      "title": "Iterated Learning Improves Compositionality in Large Vision-Language Models",
      "authors": "Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, Ranjay Krishna",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Iterated_Learning_Improves_Compositionality_in_Large_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1086",
      "paper_id": "",
      "title": "Iterative Shrinking for Referring Expression Grounding Using Deep Reinforcement Learning",
      "authors": "Mingjie Sun, Jimin Xiao, Eng Gee Lim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_Iterative_Shrinking_for_Referring_Expression_Grounding_Using_Deep_Reinforcement_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_887",
      "paper_id": "",
      "title": "Jack of All Tasks Master of Many: Designing General-Purpose Coarse-to-Fine Vision-Language Model",
      "authors": "Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, Amjad Almahairi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Pramanick_Jack_of_All_Tasks_Master_of_Many_Designing_General-Purpose_Coarse-to-Fine_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2269",
      "paper_id": "",
      "title": "Kernel Adaptive Convolution for Scene Text Detection via Distance Map Prediction",
      "authors": "Jinzhi Zheng, Heng Fan, Libo Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Kernel_Adaptive_Convolution_for_Scene_Text_Detection_via_Distance_Map_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_101",
      "paper_id": "",
      "title": "Knowledge Mining With Scene Text for Fine-Grained Recognition",
      "authors": "Hao Wang, Junchao Liao, Tianheng Cheng, Zewen Gao, Hao Liu, Bo Ren, Xiang Bai, Wenyu Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Knowledge_Mining_With_Scene_Text_for_Fine-Grained_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_1311",
      "paper_id": "",
      "title": "KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA",
      "authors": "Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, Marcus Rohrbach",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Marino_KRISP_Integrating_Implicit_and_Symbolic_Knowledge_for_Open-Domain_Knowledge-Based_VQA_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1483",
      "paper_id": "",
      "title": "L-CoIns: Language-Based Colorization With Instance Awareness",
      "authors": "Zheng Chang, Shuchen Weng, Peixuan Zhang, Yu Li, Si Li, Boxin Shi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_L-CoIns_Language-Based_Colorization_With_Instance_Awareness_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_790",
      "paper_id": "",
      "title": "Language Adaptive Weight Generation for Multi-Task Visual Grounding",
      "authors": "Wei Su, Peihan Miao, Huanzhang Dou, Gaoang Wang, Liang Qiao, Zheyang Li, Xi Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Su_Language_Adaptive_Weight_Generation_for_Multi-Task_Visual_Grounding_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_916",
      "paper_id": "",
      "title": "LaTr: Layout-Aware Transformer for Scene-Text VQA",
      "authors": "Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, R. Manmatha",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Biten_LaTr_Layout-Aware_Transformer_for_Scene-Text_VQA_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1507",
      "paper_id": "",
      "title": "LAVT: Language-Aware Vision Transformer for Referring Image Segmentation",
      "authors": "Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, Philip H.S. Torr",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_LAVT_Language-Aware_Vision_Transformer_for_Referring_Image_Segmentation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_702",
      "paper_id": "",
      "title": "LayoutFormer: Hierarchical Text Detection Towards Scene Text Understanding",
      "authors": "Min Liang, Jia-Wei Ma, Xiaobin Zhu, Jingyan Qin, Xu-Cheng Yin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_LayoutFormer_Hierarchical_Text_Detection_Towards_Scene_Text_Understanding_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1126",
      "paper_id": "",
      "title": "LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding",
      "authors": "Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, Cong Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_LayoutLLM_Layout_Instruction_Tuning_with_Large_Language_Models_for_Document_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1467",
      "paper_id": "",
      "title": "Learning Better Visual Dialog Agents With Pretrained Visual-Linguistic Representation",
      "authors": "Tao Tu, Qing Ping, Govindarajan Thattai, Gokhan Tur, Prem Natarajan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Tu_Learning_Better_Visual_Dialog_Agents_With_Pretrained_Visual-Linguistic_Representation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_466",
      "paper_id": "",
      "title": "Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning",
      "authors": "Rongjie Li, Yu Wu, Xuming He",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Learning_by_Correction_Efficient_Tuning_Task_for_Zero-Shot_Generative_Vision-Language_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_915",
      "paper_id": "",
      "title": "Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs",
      "authors": "Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael S. Ryoo, Tsung-Yu Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ranasinghe_Learning_to_Localize_Objects_Improves_Spatial_Reasoning_in_Visual-LLMs_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1303",
      "paper_id": "",
      "title": "Learning To Segment Every Referring Object Point by Point",
      "authors": "Mengxue Qu, Yu Wu, Yunchao Wei, Wu Liu, Xiaodan Liang, Yao Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Learning_To_Segment_Every_Referring_Object_Point_by_Point_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_674",
      "paper_id": "",
      "title": "Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation",
      "authors": "Shanshan Zhong, Zhongzhan Huang, Shanghua Gao, Wushao Wen, Liang Lin, Marinka Zitnik, Pan Zhou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhong_Lets_Think_Outside_the_Box_Exploring_Leap-of-Thought_in_Large_Language_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1596",
      "paper_id": "",
      "title": "Let's Verify and Reinforce Image Generation Step by Step",
      "authors": "Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Ziyu Guo, Haoquan Zhang, Manyuan Zhang, Jiaming Liu, Peng Gao, Hongsheng Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Lets_Verify_and_Reinforce_Image_Generation_Step_by_Step_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_411",
      "paper_id": "",
      "title": "Linguistics-aware Masked Image Modeling for Self-supervised Scene Text Recognition",
      "authors": "Yifei Zhang, Chang Liu, Jin Wei, Xiaomeng Yang, Yu Zhou, Can Ma, Xiangyang Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Linguistics-aware_Masked_Image_Modeling_for_Self-supervised_Scene_Text_Recognition_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_251",
      "paper_id": "",
      "title": "LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge",
      "authors": "Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, Liqiang Nie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LION_Empowering_Multimodal_Large_Language_Model_with_Dual-Level_Visual_Knowledge_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_832",
      "paper_id": "",
      "title": "LISA: Reasoning Segmentation via Large Language Model",
      "authors": "Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2582",
      "paper_id": "",
      "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
      "authors": "Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, Chunyuan Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Xiong_LLaVA-Critic_Learning_to_Evaluate_Multimodal_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1281",
      "paper_id": "",
      "title": "Locate Then Segment: A Strong Pipeline for Referring Image Segmentation",
      "authors": "Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, Tieniu Tan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Locate_Then_Segment_A_Strong_Pipeline_for_Referring_Image_Segmentation_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1965",
      "paper_id": "",
      "title": "Logical Implications for Visual Question Answering Consistency",
      "authors": "Sergio Tascon-Morales, Pablo Márquez-Neila, Raphael Sznitman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tascon-Morales_Logical_Implications_for_Visual_Question_Answering_Consistency_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_99",
      "paper_id": "",
      "title": "Look Before You Leap: Learning Landmark Features for One-Stage Visual Grounding",
      "authors": "Binbin Huang, Dongze Lian, Weixin Luo, Shenghua Gao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Look_Before_You_Leap_Learning_Landmark_Features_for_One-Stage_Visual_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_1810",
      "paper_id": "",
      "title": "LQMFormer: Language-aware Query Mask Transformer for Referring Image Segmentation",
      "authors": "Nisarg A. Shah, Vibashan VS, Vishal M. Patel",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Shah_LQMFormer_Language-aware_Query_Mask_Transformer_for_Referring_Image_Segmentation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1370",
      "paper_id": "",
      "title": "M6Doc: A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Category Dataset for Modern Document Layout Analysis",
      "authors": "Hiuyi Cheng, Peirong Zhang, Sihang Wu, Jiaxin Zhang, Qiyuan Zhu, Zecheng Xie, Jing Li, Kai Ding, Lianwen Jin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_M6Doc_A_Large-Scale_Multi-Format_Multi-Type_Multi-Layout_Multi-Language_Multi-Annotation_Category_Dataset_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1168",
      "paper_id": "",
      "title": "Maintaining Reasoning Consistency in Compositional Visual Question Answering",
      "authors": "Chenchen Jing, Yunde Jia, Yuwei Wu, Xinyu Liu, Qi Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Jing_Maintaining_Reasoning_Consistency_in_Compositional_Visual_Question_Answering_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1751",
      "paper_id": "",
      "title": "MarkushGrapher: Joint Visual and Textual Recognition of Markush Structures",
      "authors": "Lucas Morin, Valery Weber, Ahmed Nassar, Gerhard Ingmar Meijer, Luc Van Gool, Yawei Li, Peter Staar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Morin_MarkushGrapher_Joint_Visual_and_Textual_Recognition_of_Markush_Structures_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_631",
      "paper_id": "",
      "title": "Marten: Visual Question Answering with Mask Generation for Multi-modal Document Understanding",
      "authors": "Zining Wang, Tongkun Guan, Pei Fu, Chen Duan, Qianyi Jiang, Zhentao Guo, Shan Guo, Junfeng Luo, Wei Shen, Xiaokang Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Marten_Visual_Question_Answering_with_Mask_Generation_for_Multi-modal_Document_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1521",
      "paper_id": "",
      "title": "MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations",
      "authors": "Kyungho Bae, Jinhyung Kim, Sihaeng Lee, Soonyoung Lee, Gunhee Lee, Jinwoo Choi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1513",
      "paper_id": "",
      "title": "Mask Grounding for Referring Image Segmentation",
      "authors": "Yong Xien Chng, Henry Zheng, Yizeng Han, Xuchong Qiu, Gao Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chng_Mask_Grounding_for_Referring_Image_Segmentation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1777",
      "paper_id": "",
      "title": "Mask4Align: Aligned Entity Prompting with Color Masks for Multi-Entity Localization Problems",
      "authors": "Haoquan Zhang, Ronggang Huang, Yi Xie, Huaidong Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Mask4Align_Aligned_Entity_Prompting_with_Color_Masks_for_Multi-Entity_Localization_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_59",
      "paper_id": "",
      "title": "Meta Compositional Referring Expression Segmentation",
      "authors": "Li Xu, Mark He Huang, Xindi Shang, Zehuan Yuan, Ying Sun, Jun Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Meta_Compositional_Referring_Expression_Segmentation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_507",
      "paper_id": "",
      "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research",
      "authors": "James Burgess, Jeffrey J Nirschl, Laura Bravo-Sánchez, Alejandro Lozano, Sanket Rajan Gupte, Jesus G. Galaz-Montoya, Yuhui Zhang, Yuchang Su, Disha Bhowmik, Zachary Coman, Sarina M Hasan, Alexandra Johannesson, William D. Leineweber, Malvika G Nair, Ridhi Yarlagadda, Connor Zuraski, Wah Chiu, Sarah Cohen, Jan N. Hansen, Manuel D Leonetti, Chad Liu, Emma Lundberg, Serena Yeung-Levy",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Burgess_MicroVQA_A_Multimodal_Reasoning_Benchmark_for_Microscopy-Based_Scientific_Research_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_810",
      "paper_id": "",
      "title": "Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key",
      "authors": "Zhihe Yang, Xufang Luo, Dongqi Han, Yunjian Xu, Dongsheng Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1287",
      "paper_id": "",
      "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding",
      "authors": "Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, Lidong Bing",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Leng_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_through_Visual_Contrastive_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_749",
      "paper_id": "",
      "title": "Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention",
      "authors": "Wenbin An, Feng Tian, Sicong Leng, Jiahao Nie, Haonan Lin, Qianying Wang, Ping Chen, Xiaoqin Zhang, Shijian Lu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/An_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_with_Assembly_of_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1190",
      "paper_id": "",
      "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
      "authors": "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yue_MMMU_A_Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1157",
      "paper_id": "",
      "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
      "authors": "Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, Chengye Wang, Ziyao Shangguan, Zhenwen Liang, Yixin Liu, Chen Zhao, Arman Cohan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_MMVU_Measuring_Expert-Level_Multi-Discipline_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2444",
      "paper_id": "",
      "title": "ModaVerse: Efficiently Transforming Modalities with LLMs",
      "authors": "Xinyu Wang, Bohan Zhuang, Qi Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_ModaVerse_Efficiently_Transforming_Modalities_with_LLMs_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2180",
      "paper_id": "",
      "title": "Modeling Entities As Semantic Points for Visual Information Extraction in the Wild",
      "authors": "Zhibo Yang, Rujiao Long, Pengfei Wang, Sibo Song, Humen Zhong, Wenqing Cheng, Xiang Bai, Cong Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Modeling_Entities_As_Semantic_Points_for_Visual_Information_Extraction_in_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_282",
      "paper_id": "",
      "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models",
      "authors": "Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_643",
      "paper_id": "",
      "title": "Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models",
      "authors": "Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, Xiang Bai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Monkey_Image_Resolution_and_Text_Label_Are_Important_Things_for_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1523",
      "paper_id": "",
      "title": "Mosaic of Modalities: A Comprehensive Benchmark for Multimodal Graph Learning",
      "authors": "Jing Zhu, Yuhang Zhou, Shengyi Qian, Zhongmou He, Tong Zhao, Neil Shah, Danai Koutra",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Mosaic_of_Modalities_A_Comprehensive_Benchmark_for_Multimodal_Graph_Learning_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1615",
      "paper_id": "",
      "title": "MOST: A Multi-Oriented Scene Text Detector With Localization Refinement",
      "authors": "Minghang He, Minghui Liao, Zhibo Yang, Humen Zhong, Jun Tang, Wenqing Cheng, Cong Yao, Yongpan Wang, Xiang Bai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/He_MOST_A_Multi-Oriented_Scene_Text_Detector_With_Localization_Refinement_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1194",
      "paper_id": "",
      "title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models",
      "authors": "Wenyi Hong, Yean Cheng, Zhuoyi Yang, Weihan Wang, Lefan Wang, Xiaotao Gu, Shiyu Huang, Yuxiao Dong, Jie Tang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hong_MotionBench_Benchmarking_and_Improving_Fine-grained_Video_Motion_Understanding_for_Vision_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_464",
      "paper_id": "",
      "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration",
      "authors": "Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_889",
      "paper_id": "",
      "title": "MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-Based Visual Question Answering",
      "authors": "Yang Ding, Jing Yu, Bang Liu, Yue Hu, Mingxin Cui, Qi Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_MuKEA_Multimodal_Knowledge_Extraction_and_Accumulation_for_Knowledge-Based_Visual_Question_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_16",
      "paper_id": "",
      "title": "Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices",
      "authors": "Junyan Lin, Haoran Chen, Yue Fan, Yingqi Fan, Xin Jin, Hui Su, Jinlan Fu, Xiaoyu Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_Multi-Layer_Visual_Feature_Fusion_in_Multimodal_LLMs_Methods_Analysis_and_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1754",
      "paper_id": "",
      "title": "Multi-Modal Dynamic Graph Transformer for Visual Grounding",
      "authors": "Sijia Chen, Baochun Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Multi-Modal_Dynamic_Graph_Transformer_for_Visual_Grounding_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1941",
      "paper_id": "",
      "title": "Multi-Modal Hallucination Control by Visual Information Grounding",
      "authors": "Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Favero_Multi-Modal_Hallucination_Control_by_Visual_Information_Grounding_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_394",
      "paper_id": "",
      "title": "Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer",
      "authors": "Zhen Zhao, Jingqun Tang, Chunhui Lin, Binghong Wu, Can Huang, Hao Liu, Xin Tan, Zhizhong Zhang, Yuan Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Multi-modal_In-Context_Learning_Makes_an_Ego-evolving_Scene_Text_Recognizer_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2106",
      "paper_id": "",
      "title": "Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception",
      "authors": "Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, Xuansong Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/He_Multi-modal_Instruction_Tuned_LLMs_with_Fine-grained_Visual_Perception_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2864",
      "paper_id": "",
      "title": "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts",
      "authors": "Peijie Wang, Zhong-Zhi Li, Fei Yin, Dekang Ran, Cheng-Lin Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_MV-MATH_Evaluating_Multimodal_Math_Reasoning_in_Multi-Visual_Contexts_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1524",
      "paper_id": "",
      "title": "Neural Collaborative Graph Machines for Table Structure Recognition",
      "authors": "Hao Liu, Xin Li, Bing Liu, Deqiang Jiang, Yinsong Liu, Bo Ren",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Neural_Collaborative_Graph_Machines_for_Table_Structure_Recognition_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1324",
      "paper_id": "",
      "title": "NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks",
      "authors": "Fawaz Sammani, Tanmoy Mukherjee, Nikos Deligiannis",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Sammani_NLX-GPT_A_Model_for_Natural_Language_Explanations_in_Vision_and_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2158",
      "paper_id": "",
      "title": "Non-Natural Image Understanding with Advancing Frequency-based Vision Encoders",
      "authors": "Wang Lin, QingSong Wang, Yueying Feng, Shulei Wang, Tao Jin, Zhou Zhao, Fei Wu, Chang Yao, Jingyuan Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_Non-Natural_Image_Understanding_with_Advancing_Frequency-based_Vision_Encoders_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_647",
      "paper_id": "",
      "title": "Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models",
      "authors": "Davide Berasi, Matteo Farina, Massimiliano Mancini, Elisa Ricci, Nicola Strisciuglio",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_265",
      "paper_id": "",
      "title": "Notes-guided MLLM Reasoning: Enhancing MLLM with Knowledge and Visual Notes for Visual Question Answering",
      "authors": "Wenlong Fang, Qiaofeng Wu, Jing Chen, Yun Xue",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Fang_Notes-guided_MLLM_Reasoning_Enhancing_MLLM_with_Knowledge_and_Visual_Notes_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_740",
      "paper_id": "",
      "title": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection",
      "authors": "Le Yang, Ziwei Zheng, Boxu Chen, Zhengyu Zhao, Chenhao Lin, Chao Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Nullu_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_via_HalluSpace_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2490",
      "paper_id": "",
      "title": "Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding",
      "authors": "Wei Suo, Lijun Zhang, Mengyang Sun, Lin Yuanbo Wu, Peng Wang, Yanning Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_257",
      "paper_id": "",
      "title": "ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models",
      "authors": "Yahan Tu, Rui Hu, Jitao Sang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tu_ODE_Open-Set_Evaluation_of_Hallucinations_in_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1027",
      "paper_id": "",
      "title": "ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting",
      "authors": "Chen Duan, Pei Fu, Shan Guo, Qianyi Jiang, Xiaoming Wei",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Duan_ODM_A_Text-Image_Further_Alignment_Pre-training_Approach_for_Scene_Text_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2263",
      "paper_id": "",
      "title": "Olympus: A Universal Task Router for Computer Vision Tasks",
      "authors": "Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1758",
      "paper_id": "",
      "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
      "authors": "Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, Jin Shi, Fan Wu, Pei Chu, Minghao Liu, Zhenxiang Li, Chao Xu, Bo Zhang, Botian Shi, Zhongying Tu, Conghui He",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ouyang_OmniDocBench_Benchmarking_Diverse_PDF_Document_Parsing_with_Comprehensive_Annotations_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1057",
      "paper_id": "",
      "title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts",
      "authors": "Yuxuan Wang, Yueqian Wang, Bo Chen, Tong Wu, Dongyan Zhao, Zilong Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_OmniMMI_A_Comprehensive_Multi-modal_Interaction_Benchmark_in_Streaming_Video_Contexts_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2341",
      "paper_id": "",
      "title": "OmniParser: A Unified Framework for Text Spotting Key Information Extraction and Table Recognition",
      "authors": "Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, Zhibo Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wan_OmniParser_A_Unified_Framework_for_Text_Spotting_Key_Information_Extraction_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1732",
      "paper_id": "",
      "title": "OneLLM: One Framework to Align All Modalities with Language",
      "authors": "Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Han_OneLLM_One_Framework_to_Align_All_Modalities_with_Language_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1815",
      "paper_id": "",
      "title": "Open-Set Text Recognition via Character-Context Decoupling",
      "authors": "Chang Liu, Chun Yang, Xu-Cheng Yin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Open-Set_Text_Recognition_via_Character-Context_Decoupling_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1028",
      "paper_id": "",
      "title": "OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
      "authors": "Pengfei Zhou, Xiaopeng Peng, Jiajun Song, Chuanhao Li, Zhaopan Xu, Yue Yang, Ziyao Guo, Hao Zhang, Yuqi Lin, Yefei He, Lirui Zhao, Shuo Liu, Tianhua Li, Yuxuan Xie, Xiaojun Chang, Yu Qiao, Wenqi Shao, Kaipeng Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1453",
      "paper_id": "",
      "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation",
      "authors": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_OPERA_Alleviating_Hallucination_in_Multi-Modal_Large_Language_Models_via_Over-Trust_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2114",
      "paper_id": "",
      "title": "Osprey: Pixel Understanding with Visual Instruction Tuning",
      "authors": "Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, Jianke Zhu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Osprey_Pixel_Understanding_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1896",
      "paper_id": "",
      "title": "OTE: Exploring Accurate Scene Text Recognition Using One Token",
      "authors": "Jianjun Xu, Yuxin Wang, Hongtao Xie, Yongdong Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_OTE_Exploring_Accurate_Scene_Text_Recognition_Using_One_Token_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1904",
      "paper_id": "",
      "title": "Patch Matters: Training-free Fine-grained Image Caption Enhancement via Local Perception",
      "authors": "Ruotian Peng, Haiying He, Yake Wei, Yandong Wen, Di Hu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Peng_Patch_Matters_Training-free_Fine-grained_Image_Caption_Enhancement_via_Local_Perception_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1251",
      "paper_id": "",
      "title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
      "authors": "Yangyu Huang, Tianyi Gao, Haoran Xu, Qihao Zhao, Yang Song, Zhipeng Gui, Tengchao Lv, Hao Chen, Lei Cui, Scarlett Li, Furu Wei",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_PEACE_Empowering_Geologic_Map_Holistic_Understanding_with_MLLMs_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_209",
      "paper_id": "",
      "title": "Perception and Semantic Aware Regularization for Sequential Confidence Calibration",
      "authors": "Zhenghua Peng, Yu Luo, Tianshui Chen, Keke Xu, Shuangping Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Perception_and_Semantic_Aware_Regularization_for_Sequential_Confidence_Calibration_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_482",
      "paper_id": "",
      "title": "Perception Matters: Detecting Perception Failures of VQA Models Using Metamorphic Testing",
      "authors": "Yuanyuan Yuan, Shuai Wang, Mingyue Jiang, Tsong Yueh Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Perception_Matters_Detecting_Perception_Failures_of_VQA_Models_Using_Metamorphic_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1707",
      "paper_id": "",
      "title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models",
      "authors": "Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda G. Shapiro, Ranjay Krishna",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Bigverdi_Perception_Tokens_Enhance_Visual_Reasoning_in_Multimodal_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2168",
      "paper_id": "",
      "title": "PerceptionGPT: Effectively Fusing Visual Perception into LLM",
      "authors": "Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, Tong Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Pi_PerceptionGPT_Effectively_Fusing_Visual_Perception_into_LLM_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1232",
      "paper_id": "",
      "title": "Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model",
      "authors": "Yuting Zhang, Hao Lu, Qingyong Hu, Yin Wang, Kaishen Yuan, Xin Liu, Kaishun Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Period-LLM_Extending_the_Periodic_Capability_of_Multimodal_Large_Language_Model_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_28",
      "paper_id": "",
      "title": "PhD: A ChatGPT-Prompted Visual Hallucination Evaluation Dataset",
      "authors": "Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Xirong Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_635",
      "paper_id": "",
      "title": "PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs",
      "authors": "Michael Dorkenwald, Nimrod Barazani, Cees G. M. Snoek, Yuki M. Asano",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Dorkenwald_PIN_Positional_Insert_Unlocks_Object_Localisation_Abilities_in_VLMs_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2390",
      "paper_id": "",
      "title": "Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs",
      "authors": "Shiyu Xuan, Qingpei Guo, Ming Yang, Shiliang Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xuan_Pink_Unveiling_the_Power_of_Referential_Comprehension_for_Multi-modal_LLMs_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1478",
      "paper_id": "",
      "title": "Pixel-Aligned Language Model",
      "authors": "Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Chen Sun, Xiaolong Wang, Cordelia Schmid",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Pixel-Aligned_Language_Model_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2270",
      "paper_id": "",
      "title": "PixelLM: Pixel Reasoning with Large Multimodal Model",
      "authors": "Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, Xiaojie Jin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ren_PixelLM_Pixel_Reasoning_with_Large_Multimodal_Model_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1426",
      "paper_id": "",
      "title": "Polos: Multimodal Metric Learning from Human Feedback for Image Captioning",
      "authors": "Yuiga Wada, Kanta Kaneda, Daichi Saito, Komei Sugiura",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wada_Polos_Multimodal_Metric_Learning_from_Human_Feedback_for_Image_Captioning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_122",
      "paper_id": "",
      "title": "PolyFormer: Referring Image Segmentation As Sequential Polygon Generation",
      "authors": "Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, R. Manmatha",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PolyFormer_Referring_Image_Segmentation_As_Sequential_Polygon_Generation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_2697",
      "paper_id": "",
      "title": "POPEN: Preference-Based Optimization and Ensemble for LVLM-Based Reasoning Segmentation",
      "authors": "Lanyun Zhu, Tianrun Chen, Qianxiong Xu, Xuanyi Liu, Deyi Ji, Haiyang Wu, De Wen Soh, Jun Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_POPEN_Preference-Based_Optimization_and_Ensemble_for_LVLM-Based_Reasoning_Segmentation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_404",
      "paper_id": "",
      "title": "Position-Guided Text Prompt for Vision-Language Pre-Training",
      "authors": "Jinpeng Wang, Pan Zhou, Mike Zheng Shou, Shuicheng Yan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Position-Guided_Text_Prompt_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_520",
      "paper_id": "",
      "title": "Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation",
      "authors": "Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Sarto_Positive-Augmented_Contrastive_Learning_for_Image_and_Video_Captioning_Evaluation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr21_1505",
      "paper_id": "",
      "title": "PQA: Perceptual Question Answering",
      "authors": "Yonggang Qi, Kai Zhang, Aneeshan Sain, Yi-Zhe Song",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Qi_PQA_Perceptual_Question_Answering_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_1186",
      "paper_id": "",
      "title": "Primitive Representation Learning for Scene Text Recognition",
      "authors": "Ruijie Yan, Liangrui Peng, Shanyu Xiao, Gang Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_Primitive_Representation_Learning_for_Scene_Text_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_448",
      "paper_id": "",
      "title": "Progressive Contour Regression for Arbitrary-Shape Scene Text Detection",
      "authors": "Pengwen Dai, Sanyi Zhang, Hua Zhang, Xiaochun Cao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Dai_Progressive_Contour_Regression_for_Arbitrary-Shape_Scene_Text_Detection_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_1009",
      "paper_id": "",
      "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
      "authors": "Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Prompt_Highlighter_Interactive_Control_for_Multi-Modal_LLMs_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_230",
      "paper_id": "",
      "title": "Prompt-Driven Referring Image Segmentation with Instance Contrasting",
      "authors": "Chao Shang, Zichen Song, Heqian Qiu, Lanxiao Wang, Fanman Meng, Hongliang Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Shang_Prompt-Driven_Referring_Image_Segmentation_with_Instance_Contrasting_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_286",
      "paper_id": "",
      "title": "Prompting Large Language Models With Answer Heuristics for Knowledge-Based Visual Question Answering",
      "authors": "Zhenwei Shao, Zhou Yu, Meng Wang, Jun Yu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Shao_Prompting_Large_Language_Models_With_Answer_Heuristics_for_Knowledge-Based_Visual_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1332",
      "paper_id": "",
      "title": "Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding",
      "authors": "Haojun Jiang, Yuanze Lin, Dongchen Han, Shiji Song, Gao Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Pseudo-Q_Generating_Pseudo_Language_Queries_for_Visual_Grounding_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1208",
      "paper_id": "",
      "title": "PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents",
      "authors": "Brandon Smock, Rohith Pesala, Robin Abraham",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Smock_PubTables-1M_Towards_Comprehensive_Table_Extraction_From_Unstructured_Documents_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1916",
      "paper_id": "",
      "title": "Pushing the Performance Limit of Scene Text Recognizer Without Human Annotation",
      "authors": "Caiyuan Zheng, Hui Li, Seon-Min Rhee, Seungju Han, Jae-Joon Han, Peng Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Pushing_the_Performance_Limit_of_Scene_Text_Recognizer_Without_Human_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1384",
      "paper_id": "",
      "title": "PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models",
      "authors": "Chenyu Yang, Xuan Dong, Xizhou Zhu, Weijie Su, Jiahao Wang, Hao Tian, Zhe Chen, Wenhai Wang, Lewei Lu, Jifeng Dai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_PVC_Progressive_Visual_Token_Compression_for_Unified_Image_and_Video_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1236",
      "paper_id": "",
      "title": "Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models",
      "authors": "Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Kaixin Xu, Chunyi Li, Jingwen Hou, Guangtao Zhai, Geng Xue, Wenxiu Sun, Qiong Yan, Weisi Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Q-Instruct_Improving_Low-level_Visual_Abilities_for_Multi-modality_Foundation_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_538",
      "paper_id": "",
      "title": "Query and Attention Augmentation for Knowledge-Based Explainable Reasoning",
      "authors": "Yifeng Zhang, Ming Jiang, Qi Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Query_and_Attention_Augmentation_for_Knowledge-Based_Explainable_Reasoning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_783",
      "paper_id": "",
      "title": "Question Aware Vision Transformer for Multimodal Reasoning",
      "authors": "Roy Ganz, Yair Kittenplon, Aviad Aberdam, Elad Ben Avraham, Oren Nuriel, Shai Mazor, Ron Litman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ganz_Question_Aware_Vision_Transformer_for_Multimodal_Reasoning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2064",
      "paper_id": "",
      "title": "RAP: Retrieval-Augmented Personalization for Multimodal Large Language Models",
      "authors": "Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, Xiangyu Yue",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hao_RAP_Retrieval-Augmented_Personalization_for_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_1021",
      "paper_id": "",
      "title": "Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition",
      "authors": "Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, Yongdong Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_Read_Like_Humans_Autonomous_Bidirectional_and_Iterative_Language_Modeling_for_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1417",
      "paper_id": "",
      "title": "Reasoning to Attend: Try to Understand How <SEG> Token Works",
      "authors": "Rui Qian, Xin Yin, Dejing Dou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Qian_Reasoning_to_Attend_Try_to_Understand_How_SEG_Token_Works_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1011",
      "paper_id": "",
      "title": "RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension",
      "authors": "Lei Jin, Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Annan Shu, Rongrong Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2460",
      "paper_id": "",
      "title": "Referring Expression Counting",
      "authors": "Siyang Dai, Jun Liu, Ngai-Man Cheung",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Dai_Referring_Expression_Counting_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2309",
      "paper_id": "",
      "title": "RefTeacher: A Strong Baseline for Semi-Supervised Referring Expression Comprehension",
      "authors": "Jiamu Sun, Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, Rongrong Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_RefTeacher_A_Strong_Baseline_for_Semi-Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_948",
      "paper_id": "",
      "title": "RegionGPT: Towards Region Understanding Vision Language Model",
      "authors": "Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, Sifei Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_RegionGPT_Towards_Region_Understanding_Vision_Language_Model_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1566",
      "paper_id": "",
      "title": "Relation-aware Instance Refinement for Weakly Supervised Visual Grounding",
      "authors": "Yongfei Liu, Bo Wan, Lin Ma, Xuming He",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Relation-aware_Instance_Refinement_for_Weakly_Supervised_Visual_Grounding_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2703",
      "paper_id": "",
      "title": "Relation-Rich Visual Document Generator for Visual Information Extraction",
      "authors": "Zi-Han Jiang, Chien-Wei Lin, Wei-Hua Li, Hsuan-Tung Liu, Yi-Ren Yeh, Chu-Song Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Jiang_Relation-Rich_Visual_Document_Generator_for_Visual_Information_Extraction_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1700",
      "paper_id": "",
      "title": "ReSTR: Convolution-Free Referring Image Segmentation Using Transformers",
      "authors": "Namyup Kim, Dongwon Kim, Cuiling Lan, Wenjun Zeng, Suha Kwak",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_ReSTR_Convolution-Free_Referring_Image_Segmentation_Using_Transformers_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_262",
      "paper_id": "",
      "title": "Rethinking Text Segmentation: A Novel Dataset and a Text-Specific Refinement Approach",
      "authors": "Xingqian Xu, Zhifei Zhang, Zhaowen Wang, Brian Price, Zhonghao Wang, Humphrey Shi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Rethinking_Text_Segmentation_A_Novel_Dataset_and_a_Text-Specific_Refinement_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1852",
      "paper_id": "",
      "title": "REVEAL: Retrieval-Augmented Visual-Language Pre-Training With Multi-Source Multimodal Knowledge Memory",
      "authors": "Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross, Alireza Fathi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_REVEAL_Retrieval-Augmented_Visual-Language_Pre-Training_With_Multi-Source_Multimodal_Knowledge_Memory_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1758",
      "paper_id": "",
      "title": "Revisiting Counterfactual Problems in Referring Expression Comprehension",
      "authors": "Zhihan Yu, Ruifan Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_Revisiting_Counterfactual_Problems_in_Referring_Expression_Comprehension_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_378",
      "paper_id": "",
      "title": "REX: Reasoning-Aware and Grounded Explanation",
      "authors": "Shi Chen, Qi Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_REX_Reasoning-Aware_and_Grounded_Explanation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2614",
      "paper_id": "",
      "title": "RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness",
      "authors": "Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, Xiaocheng Feng, Jun Song, Bo Zheng, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1301",
      "paper_id": "",
      "title": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback",
      "authors": "Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, Tat-Seng Chua",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_RLHF-V_Towards_Trustworthy_MLLMs_via_Behavior_Alignment_from_Fine-grained_Correctional_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2269",
      "paper_id": "",
      "title": "RMLVQA: A Margin Loss Approach for Visual Question Answering With Language Biases",
      "authors": "Abhipsa Basu, Sravanti Addepalli, R. Venkatesh Babu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Basu_RMLVQA_A_Margin_Loss_Approach_for_Visual_Question_Answering_With_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1310",
      "paper_id": "",
      "title": "ROD-MLLM: Towards More Reliable Object Detection in Multimodal Large Language Models",
      "authors": "Heng Yin, Yuqiang Ren, Ke Yan, Shouhong Ding, Yongtao Hao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yin_ROD-MLLM_Towards_More_Reliable_Object_Detection_in_Multimodal_Large_Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_577",
      "paper_id": "",
      "title": "Roses Are Red, Violets Are Blue... but Should VQA Expect Them To?",
      "authors": "Corentin Kervadec, Grigory Antipov, Moez Baccouche, Christian Wolf",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Kervadec_Roses_Are_Red_Violets_Are_Blue..._but_Should_VQA_Expect_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_794",
      "paper_id": "",
      "title": "Rotated Multi-Scale Interaction Network for Referring Remote Sensing Image Segmentation",
      "authors": "Sihan Liu, Yiwei Ma, Xiaoqing Zhang, Haowei Wang, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Rotated_Multi-Scale_Interaction_Network_for_Referring_Remote_Sensing_Image_Segmentation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1581",
      "paper_id": "",
      "title": "RSTNet: Captioning With Adaptive Attention on Visual and Non-Visual Words",
      "authors": "Xuying Zhang, Xiaoshuai Sun, Yunpeng Luo, Jiayi Ji, Yiyi Zhou, Yongjian Wu, Feiyue Huang, Rongrong Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_RSTNet_Captioning_With_Adaptive_Attention_on_Visual_and_Non-Visual_Words_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_2138",
      "paper_id": "",
      "title": "S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning",
      "authors": "Wei Suo, Mengyang Sun, Weisong Liu, Yiqi Gao, Peng Wang, Yanning Zhang, Qi Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Suo_S3C_Semi-Supervised_VQA_Natural_Language_Explanation_via_Self-Critical_Learning_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1639",
      "paper_id": "",
      "title": "SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models",
      "authors": "Tongtian Yue, Jie Cheng, Longteng Guo, Xingyuan Dai, Zijia Zhao, Xingjian He, Gang Xiong, Yisheng Lv, Jing Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yue_SC-Tune_Unleashing_Self-Consistent_Referential_Comprehension_in_Large_Vision_Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1235",
      "paper_id": "",
      "title": "Scale-Localized Abstract Reasoning",
      "authors": "Yaniv Benny, Niv Pekar, Lior Wolf",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Benny_Scale-Localized_Abstract_Reasoning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_1294",
      "paper_id": "",
      "title": "ScanFormer: Referring Expression Comprehension by Iteratively Scanning",
      "authors": "Wei Su, Peihan Miao, Huanzhang Dou, Xi Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Su_ScanFormer_Referring_Expression_Comprehension_by_Iteratively_Scanning_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_783",
      "paper_id": "",
      "title": "Scene Text Retrieval via Joint Text Detection and Similarity Learning",
      "authors": "Hao Wang, Xiang Bai, Mingkun Yang, Shenggao Zhu, Jing Wang, Wenyu Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Scene_Text_Retrieval_via_Joint_Text_Detection_and_Similarity_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_1170",
      "paper_id": "",
      "title": "See Say and Segment: Teaching LMMs to Overcome False Premises",
      "authors": "Tsung-Han Wu, Giscard Biamby, David Chan, Lisa Dunlap, Ritwik Gupta, Xudong Wang, Joseph E. Gonzalez, Trevor Darrell",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_See_Say_and_Segment_Teaching_LMMs_to_Overcome_False_Premises_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_2672",
      "paper_id": "",
      "title": "SEED-Bench: Benchmarking Multimodal Large Language Models",
      "authors": "Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_SEED-Bench_Benchmarking_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_2346",
      "paper_id": "",
      "title": "Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding",
      "authors": "Feilong Tang, Chengzhi Liu, Zhongxing Xu, Ming Hu, Zile Huang, Haochen Xue, Ziyang Chen, Zelin Peng, Zhiwei Yang, Sijin Zhou, Wenxue Li, Yulong Li, Wenxuan Song, Shiyan Su, Wei Feng, Jionglong Su, Mingquan Lin, Yifan Peng, Xuelian Cheng, Imran Razzak, Zongyuan Ge",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_849",
      "paper_id": "",
      "title": "SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories",
      "authors": "Muzhi Zhu, Yuzhuo Tian, Hao Chen, Chunluan Zhou, Qingpei Guo, Yang Liu, Ming Yang, Chunhua Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_SegAgent_Exploring_Pixel_Understanding_Capabilities_in_MLLMs_by_Imitating_Human_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_596",
      "paper_id": "",
      "title": "Self-Attention Based Text Knowledge Mining for Text Detection",
      "authors": "Qi Wan, Haoqin Ji, Linlin Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wan_Self-Attention_Based_Text_Knowledge_Mining_for_Text_Detection_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr23_1159",
      "paper_id": "",
      "title": "Self-Supervised Implicit Glyph Attention for Text Recognition",
      "authors": "Tongkun Guan, Chaochen Gu, Jingzheng Tu, Xue Yang, Qi Feng, Yudi Zhao, Wei Shen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Guan_Self-Supervised_Implicit_Glyph_Attention_for_Text_Recognition_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_1060",
      "paper_id": "",
      "title": "Self-Training Large Language Models for Improved Visual Program Synthesis With Visual Reinforcement",
      "authors": "Zaid Khan, Vijay Kumar BG, Samuel Schulter, Yun Fu, Manmohan Chandraker",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Khan_Self-Training_Large_Language_Models_for_Improved_Visual_Program_Synthesis_With_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1195",
      "paper_id": "",
      "title": "SelfDoc: Self-Supervised Document Representation Learning",
      "authors": "Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I. Morariu, Handong Zhao, Rajiv Jain, Varun Manjunatha, Hongfu Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_SelfDoc_Self-Supervised_Document_Representation_Learning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr21_355",
      "paper_id": "",
      "title": "Semantic-Aware Video Text Detection",
      "authors": "Wei Feng, Fei Yin, Xu-Yao Zhang, Cheng-Lin Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Feng_Semantic-Aware_Video_Text_Detection_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2055",
      "paper_id": "",
      "title": "SemiETS: Integrating Spatial and Content Consistencies for Semi-Supervised End-to-end Text Spotting",
      "authors": "Dongliang Luo, Hanshen Zhu, Ziyang Zhang, Dingkang Liang, Xudong Xie, Yuliang Liu, Xiang Bai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Luo_SemiETS_Integrating_Spatial_and_Content_Consistencies_for_Semi-Supervised_End-to-end_Text_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_779",
      "paper_id": "",
      "title": "Separating Skills and Concepts for Novel Visual Question Answering",
      "authors": "Spencer Whitehead, Hui Wu, Heng Ji, Rogerio Feris, Kate Saenko",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Whitehead_Separating_Skills_and_Concepts_for_Novel_Visual_Question_Answering_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_2294",
      "paper_id": "",
      "title": "Separation of Powers: On Segregating Knowledge from Observation in LLM-enabled Knowledge-based Visual Question Answering",
      "authors": "Zhen Yang, Zhuo Tao, Qi Chen, Liang Li, Yuankai Qi, Anton van den Hengel, Qingming Huang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Separation_of_Powers_On_Segregating_Knowledge_from_Observation_in_LLM-enabled_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr21_287",
      "paper_id": "",
      "title": "Sequence-to-Sequence Contrastive Learning for Text Recognition",
      "authors": "Aviad Aberdam, Ron Litman, Shahar Tsiper, Oron Anschel, Ron Slossberg, Shai Mazor, R. Manmatha, Pietro Perona",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Aberdam_Sequence-to-Sequence_Contrastive_Learning_for_Text_Recognition_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr22_231",
      "paper_id": "",
      "title": "Shifting More Attention to Visual Backbone: Query-Modulated Refinement Networks for End-to-End Visual Grounding",
      "authors": "Jiabo Ye, Junfeng Tian, Ming Yan, Xiaoshan Yang, Xuwu Wang, Ji Zhang, Liang He, Xin Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ye_Shifting_More_Attention_to_Visual_Backbone_Query-Modulated_Refinement_Networks_for_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_1356",
      "paper_id": "",
      "title": "Show, Deconfound and Tell: Image Captioning With Causal Inference",
      "authors": "Bing Liu, Dong Wang, Xu Yang, Yong Zhou, Rui Yao, Zhiwen Shao, Jiaqi Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Show_Deconfound_and_Tell_Image_Captioning_With_Causal_Inference_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_2",
      "paper_id": "",
      "title": "SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization",
      "authors": "Canjie Luo, Lianwen Jin, Jingdong Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Luo_SimAN_Exploring_Self-Supervised_Representation_Learning_of_Scene_Text_via_Similarity-Aware_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr23_817",
      "paper_id": "",
      "title": "Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding",
      "authors": "Tal Shaharabany, Lior Wolf",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Shaharabany_Similarity_Maps_for_Self-Training_Weakly-Supervised_Phrase_Grounding_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_985",
      "paper_id": "",
      "title": "SimVQA: Exploring Simulated Environments for Visual Question Answering",
      "authors": "Paola Cascante-Bonilla, Hui Wu, Letao Wang, Rogerio S. Feris, Vicente Ordonez",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Cascante-Bonilla_SimVQA_Exploring_Simulated_Environments_for_Visual_Question_Answering_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2492",
      "paper_id": "",
      "title": "Stop Learning it all to Mitigate Visual Hallucination, Focus on the Hallucination Target.",
      "authors": "Dokyoon Yoon, Youngsook Song, Woomyoung Park",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yoon_Stop_Learning_it_all_to_Mitigate_Visual_Hallucination_Focus_on_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_914",
      "paper_id": "",
      "title": "Super-CLEVR: A Virtual Benchmark To Diagnose Domain Robustness in Visual Reasoning",
      "authors": "Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, Alan L. Yuille",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Super-CLEVR_A_Virtual_Benchmark_To_Diagnose_Domain_Robustness_in_Visual_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_226",
      "paper_id": "",
      "title": "SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering",
      "authors": "Vipul Gupta, Zhuowan Li, Adam Kortylewski, Chenyu Zhang, Yingwei Li, Alan Yuille",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_SwapMix_Diagnosing_and_Regularizing_the_Over-Reliance_on_Visual_Context_in_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_811",
      "paper_id": "",
      "title": "SwinTextSpotter: Scene Text Spotting via Better Synergy Between Text Detection and Text Recognition",
      "authors": "Mingxin Huang, Yuliang Liu, Zhenghao Peng, Chongyu Liu, Dahua Lin, Shenggao Zhu, Nicholas Yuan, Kai Ding, Lianwen Jin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_SwinTextSpotter_Scene_Text_Spotting_via_Better_Synergy_Between_Text_Detection_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2202",
      "paper_id": "",
      "title": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding",
      "authors": "Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_SynerGen-VL_Towards_Synergistic_Image_Understanding_and_Generation_with_Vision_Experts_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_114",
      "paper_id": "",
      "title": "SynTab-LLaVA: Enhancing Multimodal Table Understanding with Decoupled Synthesis",
      "authors": "Bangbang Zhou, Zuan Gao, Zixiao Wang, Boqiang Zhang, Yuxin Wang, Zhineng Chen, Hongtao Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_SynTab-LLaVA_Enhancing_Multimodal_Table_Understanding_with_Decoupled_Synthesis_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_1673",
      "paper_id": "",
      "title": "Synthesize Diagnose and Optimize: Towards Fine-Grained Vision-Language Understanding",
      "authors": "Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, Zuxuan Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_Synthesize_Diagnose_and_Optimize_Towards_Fine-Grained_Vision-Language_Understanding_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1125",
      "paper_id": "",
      "title": "Synthesize Step-by-Step: Tools Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",
      "authors": "Zhuowan Li, Bhavan Jasani, Peng Tang, Shabnam Ghadar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Synthesize_Step-by-Step_Tools_Templates_and_LLMs_as_Data_Generators_for_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_683",
      "paper_id": "",
      "title": "TableFormer: Table Structure Understanding With Transformers",
      "authors": "Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, Peter Staar",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Nassar_TableFormer_Table_Structure_Understanding_With_Transformers_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_1381",
      "paper_id": "",
      "title": "TAP: Text-Aware Pre-Training for Text-VQA and Text-Caption",
      "authors": "Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, Jiebo Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_TAP_Text-Aware_Pre-Training_for_Text-VQA_and_Text-Caption_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_9",
      "paper_id": "",
      "title": "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment",
      "authors": "Ziang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, Xinhao Li, Xiangyu Zeng, Zilei Wang, Yali Wang, Yu Qiao, Limin Wang, Yi Wang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yan_Task_Preference_Optimization_Improving_Multimodal_Large_Language_Models_with_Vision_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1660",
      "paper_id": "",
      "title": "Task-aware Cross-modal Feature Refinement Transformer with Large Language Models for Visual Grounding",
      "authors": "Wenbo Chen, Zhen Xu, Ruotao Xu, Si Wu, Hau-San Wong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Task-aware_Cross-modal_Feature_Refinement_Transformer_with_Large_Language_Models_for_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_628",
      "paper_id": "",
      "title": "Teaching Large Language Models to Regress Accurate Image Quality Scores Using Score Distribution",
      "authors": "Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, Chao Dong",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/You_Teaching_Large_Language_Models_to_Regress_Accurate_Image_Quality_Scores_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_759",
      "paper_id": "",
      "title": "Text Grouping Adapter: Adapting Pre-trained Text Detector for Layout Analysis",
      "authors": "Tianci Bi, Xiaoyi Zhang, Zhizheng Zhang, Wenxuan Xie, Cuiling Lan, Yan Lu, Nanning Zheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Bi_Text_Grouping_Adapter_Adapting_Pre-trained_Text_Detector_for_Layout_Analysis_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_706",
      "paper_id": "",
      "title": "Text Spotting Transformers",
      "authors": "Xiang Zhang, Yongwen Su, Subarna Tripathi, Zhuowen Tu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Text_Spotting_Transformers_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_947",
      "paper_id": "",
      "title": "TextOCR: Towards Large-Scale End-to-End Reasoning for Arbitrary-Shaped Scene Text",
      "authors": "Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, Tal Hassner",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Singh_TextOCR_Towards_Large-Scale_End-to-End_Reasoning_for_Arbitrary-Shaped_Scene_Text_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_1962",
      "paper_id": "",
      "title": "The Art of Deception: Color Visual Illusions and Diffusion Models",
      "authors": "Alexandra Gomez-Villa, Kai Wang, C.Alejandro Parraga, Bartłomiej Twardowski, Jesus Malo, Javier Vazquez-Corral, Joost van den Weijer",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Gomez-Villa_The_Art_of_Deception_Color_Visual_Illusions_and_Diffusion_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1702",
      "paper_id": "",
      "title": "The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training",
      "authors": "Gi-Cheon Kang, Sungdong Kim, Jin-Hwa Kim, Donghyun Kwak, Byoung-Tak Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_The_Dialog_Must_Go_On_Improving_Visual_Dialog_via_Generative_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr25_1046",
      "paper_id": "",
      "title": "The Photographer's Eye: Teaching Multimodal Large Language Models to See, and Critique Like Photographers",
      "authors": "Daiqing Qi, Handong Zhao, Jing Shi, Simon Jenni, Yifei Fan, Franck Dernoncourt, Scott Cohen, Sheng Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Qi_The_Photographers_Eye_Teaching_Multimodal_Large_Language_Models_to_See_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_761",
      "paper_id": "",
      "title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models",
      "authors": "Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, C. J. Taylor, Stefano Soatto",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Kaul_THRONE_An_Object-based_Hallucination_Benchmark_for_the_Free-form_Generations_of_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_406",
      "paper_id": "",
      "title": "Towards Accurate Text-Based Image Captioning With Content Diversity Exploration",
      "authors": "Guanghui Xu, Shuaicheng Niu, Mingkui Tan, Yucheng Luo, Qing Du, Qi Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Towards_Accurate_Text-Based_Image_Captioning_With_Content_Diversity_Exploration_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_191",
      "paper_id": "",
      "title": "Towards Better Vision-Inspired Vision-Language Models",
      "authors": "Yun-Hao Cao, Kaixiang Ji, Ziyuan Huang, Chuanyang Zheng, Jiajia Liu, Jian Wang, Jingdong Chen, Ming Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Cao_Towards_Better_Vision-Inspired_Vision-Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_340",
      "paper_id": "",
      "title": "Towards End-to-End Unified Scene Text Detection and Layout Analysis",
      "authors": "Shangbang Long, Siyang Qin, Dmitry Panteleev, Alessandro Bissacco, Yasuhisa Fujii, Michalis Raptis",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Long_Towards_End-to-End_Unified_Scene_Text_Detection_and_Layout_Analysis_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2120",
      "paper_id": "",
      "title": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark",
      "authors": "Hao Guo, Xugong Qin, Jun Jie Ou Yang, Peng Zhang, Gangyan Zeng, Yubo Li, Hailun Lin",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_Towards_Natural_Language-Based_Document_Image_Retrieval_New_Dataset_and_Benchmark_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2600",
      "paper_id": "",
      "title": "Towards Smart Point-and-Shoot Photography",
      "authors": "Jiawan Li, Fei Zhou, Zhipeng Zhong, Jiongzhi Lin, Guoping Qiu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Towards_Smart_Point-and-Shoot_Photography_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_181",
      "paper_id": "",
      "title": "Towards Understanding How Knowledge Evolves in Large Vision-Language Models",
      "authors": "Sudong Wang, Yunjian Zhang, Yao Zhu, Jianing Li, Zizhe Wang, Yanwei Liu, Xiangyang Ji",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Towards_Understanding_How_Knowledge_Evolves_in_Large_Vision-Language_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr23_1458",
      "paper_id": "",
      "title": "Towards Unified Scene Text Spotting Based on Sequence Generation",
      "authors": "Taeho Kil, Seonghyeon Kim, Sukmin Seo, Yoonsik Kim, Daehee Kim",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kil_Towards_Unified_Scene_Text_Spotting_Based_on_Sequence_Generation_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1860",
      "paper_id": "",
      "title": "Towards Weakly-Supervised Text Spotting Using a Multi-Task Transformer",
      "authors": "Yair Kittenplon, Inbal Lavi, Sharon Fogel, Yarin Bar, R. Manmatha, Pietro Perona",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kittenplon_Towards_Weakly-Supervised_Text_Spotting_Using_a_Multi-Task_Transformer_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_893",
      "paper_id": "",
      "title": "Transform-Retrieve-Generate: Natural Language-Centric Outside-Knowledge Visual Question Answering",
      "authors": "Feng Gao, Qing Ping, Govind Thattai, Aishwarya Reganti, Ying Nian Wu, Prem Natarajan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_Transform-Retrieve-Generate_Natural_Language-Centric_Outside-Knowledge_Visual_Question_Answering_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr21_251",
      "paper_id": "",
      "title": "Transformation Driven Visual Reasoning",
      "authors": "Xin Hong, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Transformation_Driven_Visual_Reasoning_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr24_2357",
      "paper_id": "",
      "title": "TRINS: Towards Multimodal Language Models that Can Read",
      "authors": "Ruiyi Zhang, Yanzhe Zhang, Jian Chen, Yufan Zhou, Jiuxiang Gu, Changyou Chen, Tong Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_TRINS_Towards_Multimodal_Language_Models_that_Can_Read_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_831",
      "paper_id": "",
      "title": "Turning a CLIP Model Into a Scene Text Detector",
      "authors": "Wenwen Yu, Yuliang Liu, Wei Hua, Deqiang Jiang, Bo Ren, Xiang Bai",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Turning_a_CLIP_Model_Into_a_Scene_Text_Detector_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_2311",
      "paper_id": "",
      "title": "Unicode Analogies: An Anti-Objectivist Visual Reasoning Challenge",
      "authors": "Steven Spratley, Krista A. Ehinger, Tim Miller",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Spratley_Unicode_Analogies_An_Anti-Objectivist_Visual_Reasoning_Challenge_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_2486",
      "paper_id": "",
      "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action",
      "authors": "Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, Aniruddha Kembhavi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Unified-IO_2_Scaling_Autoregressive_Multimodal_Models_with_Vision_Language_Audio_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_1223",
      "paper_id": "",
      "title": "Unifying Vision, Text, and Layout for Universal Document Processing",
      "authors": "Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Unifying_Vision_Text_and_Layout_for_Universal_Document_Processing_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr24_348",
      "paper_id": "",
      "title": "Unveiling Parts Beyond Objects: Towards Finer-Granularity Referring Expression Segmentation",
      "authors": "Wenxuan Wang, Tongtian Yue, Yisi Zhang, Longteng Guo, Xingjian He, Xinlong Wang, Jing Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Unveiling_Parts_Beyond_Objects_Towards_Finer-Granularity_Referring_Expression_Segmentation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1090",
      "paper_id": "",
      "title": "Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly",
      "authors": "Yexin Liu, Zhengyang Liang, Yueze Wang, Xianfeng Wu, Feilong Tang, Muyang He, Jian Li, Zheng Liu, Harry Yang, Sernam Lim, Bo Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Unveiling_the_Ignorance_of_MLLMs_Seeing_Clearly_Answering_Incorrectly_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_730",
      "paper_id": "",
      "title": "Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach",
      "authors": "Jing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, Bingjie Wang, Chenliang Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Bi_Unveiling_Visual_Perception_in_Language_Models_An_Attention_Head_Analysis_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_950",
      "paper_id": "",
      "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation",
      "authors": "Qihui Zhang, Munan Ning, Zheyuan Liu, Yue Huang, Shuo Yang, Yanbo Wang, Jiayi Ye, Xiao Chen, Yibing Song, Li Yuan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_UPME_An_Unsupervised_Peer_Review_Framework_for_Multimodal_Large_Language_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_567",
      "paper_id": "",
      "title": "UTC: A Unified Transformer With Inter-Task Contrastive Learning for Visual Dialog",
      "authors": "Cheng Chen, Zhenshan Tan, Qingrong Cheng, Xin Jiang, Qun Liu, Yudong Zhu, Xiaodong Gu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UTC_A_Unified_Transformer_With_Inter-Task_Contrastive_Learning_for_Visual_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr22_384",
      "paper_id": "",
      "title": "V-Doc: Visual Questions Answers With Documents",
      "authors": "Yihao Ding, Zhe Huang, Runlin Wang, YanHang Zhang, Xianru Chen, Yuzhong Ma, Hyunsuk Chung, Soyeon Caren Han",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_V-Doc_Visual_Questions_Answers_With_Documents_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_1922",
      "paper_id": "",
      "title": "V?: Guided Visual Search as a Core Mechanism in Multimodal LLMs",
      "authors": "Penghao Wu, Saining Xie",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_V_Guided_Visual_Search_as_a_Core_Mechanism_in_Multimodal_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1927",
      "paper_id": "",
      "title": "V^2Dial: Unification of Video and Visual Dialog via Multimodal Experts",
      "authors": "Adnen Abdessaied, Anna Rohrbach, Marcus Rohrbach, Andreas Bulling",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Abdessaied_V2Dial_Unification_of_Video_and_Visual_Dialog_via_Multimodal_Experts_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_708",
      "paper_id": "",
      "title": "VALHALLA: Visual Hallucination for Machine Translation",
      "authors": "Yi Li, Rameswar Panda, Yoon Kim, Chun-Fu (Richard) Chen, Rogerio S. Feris, David Cox, Nuno Vasconcelos",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_VALHALLA_Visual_Hallucination_for_Machine_Translation_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_240",
      "paper_id": "",
      "title": "VASparse: Towards Efficient Visual Hallucination Mitigation via Visual-Aware Token Sparsification",
      "authors": "Xianwei Zhuang, Zhihong Zhu, Yuxin Xie, Liming Liang, Yuexian Zou",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhuang_VASparse_Towards_Efficient_Visual_Hallucination_Mitigation_via_Visual-Aware_Token_Sparsification_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_503",
      "paper_id": "",
      "title": "VCoder: Versatile Vision Encoders for Multimodal Large Language Models",
      "authors": "Jitesh Jain, Jianwei Yang, Humphrey Shi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Jain_VCoder_Versatile_Vision_Encoders_for_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1695",
      "paper_id": "",
      "title": "VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents",
      "authors": "Ryota Tanaka, Taichi Iki, Taku Hasegawa, Kyosuke Nishida, Kuniko Saito, Jun Suzuki",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tanaka_VDocRAG_Retrieval-Augmented_Generation_over_Visually-Rich_Documents_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_484",
      "paper_id": "",
      "title": "VEU-Bench: Towards Comprehensive Understanding of Video Editing",
      "authors": "Bozheng Li, Yongliang Wu, Yi Lu, Jiashuo Yu, Licheng Tang, Jiawang Cao, Wenqing Zhu, Yuyang Sun, Jay Wu, Wenbo Zhu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_886",
      "paper_id": "",
      "title": "ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation",
      "authors": "Ali Athar, Xueqing Deng, Liang-Chieh Chen",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Athar_ViCaS_A_Dataset_for_Combining_Holistic_and_Pixel-level_Video_Understanding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_254",
      "paper_id": "",
      "title": "VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?",
      "authors": "Yunlong Tang, Junjia Guo, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing Bi, Zeliang Zhang, Pooyan Fazli, Chenliang Xu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_VidComposition_Can_MLLMs_Analyze_Compositions_in_Compiled_Videos_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1706",
      "paper_id": "",
      "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis",
      "authors": "Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, Xing Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_720",
      "paper_id": "",
      "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation",
      "authors": "Ziyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan Kankanhalli, Junnan Li",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Luo_VideoAutoArena_An_Automated_Arena_for_Evaluating_Large_Multimodal_Models_in_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_1803",
      "paper_id": "",
      "title": "VideoGLaMM : A Large Multimodal Model for Pixel-Level Visual Grounding in Videos",
      "authors": "Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, Salman Khan",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Munasinghe_VideoGLaMM__A_Large_Multimodal_Model_for_Pixel-Level_Visual_Grounding_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_518",
      "paper_id": "",
      "title": "VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding",
      "authors": "Chaoyu Li, Eun Woo Im, Pooyan Fazli",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_VidHalluc_Evaluating_Temporal_Hallucinations_in_Multimodal_Large_Language_Models_for_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2611",
      "paper_id": "",
      "title": "VILA: On Pre-training for Visual Language Models",
      "authors": "Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, Song Han",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_VILA_On_Pre-training_for_Visual_Language_Models_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_897",
      "paper_id": "",
      "title": "ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts",
      "authors": "Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Cai_ViP-LLaVA_Making_Large_Multimodal_Models_Understand_Arbitrary_Visual_Prompts_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr25_1461",
      "paper_id": "",
      "title": "VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning",
      "authors": "Xueqing Wu, Yuheng Ding, Bingxuan Li, Pan Lu, Da Yin, Kai-Wei Chang, Nanyun Peng",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_VISCO_Benchmarking_Fine-Grained_Critique_and_Correction_Towards_Self-Improvement_in_Visual_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2708",
      "paper_id": "",
      "title": "Vision-Language Models Do Not Understand Negation",
      "authors": "Kumail Alhamoud, Shaden Alshammari, Yonglong Tian, Guohao Li, Philip H.S. Torr, Yoon Kim, Marzyeh Ghassemi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Alhamoud_Vision-Language_Models_Do_Not_Understand_Negation_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_866",
      "paper_id": "",
      "title": "Vision-Language Pre-Training for Boosting Scene Text Detectors",
      "authors": "Sibo Song, Jianqiang Wan, Zhibo Yang, Jun Tang, Wenqing Cheng, Xiang Bai, Cong Yao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Song_Vision-Language_Pre-Training_for_Boosting_Scene_Text_Detectors_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_1390",
      "paper_id": "",
      "title": "VisionArena: 230k Real World User-VLM Conversations with Preference Labels",
      "authors": "Christopher Chou, Lisa Dunlap, Koki Mashita, Krishna Mandal, Trevor Darrell, Ion Stoica, Joseph E. Gonzalez, Wei-Lin Chiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chou_VisionArena_230k_Real_World_User-VLM_Conversations_with_Preference_Labels_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_1964",
      "paper_id": "",
      "title": "Visual Abductive Reasoning",
      "authors": "Chen Liang, Wenguan Wang, Tianfei Zhou, Yi Yang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Visual_Abductive_Reasoning_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_710",
      "paper_id": "",
      "title": "Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation",
      "authors": "Yunhao Ge, Xiaohui Zeng, Jacob Samuel Huffman, Tsung-Yi Lin, Ming-Yu Liu, Yin Cui",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ge_Visual_Fact_Checker_Enabling_High-Fidelity_Detailed_Caption_Generation_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr24_1534",
      "paper_id": "",
      "title": "Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models",
      "authors": "Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, Ariel Fuxman",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_Visual_Program_Distillation_Distilling_Tools_and_Programmatic_Reasoning_into_Vision-Language_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_493",
      "paper_id": "",
      "title": "Visual Programming: Compositional Visual Reasoning Without Training",
      "authors": "Tanmay Gupta, Aniruddha Kembhavi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr23_2123",
      "paper_id": "",
      "title": "Visual Recognition by Request",
      "authors": "Chufeng Tang, Lingxi Xie, Xiaopeng Zhang, Xiaolin Hu, Qi Tian",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Visual_Recognition_by_Request_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    },
    {
      "id": "cvpr22_1684",
      "paper_id": "",
      "title": "VisualHow: Multimodal Problem Solving",
      "authors": "Jinhui Yang, Xianyu Chen, Ming Jiang, Shi Chen, Louis Wang, Qi Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_VisualHow_Multimodal_Problem_Solving_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_2195",
      "paper_id": "",
      "title": "ViUniT: Visual Unit Tests for More Robust Visual Programming",
      "authors": "Artemis Panagopoulou, Honglu Zhou, Silvio Savarese, Caiming Xiong, Chris Callison-Burch, Mark Yatskar, Juan Carlos Niebles",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Panagopoulou_ViUniT_Visual_Unit_Tests_for_More_Robust_Visual_Programming_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_2420",
      "paper_id": "",
      "title": "VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models",
      "authors": "Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, Qi Liu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_2139",
      "paper_id": "",
      "title": "VTQA: Visual Text Question Answering via Entity Alignment and Cross-Media Reasoning",
      "authors": "Kang Chen, Xiangqian Wu",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_VTQA_Visual_Text_Question_Answering_via_Entity_Alignment_and_Cross-Media_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_1268",
      "paper_id": "",
      "title": "Weakly-Supervised Generation and Grounding of Visual Descriptions With Conditional Generative Models",
      "authors": "Effrosyni Mavroudi, René Vidal",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Mavroudi_Weakly-Supervised_Generation_and_Grounding_of_Visual_Descriptions_With_Conditional_Generative_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_590",
      "paper_id": "",
      "title": "WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation",
      "authors": "Silin Cheng, Yang Liu, Xinwei He, Sebastien Ourselin, Lei Tan, Gen Luo",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Cheng_WeakMCN_Multi-task_Collaborative_Network_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_927",
      "paper_id": "",
      "title": "WebQA: Multihop and Multimodal QA",
      "authors": "Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, Yonatan Bisk",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Chang_WebQA_Multihop_and_Multimodal_QA_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr24_871",
      "paper_id": "",
      "title": "What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models",
      "authors": "Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Yongshuo Zong, Xin Wen, Bingchen Zhao",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_What_If_the_TV_Was_Off_Examining_Counterfactual_Reasoning_Abilities_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr21_1365",
      "paper_id": "",
      "title": "What if We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels",
      "authors": "Jeonghun Baek, Yusuke Matsui, Kiyoharu Aizawa",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Baek_What_if_We_Only_Use_Real_Datasets_for_Scene_Text_CVPR_2021_paper.pdf",
      "venue": "cvpr21"
    },
    {
      "id": "cvpr25_946",
      "paper_id": "",
      "title": "What's in the Image? A Deep-Dive into the Vision of Vision Language Models",
      "authors": "Omri Kaduri, Shai Bagon, Tali Dekel",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kaduri_Whats_in_the_Image_A_Deep-Dive_into_the_Vision_of_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_150",
      "paper_id": "",
      "title": "When Visual Grounding Meets Gigapixel-level Large-scale Scenes: Benchmark and Approach",
      "authors": "Tao Ma, Bing Bai, Haozhe Lin, Heyuan Wang, Yu Wang, Lin Luo, Lu Fang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_When_Visual_Grounding_Meets_Gigapixel-level_Large-scale_Scenes_Benchmark_and_Approach_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr22_539",
      "paper_id": "",
      "title": "Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality",
      "authors": "Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, Candace Ross",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Thrush_Winoground_Probing_Vision_and_Language_Models_for_Visio-Linguistic_Compositionality_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_11",
      "paper_id": "",
      "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
      "authors": "Ailin Deng, Tri Cao, Zhirui Chen, Bryan Hooi",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Deng_Words_or_Vision_Do_Vision-Language_Models_Have_Blind_Faith_in_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr25_841",
      "paper_id": "",
      "title": "XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?",
      "authors": "Fengxiang Wang, Hongzhen Wang, Zonghao Guo, Di Wang, Yulin Wang, Mingshuo Chen, Qiang Ma, Long Lan, Wenjing Yang, Jing Zhang, Zhiyuan Liu, Maosong Sun",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr22_375",
      "paper_id": "",
      "title": "XYLayoutLM: Towards Layout-Aware Multimodal Networks for Visually-Rich Document Understanding",
      "authors": "Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan, Weiqiang Wang, Ming Gu, Liqing Zhang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_XYLayoutLM_Towards_Layout-Aware_Multimodal_Networks_for_Visually-Rich_Document_Understanding_CVPR_2022_paper.pdf",
      "venue": "cvpr22"
    },
    {
      "id": "cvpr25_373",
      "paper_id": "",
      "title": "Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding",
      "authors": "Seil Kang, Jinyeong Kim, Junhyeok Kim, Seong Jae Hwang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads_CVPR_2025_paper.pdf",
      "venue": "cvpr25"
    },
    {
      "id": "cvpr24_214",
      "paper_id": "",
      "title": "Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions",
      "authors": "Zeyu Han, Fangrui Zhu, Qianru Lao, Huaizu Jiang",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Han_Zero-shot_Referring_Expression_Comprehension_via_Structural_Similarity_Between_Images_and_CVPR_2024_paper.pdf",
      "venue": "cvpr24"
    },
    {
      "id": "cvpr23_2087",
      "paper_id": "",
      "title": "Zero-Shot Referring Image Segmentation With Global-Local Context Features",
      "authors": "Seonghoon Yu, Paul Hongsuck Seo, Jeany Son",
      "paper_url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Zero-Shot_Referring_Image_Segmentation_With_Global-Local_Context_Features_CVPR_2023_paper.pdf",
      "venue": "cvpr23"
    }
  ]
}